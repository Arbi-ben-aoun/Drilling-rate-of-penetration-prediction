{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,load_model,save_model\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('well_without_outlier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Flow Out %',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROP(1 m)</th>\n",
       "      <th>Depth(m)</th>\n",
       "      <th>weight on bit (kg)</th>\n",
       "      <th>Rotary Speed (rpm)</th>\n",
       "      <th>Pump Press (KPa)</th>\n",
       "      <th>Temp In(degC)</th>\n",
       "      <th>Flow In(liters/min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "      <td>7293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.569735</td>\n",
       "      <td>1170.125075</td>\n",
       "      <td>10492.418940</td>\n",
       "      <td>54.855718</td>\n",
       "      <td>8737.605204</td>\n",
       "      <td>47.953857</td>\n",
       "      <td>2710.542394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.194831</td>\n",
       "      <td>654.397245</td>\n",
       "      <td>4130.250795</td>\n",
       "      <td>25.296998</td>\n",
       "      <td>3378.177407</td>\n",
       "      <td>6.626395</td>\n",
       "      <td>511.248043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.490000</td>\n",
       "      <td>29.440000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.470000</td>\n",
       "      <td>601.940000</td>\n",
       "      <td>8308.390000</td>\n",
       "      <td>38.120000</td>\n",
       "      <td>4593.170000</td>\n",
       "      <td>42.720000</td>\n",
       "      <td>2347.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.470000</td>\n",
       "      <td>1176.130000</td>\n",
       "      <td>10807.260000</td>\n",
       "      <td>50.380000</td>\n",
       "      <td>9877.500000</td>\n",
       "      <td>47.340000</td>\n",
       "      <td>2650.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.460000</td>\n",
       "      <td>1736.100000</td>\n",
       "      <td>13460.320000</td>\n",
       "      <td>75.950000</td>\n",
       "      <td>11510.100000</td>\n",
       "      <td>52.700000</td>\n",
       "      <td>3120.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>274.750000</td>\n",
       "      <td>2296.940000</td>\n",
       "      <td>21337.870000</td>\n",
       "      <td>178.860000</td>\n",
       "      <td>15171.960000</td>\n",
       "      <td>63.510000</td>\n",
       "      <td>5864.130000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROP(1 m)     Depth(m)  weight on bit (kg)  Rotary Speed (rpm)  \\\n",
       "count  7293.000000  7293.000000         7293.000000         7293.000000   \n",
       "mean     12.569735  1170.125075        10492.418940           54.855718   \n",
       "std      20.194831   654.397245         4130.250795           25.296998   \n",
       "min       0.000000    25.960000            0.000000            0.000000   \n",
       "25%       3.470000   601.940000         8308.390000           38.120000   \n",
       "50%       5.470000  1176.130000        10807.260000           50.380000   \n",
       "75%      13.460000  1736.100000        13460.320000           75.950000   \n",
       "max     274.750000  2296.940000        21337.870000          178.860000   \n",
       "\n",
       "       Pump Press (KPa)  Temp In(degC)  Flow In(liters/min)  \n",
       "count       7293.000000    7293.000000          7293.000000  \n",
       "mean        8737.605204      47.953857          2710.542394  \n",
       "std         3378.177407       6.626395           511.248043  \n",
       "min          137.490000      29.440000             0.000000  \n",
       "25%         4593.170000      42.720000          2347.940000  \n",
       "50%         9877.500000      47.340000          2650.580000  \n",
       "75%        11510.100000      52.700000          3120.960000  \n",
       "max        15171.960000      63.510000          5864.130000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statistical description of the selected features for the target variable ROP\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Shape of x is : (7293, 6)\n",
      "the Shape of y is : (7293,)\n"
     ]
    }
   ],
   "source": [
    "X=df.drop('ROP(1 m)',axis=1)\n",
    "y=df['ROP(1 m)']\n",
    "print('the Shape of x is :',X.shape)\n",
    "print('the Shape of y is :',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the normalized features into numpy array \n",
    "scaler=MinMaxScaler().fit(X_train)\n",
    "scaled_X_train=np.array(scaler.transform(X_train))\n",
    "scaled_X_test=np.array(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the target variable into numpy array \n",
    "ny_train=np.array(y_train)\n",
    "ny_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize a sequential model to be tuned later\n",
    "def ANN_model(nl=0,nn=6):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6,input_dim=6,activation=\"relu\"))\n",
    "    for i in range(nl):\n",
    "        model.add(Dense(nn, activation='relu'))  \n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['mse'] )\n",
    "    return model  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 61\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a KerasRegressor based on this ANN_model()\n",
    "Ann_reg=KerasRegressor(build_fn=ANN_model,epochs=200, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing the number of layers and number of neurons as hyperparameters \n",
    "param={'nl':[0,1,2],'nn':[2,3,6,12,24]}\n",
    "grid_cv = GridSearchCV(Ann_reg, param_grid=param, cv=5,verbose=2,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] nl=0, nn=2 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 794us/sample - loss: 527.4588 - mse: 527.4587 - val_loss: 668.2699 - val_mse: 668.2699\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 307us/sample - loss: 473.6233 - mse: 473.6232 - val_loss: 611.1885 - val_mse: 611.1885\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 421.2288 - mse: 421.2289 - val_loss: 536.8947 - val_mse: 536.8947\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 370.5135 - mse: 370.5136 - val_loss: 471.4187 - val_mse: 471.4188\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 335us/sample - loss: 329.2691 - mse: 329.2690 - val_loss: 420.7240 - val_mse: 420.7240\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 297.0577 - mse: 297.0577 - val_loss: 383.0617 - val_mse: 383.0617\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 321us/sample - loss: 275.9105 - mse: 275.9104 - val_loss: 355.9133 - val_mse: 355.9133\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 264.2105 - mse: 264.2106 - val_loss: 339.8373 - val_mse: 339.8373\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 254.4735 - mse: 254.4736 - val_loss: 329.6621 - val_mse: 329.6622\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 249.6749 - mse: 249.6748 - val_loss: 319.8391 - val_mse: 319.8391\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 248.5560 - mse: 248.5559 - val_loss: 317.1768 - val_mse: 317.1769\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 251.1305 - mse: 251.1306 - val_loss: 312.3284 - val_mse: 312.3285\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 242.9480 - mse: 242.9480 - val_loss: 308.2406 - val_mse: 308.2405\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 245.9576 - mse: 245.9576 - val_loss: 305.6360 - val_mse: 305.6359\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 244.1803 - mse: 244.1803 - val_loss: 305.6158 - val_mse: 305.6158\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 240.8231 - mse: 240.8232 - val_loss: 303.8036 - val_mse: 303.8037\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 234.7768 - mse: 234.7768 - val_loss: 303.2275 - val_mse: 303.2275\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 239.3177 - mse: 239.3177 - val_loss: 301.4852 - val_mse: 301.4853\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 235.5737 - mse: 235.5737 - val_loss: 300.6759 - val_mse: 300.6758\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 234.6447 - mse: 234.6447 - val_loss: 300.6300 - val_mse: 300.6299\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 232.3528 - mse: 232.3528 - val_loss: 297.8040 - val_mse: 297.8040\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 235.6326 - mse: 235.6326 - val_loss: 298.8668 - val_mse: 298.8668\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 238.3852 - mse: 238.3853 - val_loss: 297.8245 - val_mse: 297.8245\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 239.5068 - mse: 239.5068 - val_loss: 298.0811 - val_mse: 298.0810\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 236.7086 - mse: 236.7086 - val_loss: 297.8707 - val_mse: 297.8707\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 236.5378 - mse: 236.5376 - val_loss: 297.2368 - val_mse: 297.2368\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 233.1424 - mse: 233.1424 - val_loss: 295.4939 - val_mse: 295.4940\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 236.7797 - mse: 236.7797 - val_loss: 296.8007 - val_mse: 296.8008\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 241.0684 - mse: 241.0685 - val_loss: 296.1881 - val_mse: 296.1880\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 233.9638 - mse: 233.9639 - val_loss: 296.2114 - val_mse: 296.2114\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 232.4369 - mse: 232.4370 - val_loss: 295.6353 - val_mse: 295.6353\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 228.4270 - mse: 228.4270 - val_loss: 295.6306 - val_mse: 295.6306\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 231.4626 - mse: 231.4626 - val_loss: 293.7902 - val_mse: 293.7902\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 233.4405 - mse: 233.4404 - val_loss: 294.0181 - val_mse: 294.0181\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 236.2539 - mse: 236.2539 - val_loss: 294.0128 - val_mse: 294.0128\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 231.5702 - mse: 231.5703 - val_loss: 293.7789 - val_mse: 293.7789\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 232.1163 - mse: 232.1162 - val_loss: 294.2797 - val_mse: 294.2798\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 230.7656 - mse: 230.7655 - val_loss: 293.1904 - val_mse: 293.1904\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 239.2478 - mse: 239.2477 - val_loss: 294.3190 - val_mse: 294.3190\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 233.2379 - mse: 233.2379 - val_loss: 293.2699 - val_mse: 293.2699\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 227.2515 - mse: 227.2516 - val_loss: 292.7994 - val_mse: 292.7994\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 230.8235 - mse: 230.8236 - val_loss: 293.1085 - val_mse: 293.1085\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 232.1701 - mse: 232.1701 - val_loss: 293.3965 - val_mse: 293.3965\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 233.2935 - mse: 233.2935 - val_loss: 292.9198 - val_mse: 292.9198\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 230.7507 - mse: 230.7507 - val_loss: 292.4318 - val_mse: 292.4317\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 231.1678 - mse: 231.1678 - val_loss: 293.2546 - val_mse: 293.2546\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 231.1150 - mse: 231.1150 - val_loss: 292.4306 - val_mse: 292.4306\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 231.9952 - mse: 231.9951 - val_loss: 293.3400 - val_mse: 293.3400\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 234.3760 - mse: 234.3760 - val_loss: 293.6978 - val_mse: 293.6978\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 233.0520 - mse: 233.0520 - val_loss: 295.3822 - val_mse: 295.3821\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 234.8796 - mse: 234.8796 - val_loss: 295.7411 - val_mse: 295.7411\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 226.5378 - mse: 226.5378 - val_loss: 295.4882 - val_mse: 295.4882\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 300us/sample - loss: 230.6724 - mse: 230.6724 - val_loss: 295.4716 - val_mse: 295.4716\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 228.9042 - mse: 228.9041 - val_loss: 293.5860 - val_mse: 293.5859\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 234.0437 - mse: 234.0438 - val_loss: 295.0338 - val_mse: 295.0338\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 228.9744 - mse: 228.9745 - val_loss: 294.8509 - val_mse: 294.8509\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 225.7079 - mse: 225.7080 - val_loss: 293.9899 - val_mse: 293.9899\n",
      "[CV] ....................................... nl=0, nn=2, total=  46.2s\n",
      "[CV] nl=0, nn=2 ......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 605us/sample - loss: 477.6645 - mse: 477.6646 - val_loss: 667.2606 - val_mse: 667.2606\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 440.5154 - mse: 440.5155 - val_loss: 618.6818 - val_mse: 618.6818\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 396.5541 - mse: 396.5541 - val_loss: 555.6858 - val_mse: 555.6857\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 349.2001 - mse: 349.2001 - val_loss: 494.7915 - val_mse: 494.7914\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 306.7534 - mse: 306.7534 - val_loss: 445.9353 - val_mse: 445.9354\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 274.5671 - mse: 274.5671 - val_loss: 406.2696 - val_mse: 406.2696\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 255.9033 - mse: 255.9032 - val_loss: 383.1047 - val_mse: 383.1047\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 242.2094 - mse: 242.2095 - val_loss: 365.0320 - val_mse: 365.0320\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 231.5684 - mse: 231.5683 - val_loss: 348.8743 - val_mse: 348.8743\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 226.2718 - mse: 226.2718 - val_loss: 341.1539 - val_mse: 341.1540\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 224.5019 - mse: 224.5020 - val_loss: 332.4090 - val_mse: 332.4090\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 314us/sample - loss: 215.7859 - mse: 215.7859 - val_loss: 324.9125 - val_mse: 324.9125\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 214.5378 - mse: 214.5377 - val_loss: 318.6757 - val_mse: 318.6757\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 208.0594 - mse: 208.0593 - val_loss: 314.2878 - val_mse: 314.2879\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 211.3436 - mse: 211.3436 - val_loss: 311.4678 - val_mse: 311.4678\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 206.2280 - mse: 206.228 - 1s 270us/sample - loss: 206.7255 - mse: 206.7255 - val_loss: 309.0781 - val_mse: 309.0781\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 208.6258 - mse: 208.6259 - val_loss: 307.7839 - val_mse: 307.7838\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 211.2850 - mse: 211.2850 - val_loss: 307.5678 - val_mse: 307.5678\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 207.9632 - mse: 207.9632 - val_loss: 303.8094 - val_mse: 303.8093\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 204.9362 - mse: 204.9362 - val_loss: 302.3276 - val_mse: 302.3277\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 203.0427 - mse: 203.0427 - val_loss: 302.0490 - val_mse: 302.0490\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 209.0924 - mse: 209.0923 - val_loss: 302.9308 - val_mse: 302.9308\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 206.1014 - mse: 206.1015 - val_loss: 300.2603 - val_mse: 300.2603\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 202.5052 - mse: 202.5052 - val_loss: 302.1828 - val_mse: 302.1828\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 206.1660 - mse: 206.1661 - val_loss: 298.4232 - val_mse: 298.4231\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 200.4824 - mse: 200.4823 - val_loss: 298.2300 - val_mse: 298.2300\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 202.5121 - mse: 202.5121 - val_loss: 299.5596 - val_mse: 299.5597\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 204.1514 - mse: 204.1515 - val_loss: 297.8974 - val_mse: 297.8974\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 205.6729 - mse: 205.6729 - val_loss: 296.7196 - val_mse: 296.7196\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 209.3578 - mse: 209.3578 - val_loss: 296.5193 - val_mse: 296.5193\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 201.8980 - mse: 201.8980 - val_loss: 297.8910 - val_mse: 297.8910\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 204.8259 - mse: 204.8260 - val_loss: 296.3547 - val_mse: 296.3547\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 200.8141 - mse: 200.8141 - val_loss: 297.3376 - val_mse: 297.3377\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 202.6438 - mse: 202.6439 - val_loss: 297.3556 - val_mse: 297.3557\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 206.6341 - mse: 206.6341 - val_loss: 297.8506 - val_mse: 297.8506\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 204.6773 - mse: 204.6773 - val_loss: 297.4729 - val_mse: 297.4729\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 200.1056 - mse: 200.1055 - val_loss: 301.0107 - val_mse: 301.0107\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 202.3049 - mse: 202.3049 - val_loss: 297.7248 - val_mse: 297.7248\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 199.3608 - mse: 199.3608 - val_loss: 296.9123 - val_mse: 296.9122\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 197.8337 - mse: 197.8338 - val_loss: 297.5732 - val_mse: 297.5732\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 199.9927 - mse: 199.9928 - val_loss: 297.6521 - val_mse: 297.6522\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 202.6674 - mse: 202.6674 - val_loss: 298.0552 - val_mse: 298.0551\n",
      "[CV] ....................................... nl=0, nn=2, total=  34.6s\n",
      "[CV] nl=0, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 825us/sample - loss: 543.7646 - mse: 543.7647 - val_loss: 662.7678 - val_mse: 662.7678\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 490.2262 - mse: 490.2263 - val_loss: 608.2003 - val_mse: 608.2003\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 427.0881 - mse: 427.0881 - val_loss: 521.5131 - val_mse: 521.5132\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 366.8668 - mse: 366.8668 - val_loss: 437.6405 - val_mse: 437.6405\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 315.4105 - mse: 315.4105 - val_loss: 361.7796 - val_mse: 361.7796\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 276.8628 - mse: 276.8627 - val_loss: 330.2946 - val_mse: 330.2946\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 249.3710 - mse: 249.3711 - val_loss: 279.2539 - val_mse: 279.2539\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 234.6109 - mse: 234.6110 - val_loss: 269.9301 - val_mse: 269.9300\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 228.8172 - mse: 228.8172 - val_loss: 250.0409 - val_mse: 250.0408\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 214.1742 - mse: 214.1742 - val_loss: 240.1039 - val_mse: 240.1039\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 208.0280 - mse: 208.0279 - val_loss: 233.9299 - val_mse: 233.9300\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 214.6769 - mse: 214.6770 - val_loss: 227.6859 - val_mse: 227.6858\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 207.7301 - mse: 207.7301 - val_loss: 220.8369 - val_mse: 220.8369\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 206.2212 - mse: 206.2212 - val_loss: 219.8274 - val_mse: 219.8275\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 201.9633 - mse: 201.9633 - val_loss: 213.2714 - val_mse: 213.2714\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 202.9699 - mse: 202.9698 - val_loss: 212.3204 - val_mse: 212.3204\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 305us/sample - loss: 203.8945 - mse: 203.8945 - val_loss: 210.9007 - val_mse: 210.9007\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 197.5148 - mse: 197.5147 - val_loss: 208.9252 - val_mse: 208.9252\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 192.2839 - mse: 192.2839 - val_loss: 201.9914 - val_mse: 201.9914\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 298us/sample - loss: 194.3582 - mse: 194.3582 - val_loss: 201.5953 - val_mse: 201.5953\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 196.8733 - mse: 196.8732 - val_loss: 201.2460 - val_mse: 201.2460\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 196.3893 - mse: 196.3893 - val_loss: 197.4446 - val_mse: 197.4447\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 182.7619 - mse: 182.7619 - val_loss: 194.8791 - val_mse: 194.8791\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 186.9062 - mse: 186.9062 - val_loss: 195.8118 - val_mse: 195.8118\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 188.0187 - mse: 188.0188 - val_loss: 193.9900 - val_mse: 193.9901\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 184.9930 - mse: 184.9930 - val_loss: 190.5068 - val_mse: 190.5068\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 194.2752 - mse: 194.2752 - val_loss: 190.7905 - val_mse: 190.7905\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 304us/sample - loss: 184.4226 - mse: 184.4226 - val_loss: 192.6963 - val_mse: 192.6963\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 188.3394 - mse: 188.3394 - val_loss: 186.2471 - val_mse: 186.2471\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 188.9502 - mse: 188.9502 - val_loss: 186.5976 - val_mse: 186.5975\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 187.9397 - mse: 187.9397 - val_loss: 183.2125 - val_mse: 183.2125\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 185.5225 - mse: 185.5225 - val_loss: 181.0356 - val_mse: 181.0356\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 172.6621 - mse: 172.6621 - val_loss: 182.7656 - val_mse: 182.7656\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 175.2975 - mse: 175.2975 - val_loss: 178.9187 - val_mse: 178.9187\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 177.8502 - mse: 177.8503 - val_loss: 175.7090 - val_mse: 175.7090\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 178.0311 - mse: 178.0311 - val_loss: 176.4986 - val_mse: 176.4986\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 179.5159 - mse: 179.5159 - val_loss: 175.2744 - val_mse: 175.2744\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 180.3478 - mse: 180.3477 - val_loss: 178.2179 - val_mse: 178.2179\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 182.1838 - mse: 182.1838 - val_loss: 177.0370 - val_mse: 177.0370\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 176.7414 - mse: 176.7414 - val_loss: 178.4780 - val_mse: 178.4780\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 175.1890 - mse: 175.1891 - val_loss: 171.2176 - val_mse: 171.2176\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 170.4660 - mse: 170.4660 - val_loss: 173.6591 - val_mse: 173.6591\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 176.7630 - mse: 176.7629 - val_loss: 172.3994 - val_mse: 172.3994\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 180.0441 - mse: 180.0441 - val_loss: 171.1664 - val_mse: 171.1664\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 176.1875 - mse: 176.1875 - val_loss: 169.5799 - val_mse: 169.5799\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 169.2844 - mse: 169.2844 - val_loss: 173.4976 - val_mse: 173.4976\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 175.3738 - mse: 175.3738 - val_loss: 172.2611 - val_mse: 172.2611\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 174.9636 - mse: 174.9635 - val_loss: 168.3748 - val_mse: 168.3748\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 174.4163 - mse: 174.4163 - val_loss: 170.0114 - val_mse: 170.0114\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 169.9881 - mse: 169.988 - 1s 270us/sample - loss: 170.7576 - mse: 170.7576 - val_loss: 167.8335 - val_mse: 167.8335\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 164.5850 - mse: 164.5850 - val_loss: 166.2712 - val_mse: 166.2711\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 313us/sample - loss: 171.2711 - mse: 171.2711 - val_loss: 167.2539 - val_mse: 167.2539\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 170.3802 - mse: 170.3802 - val_loss: 163.9021 - val_mse: 163.9021\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 170.0532 - mse: 170.0532 - val_loss: 168.4067 - val_mse: 168.4068\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 315us/sample - loss: 169.9488 - mse: 169.9487 - val_loss: 166.3488 - val_mse: 166.3488\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 174.6944 - mse: 174.6944 - val_loss: 167.7732 - val_mse: 167.7732\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 307us/sample - loss: 166.1921 - mse: 166.1921 - val_loss: 167.7832 - val_mse: 167.7832\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 164.5339 - mse: 164.5339 - val_loss: 164.6836 - val_mse: 164.6836\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 169.6031 - mse: 169.6031 - val_loss: 163.8283 - val_mse: 163.8283\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 303us/sample - loss: 169.0886 - mse: 169.0886 - val_loss: 168.1426 - val_mse: 168.1426\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 162.2536 - mse: 162.2536 - val_loss: 165.6352 - val_mse: 165.6351\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 164.3963 - mse: 164.3963 - val_loss: 164.5971 - val_mse: 164.5971\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 170.6410 - mse: 170.6410 - val_loss: 166.6177 - val_mse: 166.6177\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 160.5685 - mse: 160.5685 - val_loss: 166.0796 - val_mse: 166.0796\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 305us/sample - loss: 163.6126 - mse: 163.6126 - val_loss: 163.4436 - val_mse: 163.4436\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 169.4177 - mse: 169.4177 - val_loss: 168.3581 - val_mse: 168.3581\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 289us/sample - loss: 163.5824 - mse: 163.5824 - val_loss: 166.1811 - val_mse: 166.1811\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 163.5921 - mse: 163.5920 - val_loss: 165.5434 - val_mse: 165.5434\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 164.7508 - mse: 164.7508 - val_loss: 162.4562 - val_mse: 162.4562\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 166.2591 - mse: 166.2591 - val_loss: 161.7975 - val_mse: 161.7975\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 160.1877 - mse: 160.1877 - val_loss: 160.8960 - val_mse: 160.8960\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 163.1534 - mse: 163.1534 - val_loss: 162.4457 - val_mse: 162.4457\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 169.7842 - mse: 169.7842 - val_loss: 163.1929 - val_mse: 163.1929\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 169.1937 - mse: 169.1937 - val_loss: 163.3046 - val_mse: 163.3046\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 163.2223 - mse: 163.2222 - val_loss: 160.8751 - val_mse: 160.8751\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 168.5405 - mse: 168.5405 - val_loss: 162.1377 - val_mse: 162.1377\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 162.0535 - mse: 162.0535 - val_loss: 162.3241 - val_mse: 162.3242\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 159.7609 - mse: 159.7609 - val_loss: 161.5570 - val_mse: 161.5570\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 165.8786 - mse: 165.8786 - val_loss: 162.3899 - val_mse: 162.3899\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 159.6864 - mse: 159.6863 - val_loss: 165.1366 - val_mse: 165.1365\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 162.1170 - mse: 162.1171 - val_loss: 163.2102 - val_mse: 163.2102\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 162.2576 - mse: 162.2577 - val_loss: 164.4508 - val_mse: 164.4507\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 164.8481 - mse: 164.8481 - val_loss: 163.7880 - val_mse: 163.7880\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 160.2225 - mse: 160.2225 - val_loss: 160.1844 - val_mse: 160.1844\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 167.9509 - mse: 167.9509 - val_loss: 161.5487 - val_mse: 161.5487\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 162.2011 - mse: 162.2011 - val_loss: 165.5084 - val_mse: 165.5084\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 161.3434 - mse: 161.3435 - val_loss: 164.4106 - val_mse: 164.4106\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 159.5495 - mse: 159.5495 - val_loss: 164.3779 - val_mse: 164.3779\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 161.5461 - mse: 161.5461 - val_loss: 162.4879 - val_mse: 162.4879\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 160.6264 - mse: 160.6264 - val_loss: 160.7552 - val_mse: 160.7551\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 166.1207 - mse: 166.1207 - val_loss: 165.2875 - val_mse: 165.2876\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 157.1413 - mse: 157.1413 - val_loss: 160.0566 - val_mse: 160.0566\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 152.6021 - mse: 152.6022 - val_loss: 157.8863 - val_mse: 157.8864\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 163.0570 - mse: 163.0571 - val_loss: 160.6161 - val_mse: 160.6161\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 161.4599 - mse: 161.4599 - val_loss: 158.5221 - val_mse: 158.5221\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 158.7900 - mse: 158.7900 - val_loss: 155.3619 - val_mse: 155.3619\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 157.9242 - mse: 157.9242 - val_loss: 156.1461 - val_mse: 156.1461\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 305us/sample - loss: 171.3107 - mse: 171.3108 - val_loss: 158.9389 - val_mse: 158.9389\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 158.9592 - mse: 158.9592 - val_loss: 157.2869 - val_mse: 157.2869\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 163.7810 - mse: 163.7810 - val_loss: 152.8363 - val_mse: 152.8363\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 159.2660 - mse: 159.2660 - val_loss: 151.1534 - val_mse: 151.1534\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 156.7521 - mse: 156.7521 - val_loss: 152.9671 - val_mse: 152.9670\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 300us/sample - loss: 157.3113 - mse: 157.3112 - val_loss: 155.4584 - val_mse: 155.4585\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 157.6637 - mse: 157.6637 - val_loss: 154.3816 - val_mse: 154.3815\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 155.5019 - mse: 155.5018 - val_loss: 151.6364 - val_mse: 151.6364\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 144.6725 - mse: 144.6725 - val_loss: 153.3399 - val_mse: 153.3399\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 159.6876 - mse: 159.6877 - val_loss: 152.2233 - val_mse: 152.2233\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 150.2934 - mse: 150.2934 - val_loss: 153.5899 - val_mse: 153.5899\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 162.4132 - mse: 162.4132 - val_loss: 151.3594 - val_mse: 151.3593\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 156.0494 - mse: 156.0494 - val_loss: 151.3745 - val_mse: 151.3745\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 154.5350 - mse: 154.5351 - val_loss: 152.8178 - val_mse: 152.8178\n",
      "[CV] ....................................... nl=0, nn=2, total= 1.5min\n",
      "[CV] nl=0, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 662us/sample - loss: 477.9187 - mse: 477.9188 - val_loss: 530.6732 - val_mse: 530.6732\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 428.5688 - mse: 428.5690 - val_loss: 481.8284 - val_mse: 481.8285\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 362.8366 - mse: 362.8366 - val_loss: 403.0643 - val_mse: 403.0644\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 307.5659 - mse: 307.5658 - val_loss: 333.5630 - val_mse: 333.5629\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 265.3391 - mse: 265.3391 - val_loss: 272.3339 - val_mse: 272.3340\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 231.5127 - mse: 231.5127 - val_loss: 239.9981 - val_mse: 239.9981\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 214.2166 - mse: 214.2166 - val_loss: 218.0227 - val_mse: 218.0228\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 202.3735 - mse: 202.3734 - val_loss: 202.5135 - val_mse: 202.5136\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 197.2291 - mse: 197.2290 - val_loss: 191.4812 - val_mse: 191.4812\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 190.8737 - mse: 190.8737 - val_loss: 188.7482 - val_mse: 188.7481\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 186.8698 - mse: 186.8697 - val_loss: 180.6381 - val_mse: 180.6381\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 186.8247 - mse: 186.8247 - val_loss: 174.7014 - val_mse: 174.7014\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 182.6189 - mse: 182.6189 - val_loss: 170.7165 - val_mse: 170.7165\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 180.3954 - mse: 180.3954 - val_loss: 167.2090 - val_mse: 167.2090\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 180.7099 - mse: 180.7099 - val_loss: 169.5784 - val_mse: 169.5784\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 181.3658 - mse: 181.3658 - val_loss: 165.0724 - val_mse: 165.0724\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 183.6204 - mse: 183.6204 - val_loss: 163.8031 - val_mse: 163.8031\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 178.2643 - mse: 178.2644 - val_loss: 161.7534 - val_mse: 161.7534\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 178.9274 - mse: 178.9274 - val_loss: 159.1493 - val_mse: 159.1493\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 169.8265 - mse: 169.8266 - val_loss: 157.1425 - val_mse: 157.1425\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 174.9760 - mse: 174.9760 - val_loss: 155.7579 - val_mse: 155.7579\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 169.1980 - mse: 169.1980 - val_loss: 154.9491 - val_mse: 154.9491\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 175.0194 - mse: 175.0193 - val_loss: 154.6718 - val_mse: 154.6718\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 300us/sample - loss: 176.0594 - mse: 176.0594 - val_loss: 154.6285 - val_mse: 154.6285\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 176.8582 - mse: 176.8582 - val_loss: 152.7400 - val_mse: 152.7400\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 169.0954 - mse: 169.0954 - val_loss: 153.2110 - val_mse: 153.2110\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 171.9291 - mse: 171.9291 - val_loss: 152.0736 - val_mse: 152.0736\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 169.5501 - mse: 169.5501 - val_loss: 149.8428 - val_mse: 149.8428\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 174.0546 - mse: 174.0545 - val_loss: 153.2034 - val_mse: 153.2034\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 169.1582 - mse: 169.1583 - val_loss: 148.9980 - val_mse: 148.9981\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 174.2644 - mse: 174.2643 - val_loss: 150.2651 - val_mse: 150.2651\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 170.7435 - mse: 170.7435 - val_loss: 151.7176 - val_mse: 151.7177\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 171.3244 - mse: 171.3244 - val_loss: 150.3439 - val_mse: 150.3439\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 171.8311 - mse: 171.8311 - val_loss: 147.9955 - val_mse: 147.9954\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 171.7365 - mse: 171.7366 - val_loss: 150.0748 - val_mse: 150.0748\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 166.7995 - mse: 166.7995 - val_loss: 151.0347 - val_mse: 151.0347\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 160.6495 - mse: 160.6494 - val_loss: 149.1710 - val_mse: 149.1710\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 171.1326 - mse: 171.1325 - val_loss: 149.7106 - val_mse: 149.7106\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 165.6057 - mse: 165.6057 - val_loss: 147.8943 - val_mse: 147.8943\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 168.2375 - mse: 168.2375 - val_loss: 146.7449 - val_mse: 146.7449\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 168.5455 - mse: 168.5454 - val_loss: 148.4620 - val_mse: 148.4620\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 164.5428 - mse: 164.5428 - val_loss: 146.4104 - val_mse: 146.4104\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 160.4501 - mse: 160.4501 - val_loss: 148.0996 - val_mse: 148.0996\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 162.0546 - mse: 162.0545 - val_loss: 144.5810 - val_mse: 144.5810\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 162.2533 - mse: 162.2533 - val_loss: 146.8346 - val_mse: 146.8346\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 160.7939 - mse: 160.7939 - val_loss: 149.3991 - val_mse: 149.3991\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 167.4332 - mse: 167.4332 - val_loss: 145.7799 - val_mse: 145.7799\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 167.5583 - mse: 167.5583 - val_loss: 143.8636 - val_mse: 143.8636\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 163.8262 - mse: 163.8263 - val_loss: 146.6141 - val_mse: 146.6141\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 165.4130 - mse: 165.4130 - val_loss: 143.7191 - val_mse: 143.7191\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 160.5927 - mse: 160.5927 - val_loss: 142.0380 - val_mse: 142.0381\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 170.3769 - mse: 170.3769 - val_loss: 143.9332 - val_mse: 143.9332\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 169.6731 - mse: 169.6731 - val_loss: 142.0390 - val_mse: 142.0390\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 167.4981 - mse: 167.4981 - val_loss: 141.8352 - val_mse: 141.8352\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 160.0638 - mse: 160.0638 - val_loss: 142.6297 - val_mse: 142.6297\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 160.4601 - mse: 160.4602 - val_loss: 140.7976 - val_mse: 140.7976\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 160.4880 - mse: 160.4881 - val_loss: 142.6910 - val_mse: 142.6909\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 163.8458 - mse: 163.8458 - val_loss: 142.2242 - val_mse: 142.2242\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 164.3016 - mse: 164.3016 - val_loss: 139.7297 - val_mse: 139.7296\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 162.4734 - mse: 162.4734 - val_loss: 139.9199 - val_mse: 139.9199\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 161.9521 - mse: 161.9521 - val_loss: 138.8869 - val_mse: 138.8869\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 155.9425 - mse: 155.9425 - val_loss: 143.5345 - val_mse: 143.5345\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 158.4300 - mse: 158.4300 - val_loss: 137.7275 - val_mse: 137.7276\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 160.7570 - mse: 160.7570 - val_loss: 138.5800 - val_mse: 138.5799\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 252us/sample - loss: 153.3655 - mse: 153.3655 - val_loss: 138.8527 - val_mse: 138.8526\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 164.6241 - mse: 164.6240 - val_loss: 138.8572 - val_mse: 138.8572\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 160.3590 - mse: 160.3590 - val_loss: 139.3288 - val_mse: 139.3288\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 160.1237 - mse: 160.1237 - val_loss: 138.8399 - val_mse: 138.8399\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 160.2617 - mse: 160.261 - 1s 216us/sample - loss: 157.6929 - mse: 157.6929 - val_loss: 137.9297 - val_mse: 137.9296\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 159.0626 - mse: 159.0626 - val_loss: 137.0059 - val_mse: 137.0059\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 162.3443 - mse: 162.3443 - val_loss: 136.6055 - val_mse: 136.6055\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 161.2909 - mse: 161.2909 - val_loss: 138.1929 - val_mse: 138.1929\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 154.9554 - mse: 154.9554 - val_loss: 137.8864 - val_mse: 137.8864\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 156.9774 - mse: 156.9774 - val_loss: 139.3782 - val_mse: 139.3782\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 159.5344 - mse: 159.5344 - val_loss: 138.4887 - val_mse: 138.4887\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 156.1253 - mse: 156.1253 - val_loss: 137.0440 - val_mse: 137.0440\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 155.1439 - mse: 155.1440 - val_loss: 135.7115 - val_mse: 135.7115\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 159.9912 - mse: 159.9912 - val_loss: 137.8329 - val_mse: 137.8329\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 151.8183 - mse: 151.8183 - val_loss: 139.3739 - val_mse: 139.3738\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 154.4073 - mse: 154.4074 - val_loss: 137.1437 - val_mse: 137.1438\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 151.8854 - mse: 151.8854 - val_loss: 135.9114 - val_mse: 135.9114\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 154.3443 - mse: 154.3443 - val_loss: 136.6816 - val_mse: 136.6815\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 151.7633 - mse: 151.7633 - val_loss: 135.9388 - val_mse: 135.9388\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 157.2773 - mse: 157.2772 - val_loss: 135.7011 - val_mse: 135.7011\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 156.7198 - mse: 156.7198 - val_loss: 136.6859 - val_mse: 136.6859\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 156.2260 - mse: 156.2260 - val_loss: 137.8234 - val_mse: 137.8234\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 154.3355 - mse: 154.3356 - val_loss: 136.3039 - val_mse: 136.3039\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 155.1845 - mse: 155.1845 - val_loss: 137.0908 - val_mse: 137.0908\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 152.3900 - mse: 152.3901 - val_loss: 134.9539 - val_mse: 134.9539\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 154.4759 - mse: 154.4759 - val_loss: 136.3175 - val_mse: 136.3176\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 155.0167 - mse: 155.0167 - val_loss: 139.8216 - val_mse: 139.8216\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 151.8050 - mse: 151.8050 - val_loss: 135.8319 - val_mse: 135.8320\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 152.9693 - mse: 152.9693 - val_loss: 134.7927 - val_mse: 134.7926\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 159.5552 - mse: 159.5551 - val_loss: 135.6849 - val_mse: 135.6849\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 158.5909 - mse: 158.5909 - val_loss: 140.1121 - val_mse: 140.1121\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 153.8169 - mse: 153.8169 - val_loss: 138.3589 - val_mse: 138.3589\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 155.8707 - mse: 155.8706 - val_loss: 133.8707 - val_mse: 133.8707\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 152.0840 - mse: 152.0839 - val_loss: 137.7192 - val_mse: 137.7191\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 148.6836 - mse: 148.6836 - val_loss: 135.6209 - val_mse: 135.6209\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 153.4114 - mse: 153.4114 - val_loss: 132.8619 - val_mse: 132.8619\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 152.1026 - mse: 152.1026 - val_loss: 134.6056 - val_mse: 134.6056\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 144.5751 - mse: 144.5751 - val_loss: 132.8982 - val_mse: 132.8981\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 151.6296 - mse: 151.6296 - val_loss: 142.4654 - val_mse: 142.4654\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 154.1287 - mse: 154.1287 - val_loss: 131.3822 - val_mse: 131.3822\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 149.4174 - mse: 149.4174 - val_loss: 144.0553 - val_mse: 144.0553\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 150.9686 - mse: 150.9687 - val_loss: 131.3468 - val_mse: 131.3468\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 150.5992 - mse: 150.5992 - val_loss: 132.4763 - val_mse: 132.4763\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 151.8636 - mse: 151.8636 - val_loss: 130.6623 - val_mse: 130.6623\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 147.3301 - mse: 147.3300 - val_loss: 131.0221 - val_mse: 131.0221\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 148.5333 - mse: 148.5332 - val_loss: 129.6457 - val_mse: 129.6456\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 150.8361 - mse: 150.8361 - val_loss: 130.5567 - val_mse: 130.5567\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 150.4920 - mse: 150.4920 - val_loss: 131.5102 - val_mse: 131.5102\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 146.1191 - mse: 146.1191 - val_loss: 130.9733 - val_mse: 130.9733\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 147.7267 - mse: 147.7267 - val_loss: 131.2770 - val_mse: 131.2769\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 153.2027 - mse: 153.2027 - val_loss: 132.1183 - val_mse: 132.1183\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 150.3554 - mse: 150.3554 - val_loss: 135.5541 - val_mse: 135.5541\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 146.6847 - mse: 146.6847 - val_loss: 131.6484 - val_mse: 131.6484\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 303us/sample - loss: 151.6166 - mse: 151.6166 - val_loss: 130.8573 - val_mse: 130.8573\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 148.0597 - mse: 148.0598 - val_loss: 130.5950 - val_mse: 130.5951\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 280us/sample - loss: 156.3601 - mse: 156.3601 - val_loss: 131.1776 - val_mse: 131.1776\n",
      "[CV] ....................................... nl=0, nn=2, total= 1.5min\n",
      "[CV] nl=0, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 563us/sample - loss: 455.7976 - mse: 455.7976 - val_loss: 620.1799 - val_mse: 620.1798\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 405.5016 - mse: 405.5015 - val_loss: 568.4156 - val_mse: 568.4156\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 336.5397 - mse: 336.5397 - val_loss: 459.4224 - val_mse: 459.4224\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 346us/sample - loss: 288.0857 - mse: 288.0857 - val_loss: 384.2993 - val_mse: 384.2993\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 257.8332 - mse: 257.8331 - val_loss: 336.3330 - val_mse: 336.3330\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 231.6323 - mse: 231.6323 - val_loss: 312.3256 - val_mse: 312.3256\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 223.0085 - mse: 223.0084 - val_loss: 298.4259 - val_mse: 298.4259\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 209.1790 - mse: 209.1790 - val_loss: 289.1235 - val_mse: 289.1236\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 208.7214 - mse: 208.7214 - val_loss: 284.4487 - val_mse: 284.4486\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 207.0488 - mse: 207.0488 - val_loss: 282.1463 - val_mse: 282.1463\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 205.0999 - mse: 205.0999 - val_loss: 279.0230 - val_mse: 279.0229\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 202.5008 - mse: 202.5008 - val_loss: 276.6407 - val_mse: 276.6407\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 299us/sample - loss: 202.8184 - mse: 202.8184 - val_loss: 275.4597 - val_mse: 275.4597\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 197.6986 - mse: 197.6986 - val_loss: 273.2635 - val_mse: 273.2635\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 201.9317 - mse: 201.9318 - val_loss: 278.9563 - val_mse: 278.9564\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 304us/sample - loss: 196.7965 - mse: 196.7964 - val_loss: 269.8874 - val_mse: 269.8875\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 192.6240 - mse: 192.6240 - val_loss: 268.3891 - val_mse: 268.3890\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 192.0149 - mse: 192.0148 - val_loss: 269.4326 - val_mse: 269.4326\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 191.2493 - mse: 191.2493 - val_loss: 265.3708 - val_mse: 265.3708\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 191.0783 - mse: 191.0784 - val_loss: 269.7552 - val_mse: 269.7552\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 307us/sample - loss: 189.2683 - mse: 189.2684 - val_loss: 263.7340 - val_mse: 263.7340\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 190.4674 - mse: 190.4674 - val_loss: 264.8941 - val_mse: 264.8941\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 188.9609 - mse: 188.9608 - val_loss: 260.2086 - val_mse: 260.2086\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 184.5350 - mse: 184.5350 - val_loss: 259.8786 - val_mse: 259.8786\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 185.7673 - mse: 185.7673 - val_loss: 259.2875 - val_mse: 259.2875\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 186.7470 - mse: 186.7469 - val_loss: 258.0456 - val_mse: 258.0456\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 319us/sample - loss: 188.4230 - mse: 188.4229 - val_loss: 258.4566 - val_mse: 258.4566\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 185.3187 - mse: 185.3188 - val_loss: 257.8107 - val_mse: 257.8108\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 179.6824 - mse: 179.6824 - val_loss: 259.9240 - val_mse: 259.9240\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 181.3649 - mse: 181.3649 - val_loss: 255.2860 - val_mse: 255.2860\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 180.9628 - mse: 180.9628 - val_loss: 254.4454 - val_mse: 254.4454\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 182.7542 - mse: 182.7542 - val_loss: 253.3377 - val_mse: 253.3377\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 306us/sample - loss: 179.4772 - mse: 179.4772 - val_loss: 252.6905 - val_mse: 252.6905\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 181.1114 - mse: 181.1114 - val_loss: 251.2679 - val_mse: 251.2679\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 178.2528 - mse: 178.2528 - val_loss: 247.9986 - val_mse: 247.9986\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 178.9282 - mse: 178.9282 - val_loss: 247.0803 - val_mse: 247.0803\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 178.1824 - mse: 178.1824 - val_loss: 253.9326 - val_mse: 253.9326\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 174.6386 - mse: 174.6386 - val_loss: 245.9896 - val_mse: 245.9896\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 175.7985 - mse: 175.7985 - val_loss: 248.3699 - val_mse: 248.3699\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 180.5969 - mse: 180.5969 - val_loss: 246.8918 - val_mse: 246.8918\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 178.9524 - mse: 178.9524 - val_loss: 242.2246 - val_mse: 242.2247\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 178.1678 - mse: 178.1679 - val_loss: 238.2882 - val_mse: 238.2882\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 175.4300 - mse: 175.4300 - val_loss: 237.0585 - val_mse: 237.0585\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 171.5413 - mse: 171.5413 - val_loss: 234.1718 - val_mse: 234.1718\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 180.6439 - mse: 180.6438 - val_loss: 235.9598 - val_mse: 235.9598\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 173.8307 - mse: 173.8307 - val_loss: 229.8753 - val_mse: 229.8753\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 169.5337 - mse: 169.5337 - val_loss: 233.6126 - val_mse: 233.6127\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 172.2001 - mse: 172.2001 - val_loss: 234.4196 - val_mse: 234.4196\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 171.8101 - mse: 171.8101 - val_loss: 226.8988 - val_mse: 226.8988\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 171.2780 - mse: 171.2780 - val_loss: 225.6607 - val_mse: 225.6608\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 167.5262 - mse: 167.5262 - val_loss: 228.4270 - val_mse: 228.4270\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 171.8164 - mse: 171.8165 - val_loss: 225.1040 - val_mse: 225.1039\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 166.6592 - mse: 166.6592 - val_loss: 228.6051 - val_mse: 228.6051\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 168.9563 - mse: 168.9563 - val_loss: 225.4433 - val_mse: 225.4433\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 166.1981 - mse: 166.1981 - val_loss: 227.6121 - val_mse: 227.6121\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 166.4689 - mse: 166.4688 - val_loss: 220.8066 - val_mse: 220.8066\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 166.2674 - mse: 166.2674 - val_loss: 219.9646 - val_mse: 219.9646\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 163.8911 - mse: 163.8911 - val_loss: 220.3315 - val_mse: 220.3315\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 161.8710 - mse: 161.8710 - val_loss: 218.1211 - val_mse: 218.1211\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 166.8430 - mse: 166.8430 - val_loss: 219.5758 - val_mse: 219.5758\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 160.3096 - mse: 160.3096 - val_loss: 220.0601 - val_mse: 220.0601\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 169.2700 - mse: 169.2701 - val_loss: 216.6891 - val_mse: 216.6891\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 161.4107 - mse: 161.4108 - val_loss: 217.1322 - val_mse: 217.1322\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 158.3048 - mse: 158.3049 - val_loss: 213.6386 - val_mse: 213.6386\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 160.6404 - mse: 160.6404 - val_loss: 222.7406 - val_mse: 222.7406\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 162.7153 - mse: 162.7153 - val_loss: 220.6590 - val_mse: 220.6590\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 162.3120 - mse: 162.3120 - val_loss: 211.9345 - val_mse: 211.9345\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 160.9149 - mse: 160.9149 - val_loss: 216.3565 - val_mse: 216.3565\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 173.4042 - mse: 173.4042 - val_loss: 219.1996 - val_mse: 219.1996\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 137.6209 - mse: 137.621 - 1s 221us/sample - loss: 158.7111 - mse: 158.7112 - val_loss: 213.5969 - val_mse: 213.5969\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 156.6103 - mse: 156.6103 - val_loss: 216.5907 - val_mse: 216.5907\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 164.0175 - mse: 164.017 - 1s 248us/sample - loss: 158.8172 - mse: 158.8171 - val_loss: 210.7344 - val_mse: 210.7344\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 158.8762 - mse: 158.8762 - val_loss: 215.0908 - val_mse: 215.0908\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 157.1501 - mse: 157.1501 - val_loss: 221.9636 - val_mse: 221.9636\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 160.3119 - mse: 160.3119 - val_loss: 210.6381 - val_mse: 210.6381\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 159.4954 - mse: 159.4954 - val_loss: 212.6924 - val_mse: 212.6924\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 162.6212 - mse: 162.6212 - val_loss: 207.6941 - val_mse: 207.6941\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 152.8536 - mse: 152.8537 - val_loss: 208.7579 - val_mse: 208.7579\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 158.5474 - mse: 158.5475 - val_loss: 212.8184 - val_mse: 212.8184\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 157.6604 - mse: 157.6604 - val_loss: 207.0556 - val_mse: 207.0556\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 153.5338 - mse: 153.5338 - val_loss: 206.8304 - val_mse: 206.8304\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 156.1253 - mse: 156.1253 - val_loss: 206.6645 - val_mse: 206.6645\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 157.5032 - mse: 157.5032 - val_loss: 204.0657 - val_mse: 204.0657\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 153.8501 - mse: 153.8502 - val_loss: 203.5799 - val_mse: 203.5800\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 163.4805 - mse: 163.4805 - val_loss: 211.8523 - val_mse: 211.8523\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 151.1022 - mse: 151.1022 - val_loss: 213.7733 - val_mse: 213.7733\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 152.4731 - mse: 152.4731 - val_loss: 202.9590 - val_mse: 202.9590\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 145.5533 - mse: 145.5533 - val_loss: 203.4045 - val_mse: 203.4045\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 160.0752 - mse: 160.0752 - val_loss: 202.3667 - val_mse: 202.3667\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 155.1954 - mse: 155.1955 - val_loss: 202.0992 - val_mse: 202.0992\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 353us/sample - loss: 152.0078 - mse: 152.0078 - val_loss: 200.4095 - val_mse: 200.4095\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 314us/sample - loss: 153.4308 - mse: 153.4308 - val_loss: 200.6752 - val_mse: 200.6751\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 149.8701 - mse: 149.8701 - val_loss: 207.1751 - val_mse: 207.1751\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 151.8606 - mse: 151.8606 - val_loss: 208.8093 - val_mse: 208.8093\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 148.4527 - mse: 148.4527 - val_loss: 201.5014 - val_mse: 201.5014\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 151.7866 - mse: 151.7865 - val_loss: 198.1450 - val_mse: 198.1450\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 151.1114 - mse: 151.1114 - val_loss: 198.9546 - val_mse: 198.9547\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 148.1019 - mse: 148.1019 - val_loss: 199.4764 - val_mse: 199.4764\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 155.8606 - mse: 155.8605 - val_loss: 200.2835 - val_mse: 200.2835\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 155.1664 - mse: 155.1664 - val_loss: 199.1115 - val_mse: 199.1115\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 142.4243 - mse: 142.4243 - val_loss: 202.8677 - val_mse: 202.8677\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 156.9999 - mse: 156.9999 - val_loss: 199.8609 - val_mse: 199.8609\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 152.5020 - mse: 152.5020 - val_loss: 196.0015 - val_mse: 196.0015\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 148.8068 - mse: 148.8068 - val_loss: 197.0309 - val_mse: 197.0310\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 147.4682 - mse: 147.4682 - val_loss: 214.4101 - val_mse: 214.4101\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 146.0962 - mse: 146.0961 - val_loss: 196.6775 - val_mse: 196.6775\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 150.3912 - mse: 150.3912 - val_loss: 216.7534 - val_mse: 216.7534\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 154.7971 - mse: 154.7971 - val_loss: 217.9796 - val_mse: 217.9796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 151.4017 - mse: 151.4016 - val_loss: 196.4925 - val_mse: 196.4924\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 151.5097 - mse: 151.5097 - val_loss: 197.5772 - val_mse: 197.5772\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 148.1557 - mse: 148.1557 - val_loss: 196.4600 - val_mse: 196.4600\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 153.2659 - mse: 153.2659 - val_loss: 199.0342 - val_mse: 199.0342\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 143.2798 - mse: 143.2798 - val_loss: 195.0959 - val_mse: 195.0959\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 150.3924 - mse: 150.3925 - val_loss: 193.7363 - val_mse: 193.7364\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 146.1326 - mse: 146.1325 - val_loss: 203.9022 - val_mse: 203.9022\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 151.5307 - mse: 151.5307 - val_loss: 197.8285 - val_mse: 197.8285\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 151.9361 - mse: 151.9361 - val_loss: 193.9223 - val_mse: 193.9223\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 150.5058 - mse: 150.5058 - val_loss: 193.4327 - val_mse: 193.4327\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.0931 - mse: 147.0931 - val_loss: 196.5514 - val_mse: 196.5514\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 152.5791 - mse: 152.5791 - val_loss: 204.1468 - val_mse: 204.1469\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 153.0705 - mse: 153.0704 - val_loss: 195.3600 - val_mse: 195.3600\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 148.0296 - mse: 148.0296 - val_loss: 195.9986 - val_mse: 195.9986\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 147.7587 - mse: 147.7587 - val_loss: 197.0549 - val_mse: 197.0549\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 147.5367 - mse: 147.5366 - val_loss: 209.5971 - val_mse: 209.5971\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 145.8361 - mse: 145.8361 - val_loss: 191.9499 - val_mse: 191.9499\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 147.0855 - mse: 147.0854 - val_loss: 200.2334 - val_mse: 200.2334\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 310us/sample - loss: 144.6837 - mse: 144.6837 - val_loss: 198.9591 - val_mse: 198.9591\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 142.6738 - mse: 142.6739 - val_loss: 198.1144 - val_mse: 198.1144\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 155.1947 - mse: 155.1947 - val_loss: 200.8368 - val_mse: 200.8368\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 146.4343 - mse: 146.4343 - val_loss: 194.9019 - val_mse: 194.9019\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 148.9629 - mse: 148.9629 - val_loss: 192.2678 - val_mse: 192.2677\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 146.3090 - mse: 146.3089 - val_loss: 192.8532 - val_mse: 192.8532\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 145.3354 - mse: 145.3354 - val_loss: 191.0985 - val_mse: 191.0985\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 143.4749 - mse: 143.4748 - val_loss: 191.8978 - val_mse: 191.8978\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 150.2772 - mse: 150.2772 - val_loss: 233.3789 - val_mse: 233.3790\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 151.4143 - mse: 151.4143 - val_loss: 199.1986 - val_mse: 199.1986\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 145.9872 - mse: 145.9873 - val_loss: 191.6335 - val_mse: 191.6335\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 149.0736 - mse: 149.0736 - val_loss: 193.0258 - val_mse: 193.0258\n",
      "Epoch 139/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 145.4502 - mse: 145.4502 - val_loss: 195.4655 - val_mse: 195.4655\n",
      "Epoch 140/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 144.6614 - mse: 144.6614 - val_loss: 196.8094 - val_mse: 196.8094\n",
      "Epoch 141/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 147.1318 - mse: 147.1318 - val_loss: 234.3663 - val_mse: 234.3662\n",
      "Epoch 142/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 150.8278 - mse: 150.8277 - val_loss: 196.2858 - val_mse: 196.2858\n",
      "Epoch 143/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 145.2148 - mse: 145.2148 - val_loss: 201.7411 - val_mse: 201.7411\n",
      "[CV] ....................................... nl=0, nn=2, total= 1.9min\n",
      "[CV] nl=0, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 682us/sample - loss: 503.8974 - mse: 503.8976 - val_loss: 665.4183 - val_mse: 665.4182\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 453.0739 - mse: 453.0738 - val_loss: 595.2256 - val_mse: 595.2257\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 402.9041 - mse: 402.9039 - val_loss: 518.3036 - val_mse: 518.3037\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 350.5957 - mse: 350.5958 - val_loss: 444.9803 - val_mse: 444.9802\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 300.3688 - mse: 300.3689 - val_loss: 372.3351 - val_mse: 372.3351\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 264.0009 - mse: 264.0009 - val_loss: 328.5609 - val_mse: 328.5610\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 245.8115 - mse: 245.8115 - val_loss: 301.6977 - val_mse: 301.6977\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 229.4528 - mse: 229.4527 - val_loss: 288.5764 - val_mse: 288.5763\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 228.1584 - mse: 228.1585 - val_loss: 276.2799 - val_mse: 276.2799\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 219.3432 - mse: 219.3432 - val_loss: 271.1302 - val_mse: 271.1302\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 220.0701 - mse: 220.0702 - val_loss: 265.1715 - val_mse: 265.1715\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 211.7768 - mse: 211.7768 - val_loss: 264.8288 - val_mse: 264.8288\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 216.4076 - mse: 216.4076 - val_loss: 259.1650 - val_mse: 259.1650\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 307us/sample - loss: 211.9995 - mse: 211.9994 - val_loss: 256.7440 - val_mse: 256.7440\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 213.9452 - mse: 213.9452 - val_loss: 255.0500 - val_mse: 255.0501\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 330us/sample - loss: 213.7784 - mse: 213.7784 - val_loss: 252.8460 - val_mse: 252.8460\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 208.5244 - mse: 208.5244 - val_loss: 248.7814 - val_mse: 248.7814\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 204.8063 - mse: 204.8062 - val_loss: 248.2802 - val_mse: 248.2803\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 203.0662 - mse: 203.0662 - val_loss: 247.1967 - val_mse: 247.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 205.7954 - mse: 205.7954 - val_loss: 241.4233 - val_mse: 241.4233\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 199.2915 - mse: 199.2915 - val_loss: 242.7980 - val_mse: 242.7979\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 196.7583 - mse: 196.7582 - val_loss: 235.1845 - val_mse: 235.1845\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 203.1303 - mse: 203.1303 - val_loss: 236.0841 - val_mse: 236.0841\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 198.4608 - mse: 198.4608 - val_loss: 236.3112 - val_mse: 236.3112\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 198.2469 - mse: 198.2468 - val_loss: 233.5791 - val_mse: 233.5791\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 192.5848 - mse: 192.5848 - val_loss: 231.3580 - val_mse: 231.3580\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 193.6428 - mse: 193.6428 - val_loss: 236.1705 - val_mse: 236.1704\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 198.7857 - mse: 198.7857 - val_loss: 228.3417 - val_mse: 228.3417\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 199.5665 - mse: 199.5666 - val_loss: 227.4089 - val_mse: 227.4088\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 196.9020 - mse: 196.9020 - val_loss: 230.9410 - val_mse: 230.9410\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 190.2757 - mse: 190.2757 - val_loss: 223.7152 - val_mse: 223.7152\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 191.9061 - mse: 191.9061 - val_loss: 221.8404 - val_mse: 221.8404\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 189.1351 - mse: 189.1351 - val_loss: 222.0293 - val_mse: 222.0293\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 185.0660 - mse: 185.0659 - val_loss: 223.8618 - val_mse: 223.8618\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 187.7336 - mse: 187.7336 - val_loss: 219.8930 - val_mse: 219.8930\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 188.4481 - mse: 188.4481 - val_loss: 217.0437 - val_mse: 217.0438\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 188.7426 - mse: 188.7426 - val_loss: 222.1092 - val_mse: 222.1092\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 191.4280 - mse: 191.4281 - val_loss: 217.3578 - val_mse: 217.3577\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 187.7897 - mse: 187.7897 - val_loss: 212.9033 - val_mse: 212.9033\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 180.9824 - mse: 180.9824 - val_loss: 215.1565 - val_mse: 215.1565\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 184.9914 - mse: 184.9915 - val_loss: 211.1100 - val_mse: 211.1100\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 186.9445 - mse: 186.9445 - val_loss: 208.5078 - val_mse: 208.5079\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 180.4591 - mse: 180.4591 - val_loss: 214.4333 - val_mse: 214.4333\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 188.3728 - mse: 188.3727 - val_loss: 210.0858 - val_mse: 210.0858\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 186.5447 - mse: 186.5446 - val_loss: 212.7322 - val_mse: 212.7322\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 186.7535 - mse: 186.7535 - val_loss: 213.5221 - val_mse: 213.5222\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 182.2833 - mse: 182.2834 - val_loss: 210.3362 - val_mse: 210.3362\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 180.7494 - mse: 180.7493 - val_loss: 205.9342 - val_mse: 205.9342\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 184.5200 - mse: 184.5201 - val_loss: 210.8216 - val_mse: 210.8216\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 179.3759 - mse: 179.3759 - val_loss: 204.5269 - val_mse: 204.5269\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 179.9235 - mse: 179.9236 - val_loss: 202.1432 - val_mse: 202.1432\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 172.1653 - mse: 172.165 - 1s 252us/sample - loss: 183.2708 - mse: 183.2708 - val_loss: 202.5066 - val_mse: 202.5066\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 181.7267 - mse: 181.7268 - val_loss: 199.3970 - val_mse: 199.3970\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 179.5073 - mse: 179.5074 - val_loss: 197.5434 - val_mse: 197.5434\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 178.0208 - mse: 178.0208 - val_loss: 199.5083 - val_mse: 199.5082\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 178.3529 - mse: 178.3529 - val_loss: 200.0226 - val_mse: 200.0225\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 177.6767 - mse: 177.6767 - val_loss: 200.0075 - val_mse: 200.0074\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 175.1641 - mse: 175.1641 - val_loss: 197.1213 - val_mse: 197.1212\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 172.7743 - mse: 172.7743 - val_loss: 196.6506 - val_mse: 196.6506\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 184.4962 - mse: 184.4962 - val_loss: 196.2512 - val_mse: 196.2512\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 176.2358 - mse: 176.2358 - val_loss: 197.8604 - val_mse: 197.8603\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 178.5670 - mse: 178.5670 - val_loss: 197.5157 - val_mse: 197.5157\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 181.0976 - mse: 181.0975 - val_loss: 196.0351 - val_mse: 196.0351\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 179.3431 - mse: 179.3431 - val_loss: 194.9602 - val_mse: 194.9602\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 176.0071 - mse: 176.0070 - val_loss: 199.5072 - val_mse: 199.5072\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 176.0437 - mse: 176.0437 - val_loss: 197.2211 - val_mse: 197.2212\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 178.9344 - mse: 178.9344 - val_loss: 196.0994 - val_mse: 196.0994\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 169.5755 - mse: 169.5755 - val_loss: 193.8163 - val_mse: 193.8163\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 179.2726 - mse: 179.2726 - val_loss: 191.8783 - val_mse: 191.8783\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 170.3749 - mse: 170.3749 - val_loss: 197.9013 - val_mse: 197.9012\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 175.6344 - mse: 175.6344 - val_loss: 200.2210 - val_mse: 200.2210\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 166.1365 - mse: 166.1365 - val_loss: 188.5330 - val_mse: 188.5330\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 167.6736 - mse: 167.6736 - val_loss: 189.6404 - val_mse: 189.6404\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 166.8598 - mse: 166.8598 - val_loss: 184.8231 - val_mse: 184.8232\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 252us/sample - loss: 170.2257 - mse: 170.2257 - val_loss: 193.3362 - val_mse: 193.3362\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 164.2555 - mse: 164.2556 - val_loss: 188.9633 - val_mse: 188.9633\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 171.2272 - mse: 171.2272 - val_loss: 187.4320 - val_mse: 187.4319\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 163.3228 - mse: 163.3228 - val_loss: 189.2987 - val_mse: 189.2987\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 168.3168 - mse: 168.3168 - val_loss: 187.0987 - val_mse: 187.0987\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 164.9217 - mse: 164.9217 - val_loss: 187.4140 - val_mse: 187.4140\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 169.2416 - mse: 169.2416 - val_loss: 192.5961 - val_mse: 192.5961\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 165.2684 - mse: 165.2684 - val_loss: 195.5630 - val_mse: 195.5630\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 170.3730 - mse: 170.3730 - val_loss: 192.3368 - val_mse: 192.3368\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 166.2836 - mse: 166.2836 - val_loss: 188.3148 - val_mse: 188.3148\n",
      "[CV] ....................................... nl=0, nn=3, total= 1.1min\n",
      "[CV] nl=0, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 703us/sample - loss: 441.9517 - mse: 441.9516 - val_loss: 653.0915 - val_mse: 653.0914\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 385.0394 - mse: 385.0394 - val_loss: 571.8772 - val_mse: 571.8771\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 335.5294 - mse: 335.5295 - val_loss: 469.2683 - val_mse: 469.2684\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 280.6023 - mse: 280.6024 - val_loss: 392.7536 - val_mse: 392.7536\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 237.6086 - mse: 237.6086 - val_loss: 328.6990 - val_mse: 328.6989\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 206.9265 - mse: 206.9265 - val_loss: 288.7131 - val_mse: 288.7130\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 189.9764 - mse: 189.9763 - val_loss: 274.5767 - val_mse: 274.5767\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 179.9807 - mse: 179.9807 - val_loss: 249.5019 - val_mse: 249.5019\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 177.3603 - mse: 177.3603 - val_loss: 240.2437 - val_mse: 240.2437\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 173.7938 - mse: 173.7938 - val_loss: 241.2212 - val_mse: 241.2211\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 168.5898 - mse: 168.5898 - val_loss: 239.6840 - val_mse: 239.6840\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 165.1501 - mse: 165.1501 - val_loss: 227.5772 - val_mse: 227.5772\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 164.2611 - mse: 164.2611 - val_loss: 224.8919 - val_mse: 224.8918\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 168.2976 - mse: 168.2975 - val_loss: 225.1924 - val_mse: 225.1925\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 166.5248 - mse: 166.5248 - val_loss: 225.5707 - val_mse: 225.5707\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 163.7150 - mse: 163.7150 - val_loss: 218.1313 - val_mse: 218.1312\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 168.7749 - mse: 168.7749 - val_loss: 217.5044 - val_mse: 217.5044\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 165.8308 - mse: 165.8308 - val_loss: 217.2792 - val_mse: 217.2792\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 168.8807 - mse: 168.8807 - val_loss: 215.9363 - val_mse: 215.9364\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 166.0937 - mse: 166.0937 - val_loss: 217.0595 - val_mse: 217.0595\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 166.7073 - mse: 166.7073 - val_loss: 213.7096 - val_mse: 213.7096\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 166.0698 - mse: 166.0698 - val_loss: 212.6174 - val_mse: 212.6174\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 162.0503 - mse: 162.0502 - val_loss: 208.7007 - val_mse: 208.7007\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 159.8324 - mse: 159.8325 - val_loss: 219.3343 - val_mse: 219.3342\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 157.8347 - mse: 157.8347 - val_loss: 210.3576 - val_mse: 210.3576\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 161.0341 - mse: 161.0341 - val_loss: 210.9545 - val_mse: 210.9545\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 159.8972 - mse: 159.8972 - val_loss: 206.2615 - val_mse: 206.2615\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 155.9086 - mse: 155.9086 - val_loss: 201.9383 - val_mse: 201.9383\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 162.7304 - mse: 162.7303 - val_loss: 202.5827 - val_mse: 202.5827\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 155.6113 - mse: 155.6113 - val_loss: 201.8247 - val_mse: 201.8247\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 155.7711 - mse: 155.7711 - val_loss: 202.6227 - val_mse: 202.6226\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 155.7728 - mse: 155.7729 - val_loss: 203.1703 - val_mse: 203.1703\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 158.6742 - mse: 158.6742 - val_loss: 207.1841 - val_mse: 207.1841\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 151.8229 - mse: 151.8229 - val_loss: 199.3424 - val_mse: 199.3424\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 154.9203 - mse: 154.9203 - val_loss: 200.6276 - val_mse: 200.6276\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 149.1029 - mse: 149.1029 - val_loss: 197.1088 - val_mse: 197.1088\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 154.7132 - mse: 154.7132 - val_loss: 195.7685 - val_mse: 195.7685\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 144.9474 - mse: 144.9474 - val_loss: 191.7054 - val_mse: 191.7054\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 148.4645 - mse: 148.4645 - val_loss: 198.1877 - val_mse: 198.1877\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 150.1340 - mse: 150.1340 - val_loss: 196.1640 - val_mse: 196.1640\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 144.4114 - mse: 144.4114 - val_loss: 196.5503 - val_mse: 196.5503\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 148.5406 - mse: 148.5406 - val_loss: 195.9545 - val_mse: 195.9545\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 149.6500 - mse: 149.6499 - val_loss: 195.0494 - val_mse: 195.0495\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 147.8685 - mse: 147.8685 - val_loss: 195.3285 - val_mse: 195.3286\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 141.6483 - mse: 141.6483 - val_loss: 191.0369 - val_mse: 191.0369\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 143.9595 - mse: 143.9595 - val_loss: 192.5021 - val_mse: 192.5021\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 140.3571 - mse: 140.3571 - val_loss: 195.0513 - val_mse: 195.0513\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 146.4543 - mse: 146.4543 - val_loss: 191.9442 - val_mse: 191.9442\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 142.1136 - mse: 142.1136 - val_loss: 194.7857 - val_mse: 194.7857\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 137.6082 - mse: 137.6082 - val_loss: 189.2490 - val_mse: 189.2490\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 142.8435 - mse: 142.8435 - val_loss: 191.8567 - val_mse: 191.8566\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 136.6908 - mse: 136.6908 - val_loss: 190.8778 - val_mse: 190.8778\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 144.9534 - mse: 144.9534 - val_loss: 191.5193 - val_mse: 191.5193\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 144.0853 - mse: 144.0853 - val_loss: 190.3202 - val_mse: 190.3202\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 140.5223 - mse: 140.5224 - val_loss: 190.7641 - val_mse: 190.7641\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 145.8979 - mse: 145.8979 - val_loss: 194.7851 - val_mse: 194.7851\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 137.6046 - mse: 137.6046 - val_loss: 196.4693 - val_mse: 196.4694\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 133.5143 - mse: 133.5143 - val_loss: 191.4646 - val_mse: 191.4646\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 137.6719 - mse: 137.6719 - val_loss: 189.6492 - val_mse: 189.6492\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 139.4184 - mse: 139.4184 - val_loss: 193.6657 - val_mse: 193.6658\n",
      "[CV] ....................................... nl=0, nn=3, total=  44.6s\n",
      "[CV] nl=0, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 616us/sample - loss: 509.1981 - mse: 509.1980 - val_loss: 651.8063 - val_mse: 651.8062\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 459.7485 - mse: 459.7486 - val_loss: 573.5855 - val_mse: 573.5856\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 403.7449 - mse: 403.7450 - val_loss: 478.7360 - val_mse: 478.7360\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 343.9580 - mse: 343.9579 - val_loss: 393.1479 - val_mse: 393.1479\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 291.2561 - mse: 291.2562 - val_loss: 336.3732 - val_mse: 336.3733\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 251.2827 - mse: 251.2827 - val_loss: 296.6586 - val_mse: 296.6586\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 229.6622 - mse: 229.6622 - val_loss: 256.4712 - val_mse: 256.4713\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 230.1803 - mse: 230.1803 - val_loss: 247.1752 - val_mse: 247.1752\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 208.2070 - mse: 208.2070 - val_loss: 236.5307 - val_mse: 236.5307\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 209.6224 - mse: 209.6223 - val_loss: 225.3850 - val_mse: 225.3850\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 206.1010 - mse: 206.1010 - val_loss: 223.5792 - val_mse: 223.5792\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 198.0525 - mse: 198.0525 - val_loss: 216.3650 - val_mse: 216.3649\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 199.0016 - mse: 199.0016 - val_loss: 215.5466 - val_mse: 215.5466\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 187.1947 - mse: 187.1947 - val_loss: 207.6753 - val_mse: 207.6753\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 190.5608 - mse: 190.5607 - val_loss: 206.8851 - val_mse: 206.8851\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 196.0681 - mse: 196.0681 - val_loss: 199.9040 - val_mse: 199.9040\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 192.7052 - mse: 192.7052 - val_loss: 198.0714 - val_mse: 198.0714\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 194.4842 - mse: 194.4843 - val_loss: 195.0916 - val_mse: 195.0916\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 186.2666 - mse: 186.2666 - val_loss: 198.7146 - val_mse: 198.7146\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 186.4880 - mse: 186.4881 - val_loss: 195.5862 - val_mse: 195.5862\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 179.7538 - mse: 179.7538 - val_loss: 193.3273 - val_mse: 193.3273\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 179.2236 - mse: 179.2236 - val_loss: 190.4985 - val_mse: 190.4984\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 179.3516 - mse: 179.3515 - val_loss: 187.7815 - val_mse: 187.7815\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 180.9623 - mse: 180.9623 - val_loss: 182.0308 - val_mse: 182.0308\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 170.5591 - mse: 170.5591 - val_loss: 182.5335 - val_mse: 182.5335\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 170.8623 - mse: 170.8622 - val_loss: 179.4371 - val_mse: 179.4371\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 172.8036 - mse: 172.8036 - val_loss: 177.4331 - val_mse: 177.4332\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 167.6781 - mse: 167.6781 - val_loss: 176.7832 - val_mse: 176.7832\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 169.4550 - mse: 169.4550 - val_loss: 175.9491 - val_mse: 175.9491\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 168.7253 - mse: 168.7253 - val_loss: 176.4979 - val_mse: 176.4979\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 169.4830 - mse: 169.4830 - val_loss: 173.8507 - val_mse: 173.8508\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 165.0128 - mse: 165.0128 - val_loss: 173.7752 - val_mse: 173.7752\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 164.6027 - mse: 164.6027 - val_loss: 172.7501 - val_mse: 172.7501\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 164.3583 - mse: 164.3583 - val_loss: 172.6546 - val_mse: 172.6546\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 164.8907 - mse: 164.8907 - val_loss: 172.0249 - val_mse: 172.0249\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 165.0201 - mse: 165.0201 - val_loss: 170.1471 - val_mse: 170.1471\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 167.8659 - mse: 167.8659 - val_loss: 171.9037 - val_mse: 171.9037\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 169.8262 - mse: 169.8262 - val_loss: 171.4856 - val_mse: 171.4855\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 255us/sample - loss: 159.9496 - mse: 159.9496 - val_loss: 169.8014 - val_mse: 169.8014\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 161.0814 - mse: 161.0814 - val_loss: 169.0480 - val_mse: 169.0480\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 166.5124 - mse: 166.5123 - val_loss: 168.0973 - val_mse: 168.0973\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 171.0294 - mse: 171.0293 - val_loss: 167.6134 - val_mse: 167.6134\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 163.6107 - mse: 163.6107 - val_loss: 166.4197 - val_mse: 166.4197\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 156.3332 - mse: 156.3332 - val_loss: 165.9987 - val_mse: 165.9987\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 162.1894 - mse: 162.189 - 1s 255us/sample - loss: 157.4038 - mse: 157.4038 - val_loss: 166.8653 - val_mse: 166.8653\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 159.6837 - mse: 159.6836 - val_loss: 165.0625 - val_mse: 165.0624\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 168.3921 - mse: 168.3921 - val_loss: 165.9211 - val_mse: 165.9211\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 160.5016 - mse: 160.5016 - val_loss: 162.9810 - val_mse: 162.9810\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 161.2089 - mse: 161.2089 - val_loss: 164.5956 - val_mse: 164.5956\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 157.0995 - mse: 157.0995 - val_loss: 165.9718 - val_mse: 165.9718\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 161.6730 - mse: 161.6730 - val_loss: 164.3192 - val_mse: 164.3192\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 167.9692 - mse: 167.9692 - val_loss: 162.7982 - val_mse: 162.7982\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 157.9344 - mse: 157.9344 - val_loss: 160.8650 - val_mse: 160.8650\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 163.1809 - mse: 163.1809 - val_loss: 161.4405 - val_mse: 161.4405\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 165.8624 - mse: 165.8624 - val_loss: 161.0569 - val_mse: 161.0569\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 159.0795 - mse: 159.0795 - val_loss: 160.3340 - val_mse: 160.3340\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 152.3033 - mse: 152.3033 - val_loss: 159.8784 - val_mse: 159.8784\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 164.0033 - mse: 164.0033 - val_loss: 160.3535 - val_mse: 160.3535\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 152.0771 - mse: 152.0771 - val_loss: 158.1998 - val_mse: 158.1998\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 164.1403 - mse: 164.1404 - val_loss: 155.7055 - val_mse: 155.7055\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 158.2060 - mse: 158.2060 - val_loss: 157.9225 - val_mse: 157.9225\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 157.0972 - mse: 157.0973 - val_loss: 159.4167 - val_mse: 159.4167\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 156.7974 - mse: 156.7975 - val_loss: 159.1268 - val_mse: 159.1268\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 156.2601 - mse: 156.2601 - val_loss: 162.3280 - val_mse: 162.3280\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 154.3286 - mse: 154.3286 - val_loss: 158.3785 - val_mse: 158.3785\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 161.6795 - mse: 161.6795 - val_loss: 156.1015 - val_mse: 156.1015\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 149.7707 - mse: 149.7708 - val_loss: 152.4038 - val_mse: 152.4038\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 149.7020 - mse: 149.7020 - val_loss: 154.1208 - val_mse: 154.1208\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 158.6486 - mse: 158.6486 - val_loss: 153.3729 - val_mse: 153.3729\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 150.5839 - mse: 150.5839 - val_loss: 153.9166 - val_mse: 153.9166\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 161.9559 - mse: 161.9559 - val_loss: 150.7533 - val_mse: 150.7534\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 154.4412 - mse: 154.4412 - val_loss: 153.8877 - val_mse: 153.8877\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 161.3555 - mse: 161.355 - 1s 253us/sample - loss: 158.8540 - mse: 158.8540 - val_loss: 151.8435 - val_mse: 151.8435\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 153.5121 - mse: 153.5122 - val_loss: 150.5313 - val_mse: 150.5313\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 157.2635 - mse: 157.2635 - val_loss: 150.4631 - val_mse: 150.4631\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 159.8816 - mse: 159.8816 - val_loss: 151.8000 - val_mse: 151.7999\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 152.1064 - mse: 152.1065 - val_loss: 150.8102 - val_mse: 150.8102\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 150.0256 - mse: 150.0256 - val_loss: 148.3634 - val_mse: 148.3633\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 156.6976 - mse: 156.6976 - val_loss: 148.2451 - val_mse: 148.2450\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 153.4594 - mse: 153.4594 - val_loss: 147.9010 - val_mse: 147.9010\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 149.8477 - mse: 149.8477 - val_loss: 147.4495 - val_mse: 147.4495\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 149.8785 - mse: 149.8785 - val_loss: 147.8660 - val_mse: 147.8660\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 145.4889 - mse: 145.4889 - val_loss: 147.4776 - val_mse: 147.4776\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 143.1213 - mse: 143.1213 - val_loss: 147.1999 - val_mse: 147.1999\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 312us/sample - loss: 166.2439 - mse: 166.2439 - val_loss: 149.7022 - val_mse: 149.7022\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 149.3156 - mse: 149.3156 - val_loss: 144.4069 - val_mse: 144.4068\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 153.9542 - mse: 153.9542 - val_loss: 142.9945 - val_mse: 142.9946\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 144.1654 - mse: 144.1654 - val_loss: 144.7747 - val_mse: 144.7747\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 305us/sample - loss: 153.1191 - mse: 153.1191 - val_loss: 140.5186 - val_mse: 140.5186\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 149.8463 - mse: 149.8463 - val_loss: 140.7164 - val_mse: 140.7164\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 303us/sample - loss: 142.2519 - mse: 142.2519 - val_loss: 142.5140 - val_mse: 142.5140\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 310us/sample - loss: 152.5451 - mse: 152.5451 - val_loss: 148.7407 - val_mse: 148.7407\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 299us/sample - loss: 155.2416 - mse: 155.2416 - val_loss: 142.6435 - val_mse: 142.6434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 161.0895 - mse: 161.0896 - val_loss: 141.9758 - val_mse: 141.9758\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 146.9963 - mse: 146.9963 - val_loss: 138.6382 - val_mse: 138.6383\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 148.3512 - mse: 148.3513 - val_loss: 140.7887 - val_mse: 140.7887\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 298us/sample - loss: 151.2402 - mse: 151.2402 - val_loss: 139.3338 - val_mse: 139.3337\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 146.3871 - mse: 146.3871 - val_loss: 140.3326 - val_mse: 140.3326\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 149.9377 - mse: 149.9378 - val_loss: 137.1541 - val_mse: 137.1541\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 153.2574 - mse: 153.2574 - val_loss: 141.9209 - val_mse: 141.9210\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 149.5531 - mse: 149.5532 - val_loss: 137.2806 - val_mse: 137.2805\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 147.4015 - mse: 147.4015 - val_loss: 140.9863 - val_mse: 140.9863\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 144.3894 - mse: 144.3894 - val_loss: 136.2105 - val_mse: 136.2105\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 140.7284 - mse: 140.7283 - val_loss: 137.0377 - val_mse: 137.0377\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 148.8225 - mse: 148.8225 - val_loss: 135.5973 - val_mse: 135.5973\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 154.8990 - mse: 154.8990 - val_loss: 135.9942 - val_mse: 135.9942\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 147.1053 - mse: 147.1053 - val_loss: 134.1655 - val_mse: 134.1655\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 148.3871 - mse: 148.3871 - val_loss: 133.7997 - val_mse: 133.7997\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 146.7925 - mse: 146.7925 - val_loss: 134.5592 - val_mse: 134.5593\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 149.4916 - mse: 149.4916 - val_loss: 135.5767 - val_mse: 135.5767\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 146.2252 - mse: 146.2253 - val_loss: 136.1968 - val_mse: 136.1968\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 140.0950 - mse: 140.0951 - val_loss: 134.4386 - val_mse: 134.4386\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 143.4289 - mse: 143.4289 - val_loss: 133.0904 - val_mse: 133.0903\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 138.9096 - mse: 138.9096 - val_loss: 134.4889 - val_mse: 134.4890\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 140.2041 - mse: 140.2041 - val_loss: 139.2329 - val_mse: 139.2329\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 134.7566 - mse: 134.7566 - val_loss: 136.5044 - val_mse: 136.5044\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 143.1669 - mse: 143.1669 - val_loss: 138.6494 - val_mse: 138.6494\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 150.5640 - mse: 150.5640 - val_loss: 133.8562 - val_mse: 133.8562\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 142.2558 - mse: 142.2559 - val_loss: 134.6792 - val_mse: 134.6792\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 141.9220 - mse: 141.9220 - val_loss: 130.0229 - val_mse: 130.0229\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 138.8837 - mse: 138.8837 - val_loss: 130.8553 - val_mse: 130.8553\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 143.4041 - mse: 143.4041 - val_loss: 131.7445 - val_mse: 131.7445\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 141.0045 - mse: 141.0044 - val_loss: 133.6817 - val_mse: 133.6817\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 148.6433 - mse: 148.6432 - val_loss: 130.1507 - val_mse: 130.1507\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 134.8857 - mse: 134.8857 - val_loss: 130.3446 - val_mse: 130.3447\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 141.1137 - mse: 141.1137 - val_loss: 131.2911 - val_mse: 131.2912\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 144.1557 - mse: 144.1557 - val_loss: 131.7914 - val_mse: 131.7914\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 141.1741 - mse: 141.1741 - val_loss: 131.6368 - val_mse: 131.6368\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 140.6139 - mse: 140.6139 - val_loss: 127.7956 - val_mse: 127.7956\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 139.1000 - mse: 139.1000 - val_loss: 129.7969 - val_mse: 129.7969\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 134.0510 - mse: 134.0511 - val_loss: 133.1402 - val_mse: 133.1402\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 137.6414 - mse: 137.6414 - val_loss: 131.3228 - val_mse: 131.3228\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 136.0834 - mse: 136.0834 - val_loss: 129.0125 - val_mse: 129.0125\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 139.1738 - mse: 139.1738 - val_loss: 128.4426 - val_mse: 128.4426\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 135.7015 - mse: 135.7015 - val_loss: 129.2967 - val_mse: 129.2967\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 144.2256 - mse: 144.2256 - val_loss: 128.6487 - val_mse: 128.6488\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 140.4835 - mse: 140.4835 - val_loss: 130.8910 - val_mse: 130.8910\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 138.1104 - mse: 138.1104 - val_loss: 127.6756 - val_mse: 127.6756\n",
      "Epoch 139/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 136.3952 - mse: 136.3952 - val_loss: 128.1410 - val_mse: 128.1411\n",
      "Epoch 140/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 139.9949 - mse: 139.9949 - val_loss: 128.4354 - val_mse: 128.4353\n",
      "Epoch 141/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 141.4873 - mse: 141.4873 - val_loss: 130.0927 - val_mse: 130.0926\n",
      "Epoch 142/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 138.6473 - mse: 138.6473 - val_loss: 125.2346 - val_mse: 125.2346\n",
      "Epoch 143/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 140.2262 - mse: 140.2262 - val_loss: 127.6444 - val_mse: 127.6444\n",
      "Epoch 144/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 147.0155 - mse: 147.0156 - val_loss: 127.9378 - val_mse: 127.9378\n",
      "Epoch 145/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 139.1389 - mse: 139.1390 - val_loss: 127.5394 - val_mse: 127.5394\n",
      "Epoch 146/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 138.8111 - mse: 138.8111 - val_loss: 128.3759 - val_mse: 128.3758\n",
      "Epoch 147/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 143.6982 - mse: 143.6982 - val_loss: 131.7439 - val_mse: 131.7439\n",
      "Epoch 148/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 141.8141 - mse: 141.8140 - val_loss: 130.8930 - val_mse: 130.8930\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 266us/sample - loss: 137.1211 - mse: 137.1212 - val_loss: 128.4028 - val_mse: 128.4028\n",
      "Epoch 150/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 142.3632 - mse: 142.3632 - val_loss: 125.0724 - val_mse: 125.0724\n",
      "Epoch 151/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 144.3610 - mse: 144.3610 - val_loss: 124.8853 - val_mse: 124.8853\n",
      "Epoch 152/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 137.8595 - mse: 137.8595 - val_loss: 124.3770 - val_mse: 124.3769\n",
      "Epoch 153/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 135.5606 - mse: 135.5606 - val_loss: 132.6509 - val_mse: 132.6508\n",
      "Epoch 154/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 131.0680 - mse: 131.0679 - val_loss: 131.9991 - val_mse: 131.9991\n",
      "Epoch 155/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 134.2154 - mse: 134.2154 - val_loss: 128.1702 - val_mse: 128.1702\n",
      "Epoch 156/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 140.3757 - mse: 140.3757 - val_loss: 129.1642 - val_mse: 129.1642\n",
      "Epoch 157/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 134.2075 - mse: 134.2075 - val_loss: 128.3003 - val_mse: 128.3003\n",
      "Epoch 158/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 137.9422 - mse: 137.9422 - val_loss: 126.8299 - val_mse: 126.8298\n",
      "Epoch 159/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 137.8838 - mse: 137.8838 - val_loss: 128.7836 - val_mse: 128.7836\n",
      "Epoch 160/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 140.1391 - mse: 140.1391 - val_loss: 128.7143 - val_mse: 128.7144\n",
      "Epoch 161/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 142.5503 - mse: 142.5502 - val_loss: 128.4036 - val_mse: 128.4036\n",
      "Epoch 162/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 137.5565 - mse: 137.5564 - val_loss: 127.1952 - val_mse: 127.1952\n",
      "[CV] ....................................... nl=0, nn=3, total= 2.1min\n",
      "[CV] nl=0, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 3s 878us/sample - loss: 497.1393 - mse: 497.1393 - val_loss: 541.0795 - val_mse: 541.0794\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 457.1677 - mse: 457.1676 - val_loss: 504.0151 - val_mse: 504.0151\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 409.7776 - mse: 409.7775 - val_loss: 439.8275 - val_mse: 439.8274\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 359.3369 - mse: 359.3369 - val_loss: 366.9606 - val_mse: 366.9606\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 312.9620 - mse: 312.9620 - val_loss: 313.0707 - val_mse: 313.0706\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 271.2660 - mse: 271.2660 - val_loss: 247.2681 - val_mse: 247.2681\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 234.2051 - mse: 234.2050 - val_loss: 222.9043 - val_mse: 222.9043\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 213.5013 - mse: 213.5013 - val_loss: 203.3439 - val_mse: 203.3439\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 202.8506 - mse: 202.8506 - val_loss: 185.8042 - val_mse: 185.8042\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 192.6090 - mse: 192.6089 - val_loss: 171.7777 - val_mse: 171.7776\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 187.0498 - mse: 187.0498 - val_loss: 170.6598 - val_mse: 170.6597\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.8746 - mse: 185.8746 - val_loss: 156.0938 - val_mse: 156.0938\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 176.6660 - mse: 176.6660 - val_loss: 152.9592 - val_mse: 152.9592\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 169.4296 - mse: 169.4296 - val_loss: 147.6357 - val_mse: 147.6357\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 166.1149 - mse: 166.1149 - val_loss: 142.1869 - val_mse: 142.1869\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 171.2293 - mse: 171.2293 - val_loss: 135.9403 - val_mse: 135.9403\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 169.8800 - mse: 169.8800 - val_loss: 135.4108 - val_mse: 135.4108\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 163.7399 - mse: 163.7399 - val_loss: 137.2557 - val_mse: 137.2557\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 162.1229 - mse: 162.1229 - val_loss: 134.7330 - val_mse: 134.7329\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 163.8357 - mse: 163.8357 - val_loss: 130.6406 - val_mse: 130.6406\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 162.9333 - mse: 162.9333 - val_loss: 130.9835 - val_mse: 130.9835\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 157.3296 - mse: 157.3296 - val_loss: 131.9292 - val_mse: 131.9292\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 157.7029 - mse: 157.7029 - val_loss: 125.0568 - val_mse: 125.0568\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 157.3599 - mse: 157.3599 - val_loss: 124.9177 - val_mse: 124.9177\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 159.3660 - mse: 159.3660 - val_loss: 122.9526 - val_mse: 122.9526\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 153.0060 - mse: 153.0060 - val_loss: 125.6435 - val_mse: 125.6435\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 152.5098 - mse: 152.5098 - val_loss: 121.5454 - val_mse: 121.5454\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 156.2376 - mse: 156.2376 - val_loss: 119.8981 - val_mse: 119.8981\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 151.3900 - mse: 151.3900 - val_loss: 119.5117 - val_mse: 119.5117\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 152.2510 - mse: 152.2510 - val_loss: 118.6403 - val_mse: 118.6403\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 146.8121 - mse: 146.8121 - val_loss: 119.1626 - val_mse: 119.1626\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 153.9717 - mse: 153.9717 - val_loss: 118.1236 - val_mse: 118.1236\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 148.9362 - mse: 148.9361 - val_loss: 120.0790 - val_mse: 120.0791\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 147.5873 - mse: 147.5873 - val_loss: 121.3053 - val_mse: 121.3052\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 147.2599 - mse: 147.2600 - val_loss: 117.5068 - val_mse: 117.5068\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 145.4943 - mse: 145.4943 - val_loss: 117.7215 - val_mse: 117.7215\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 147.6333 - mse: 147.6333 - val_loss: 122.8958 - val_mse: 122.8958\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 152.3202 - mse: 152.3202 - val_loss: 117.9535 - val_mse: 117.9535\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 144.7731 - mse: 144.7732 - val_loss: 118.8060 - val_mse: 118.8059\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 144.7034 - mse: 144.7034 - val_loss: 118.0324 - val_mse: 118.0324\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 147.0360 - mse: 147.0360 - val_loss: 117.8288 - val_mse: 117.8288\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 148.0062 - mse: 148.0062 - val_loss: 120.8832 - val_mse: 120.8832\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 145.8706 - mse: 145.8706 - val_loss: 119.1833 - val_mse: 119.1833\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 144.0610 - mse: 144.0610 - val_loss: 117.3686 - val_mse: 117.3686\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 144.2827 - mse: 144.2828 - val_loss: 117.4570 - val_mse: 117.4570\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 146.4434 - mse: 146.4434 - val_loss: 117.1440 - val_mse: 117.1440\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 145.8090 - mse: 145.8089 - val_loss: 119.4485 - val_mse: 119.4485\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 149.2758 - mse: 149.2758 - val_loss: 117.9024 - val_mse: 117.9024\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 145.5724 - mse: 145.5724 - val_loss: 118.1556 - val_mse: 118.1556\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 146.2012 - mse: 146.2012 - val_loss: 117.5981 - val_mse: 117.5981\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 145.1445 - mse: 145.1445 - val_loss: 116.5514 - val_mse: 116.5513\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 143.3281 - mse: 143.3282 - val_loss: 117.1152 - val_mse: 117.1152\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 146.0846 - mse: 146.0846 - val_loss: 116.4965 - val_mse: 116.4965\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 145.5283 - mse: 145.5283 - val_loss: 118.0644 - val_mse: 118.0644\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 139.4633 - mse: 139.4633 - val_loss: 116.0279 - val_mse: 116.0279\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 142.7130 - mse: 142.7130 - val_loss: 116.2745 - val_mse: 116.2745\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 142.0158 - mse: 142.0158 - val_loss: 117.0304 - val_mse: 117.0304\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 147.5093 - mse: 147.5093 - val_loss: 119.3332 - val_mse: 119.3332\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 144.7650 - mse: 144.7650 - val_loss: 115.5156 - val_mse: 115.5156\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 132.6609 - mse: 132.6609 - val_loss: 116.1133 - val_mse: 116.1133\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 138.2004 - mse: 138.2004 - val_loss: 115.9521 - val_mse: 115.9521\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 145.4072 - mse: 145.4072 - val_loss: 116.1749 - val_mse: 116.1749\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 152.0095 - mse: 152.0095 - val_loss: 117.4297 - val_mse: 117.4298\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 143.7530 - mse: 143.7530 - val_loss: 115.3385 - val_mse: 115.3385\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 139.6488 - mse: 139.6488 - val_loss: 115.2832 - val_mse: 115.2832\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 141.0253 - mse: 141.0253 - val_loss: 116.6325 - val_mse: 116.6324\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 145.2165 - mse: 145.2166 - val_loss: 116.8061 - val_mse: 116.8061\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 139.2744 - mse: 139.2744 - val_loss: 117.0346 - val_mse: 117.0346\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 139.9120 - mse: 139.9120 - val_loss: 115.8928 - val_mse: 115.8928\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 138.9191 - mse: 138.9192 - val_loss: 116.2840 - val_mse: 116.2841\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 146.1171 - mse: 146.1171 - val_loss: 116.0100 - val_mse: 116.0100\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 144.5834 - mse: 144.5834 - val_loss: 116.1641 - val_mse: 116.1641\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 311us/sample - loss: 144.1576 - mse: 144.1576 - val_loss: 119.3350 - val_mse: 119.3350\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 335us/sample - loss: 139.0884 - mse: 139.0884 - val_loss: 115.6945 - val_mse: 115.6946\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 135.8571 - mse: 135.8571 - val_loss: 116.3824 - val_mse: 116.3824\n",
      "[CV] ....................................... nl=0, nn=3, total=  58.1s\n",
      "[CV] nl=0, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 696us/sample - loss: 473.8085 - mse: 473.8086 - val_loss: 628.3429 - val_mse: 628.3430\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 410.6618 - mse: 410.6617 - val_loss: 558.2227 - val_mse: 558.2228\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 355.0548 - mse: 355.0550 - val_loss: 474.5290 - val_mse: 474.5290\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 302.4262 - mse: 302.4262 - val_loss: 395.9645 - val_mse: 395.9644\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 297us/sample - loss: 257.6101 - mse: 257.6100 - val_loss: 327.2576 - val_mse: 327.2577\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 223.7424 - mse: 223.7423 - val_loss: 282.7015 - val_mse: 282.7015\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 202.5297 - mse: 202.5297 - val_loss: 261.5680 - val_mse: 261.5680\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 198.0413 - mse: 198.0413 - val_loss: 248.1790 - val_mse: 248.1790\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 186.2177 - mse: 186.2177 - val_loss: 241.2745 - val_mse: 241.2745\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 179.2391 - mse: 179.2391 - val_loss: 231.5404 - val_mse: 231.5403\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 178.8174 - mse: 178.8174 - val_loss: 232.5305 - val_mse: 232.5304\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 173.7813 - mse: 173.7813 - val_loss: 216.8877 - val_mse: 216.8877\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 170.3548 - mse: 170.3547 - val_loss: 214.0683 - val_mse: 214.0683\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 167.9931 - mse: 167.9931 - val_loss: 211.7568 - val_mse: 211.7568\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 172.4404 - mse: 172.4404 - val_loss: 209.2335 - val_mse: 209.2335\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 165.7029 - mse: 165.7030 - val_loss: 207.0805 - val_mse: 207.0805\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 173.5160 - mse: 173.5159 - val_loss: 201.2109 - val_mse: 201.2109\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 166.9737 - mse: 166.9737 - val_loss: 201.2706 - val_mse: 201.2706\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 163.9991 - mse: 163.9992 - val_loss: 198.6533 - val_mse: 198.6532\n",
      "Epoch 20/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 248us/sample - loss: 163.8200 - mse: 163.8200 - val_loss: 196.1074 - val_mse: 196.1074\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 162.2725 - mse: 162.2726 - val_loss: 193.3371 - val_mse: 193.3371\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 165.5833 - mse: 165.5834 - val_loss: 192.8546 - val_mse: 192.8546\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 155.6739 - mse: 155.6739 - val_loss: 189.9668 - val_mse: 189.9668\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 159.6001 - mse: 159.6001 - val_loss: 188.5282 - val_mse: 188.5282\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 155.5577 - mse: 155.5577 - val_loss: 186.7143 - val_mse: 186.7143\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 153.4996 - mse: 153.4996 - val_loss: 185.4764 - val_mse: 185.4764\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 161.2641 - mse: 161.2641 - val_loss: 184.0411 - val_mse: 184.0412\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 162.5096 - mse: 162.5096 - val_loss: 181.4618 - val_mse: 181.4618\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 156.5147 - mse: 156.5147 - val_loss: 181.1694 - val_mse: 181.1694\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 153.9898 - mse: 153.9898 - val_loss: 179.4214 - val_mse: 179.4214\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 150.4246 - mse: 150.4246 - val_loss: 179.3403 - val_mse: 179.3403\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 155.9734 - mse: 155.9734 - val_loss: 179.9236 - val_mse: 179.9236\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 152.3754 - mse: 152.3754 - val_loss: 179.8766 - val_mse: 179.8766\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 151.6485 - mse: 151.6484 - val_loss: 176.7296 - val_mse: 176.7296\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 155.1369 - mse: 155.1369 - val_loss: 176.3554 - val_mse: 176.3554\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 149.9523 - mse: 149.9523 - val_loss: 176.7199 - val_mse: 176.7199\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 154.1650 - mse: 154.1650 - val_loss: 176.1960 - val_mse: 176.1961\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 148.8251 - mse: 148.8251 - val_loss: 173.7402 - val_mse: 173.7402\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 151.8291 - mse: 151.8292 - val_loss: 173.8805 - val_mse: 173.8805\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 146.1977 - mse: 146.1977 - val_loss: 171.5720 - val_mse: 171.5720\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 147.8615 - mse: 147.8615 - val_loss: 171.8041 - val_mse: 171.8041\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 150.4707 - mse: 150.4707 - val_loss: 171.2567 - val_mse: 171.2567\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 149.1202 - mse: 149.1202 - val_loss: 168.9083 - val_mse: 168.9084\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 144.8858 - mse: 144.8858 - val_loss: 169.1273 - val_mse: 169.1273\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 145.3573 - mse: 145.3573 - val_loss: 169.9684 - val_mse: 169.9684\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 145.6987 - mse: 145.6988 - val_loss: 166.9082 - val_mse: 166.9082\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 140.6438 - mse: 140.6439 - val_loss: 165.6372 - val_mse: 165.6372\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 151.9439 - mse: 151.9438 - val_loss: 164.5312 - val_mse: 164.5312\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 145.3140 - mse: 145.3140 - val_loss: 164.8137 - val_mse: 164.8137\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 148.0493 - mse: 148.0493 - val_loss: 163.8411 - val_mse: 163.8411\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 142.9369 - mse: 142.9369 - val_loss: 162.6851 - val_mse: 162.6851\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 137.1663 - mse: 137.1663 - val_loss: 162.9251 - val_mse: 162.9251\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 139.5362 - mse: 139.5362 - val_loss: 160.2376 - val_mse: 160.2376\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 139.2471 - mse: 139.2471 - val_loss: 160.2064 - val_mse: 160.2065\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 137.9580 - mse: 137.9581 - val_loss: 160.2982 - val_mse: 160.2982\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 150.9343 - mse: 150.9343 - val_loss: 159.9537 - val_mse: 159.9537\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 139.1472 - mse: 139.1472 - val_loss: 158.7402 - val_mse: 158.7402\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 139.0456 - mse: 139.0456 - val_loss: 159.0577 - val_mse: 159.0577\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 142.8452 - mse: 142.8452 - val_loss: 158.7071 - val_mse: 158.7071\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 139.5990 - mse: 139.5990 - val_loss: 160.1604 - val_mse: 160.1604\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 139.4752 - mse: 139.4752 - val_loss: 157.6768 - val_mse: 157.6768\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 148.2267 - mse: 148.2267 - val_loss: 158.3982 - val_mse: 158.3982\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 141.5198 - mse: 141.5198 - val_loss: 159.3597 - val_mse: 159.3597\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 137.6444 - mse: 137.6444 - val_loss: 158.2839 - val_mse: 158.2840\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 138.3847 - mse: 138.3848 - val_loss: 158.5214 - val_mse: 158.5214\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 144.9755 - mse: 144.9755 - val_loss: 160.5891 - val_mse: 160.5891\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 140.1503 - mse: 140.1503 - val_loss: 159.1518 - val_mse: 159.1519\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 140.2470 - mse: 140.2470 - val_loss: 158.2068 - val_mse: 158.2068\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 133.8305 - mse: 133.8306 - val_loss: 157.8028 - val_mse: 157.8028\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 144.0817 - mse: 144.0817 - val_loss: 158.1209 - val_mse: 158.1208\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 135.4760 - mse: 135.4760 - val_loss: 158.5053 - val_mse: 158.5053\n",
      "[CV] ....................................... nl=0, nn=3, total=  52.7s\n",
      "[CV] nl=0, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 581us/sample - loss: 518.2743 - mse: 518.2743 - val_loss: 661.4292 - val_mse: 661.4294\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 471.1076 - mse: 471.1075 - val_loss: 601.8866 - val_mse: 601.8867\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 414.5336 - mse: 414.5336 - val_loss: 529.2188 - val_mse: 529.2189\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 355.9645 - mse: 355.9645 - val_loss: 440.6454 - val_mse: 440.6454\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 305.4792 - mse: 305.4791 - val_loss: 381.9707 - val_mse: 381.9708\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 272.7632 - mse: 272.7632 - val_loss: 336.9259 - val_mse: 336.9259\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 244.8770 - mse: 244.8770 - val_loss: 308.8533 - val_mse: 308.8533\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 232.5130 - mse: 232.5131 - val_loss: 294.4077 - val_mse: 294.4077\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 226.5176 - mse: 226.5176 - val_loss: 277.9619 - val_mse: 277.9620\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 219.9698 - mse: 219.9698 - val_loss: 265.6405 - val_mse: 265.6405\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 213.5649 - mse: 213.5649 - val_loss: 260.8168 - val_mse: 260.8167\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 217.9498 - mse: 217.9498 - val_loss: 254.4567 - val_mse: 254.4567\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 222.9218 - mse: 222.9218 - val_loss: 247.7034 - val_mse: 247.7034\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 203.6554 - mse: 203.6554 - val_loss: 247.7371 - val_mse: 247.7371\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 210.0544 - mse: 210.0543 - val_loss: 245.0517 - val_mse: 245.0516\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 194us/sample - loss: 202.7877 - mse: 202.7877 - val_loss: 239.9519 - val_mse: 239.9518\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 204.0625 - mse: 204.0625 - val_loss: 239.9554 - val_mse: 239.9553\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 205.2537 - mse: 205.2537 - val_loss: 236.2030 - val_mse: 236.2029\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 200.3777 - mse: 200.3776 - val_loss: 234.4970 - val_mse: 234.4969\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 203.5320 - mse: 203.5320 - val_loss: 231.2004 - val_mse: 231.2004\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 203.8670 - mse: 203.8670 - val_loss: 226.1532 - val_mse: 226.1532\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 198.5855 - mse: 198.5855 - val_loss: 227.7352 - val_mse: 227.7352\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 199.4133 - mse: 199.4133 - val_loss: 224.5362 - val_mse: 224.5362\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 193.8076 - mse: 193.8075 - val_loss: 221.3550 - val_mse: 221.3550\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 190.2021 - mse: 190.2021 - val_loss: 222.7199 - val_mse: 222.7198\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 194.1245 - mse: 194.1244 - val_loss: 215.5594 - val_mse: 215.5593\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 193.1492 - mse: 193.1492 - val_loss: 217.2967 - val_mse: 217.2967\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 190.9824 - mse: 190.9824 - val_loss: 213.9264 - val_mse: 213.9264\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 191.3336 - mse: 191.3335 - val_loss: 208.4570 - val_mse: 208.4571\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 187.6860 - mse: 187.6860 - val_loss: 212.5990 - val_mse: 212.5990\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 194.7725 - mse: 194.7725 - val_loss: 210.4594 - val_mse: 210.4594\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 188.7664 - mse: 188.7664 - val_loss: 208.6989 - val_mse: 208.6989\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 189.1981 - mse: 189.1981 - val_loss: 215.2258 - val_mse: 215.2258\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 182.5150 - mse: 182.5150 - val_loss: 204.6198 - val_mse: 204.6198\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 191.2862 - mse: 191.2862 - val_loss: 206.8951 - val_mse: 206.8952\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 180.8897 - mse: 180.8896 - val_loss: 202.1910 - val_mse: 202.1910\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 191.5928 - mse: 191.5928 - val_loss: 203.2586 - val_mse: 203.2586\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 184.4135 - mse: 184.4135 - val_loss: 203.5427 - val_mse: 203.5427\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 186.4004 - mse: 186.4004 - val_loss: 198.2456 - val_mse: 198.2456\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 190.8600 - mse: 190.8600 - val_loss: 203.3253 - val_mse: 203.3253\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.5364 - mse: 185.5365 - val_loss: 197.4293 - val_mse: 197.4292\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 186.4646 - mse: 186.4646 - val_loss: 197.6426 - val_mse: 197.6426\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 182.9163 - mse: 182.9164 - val_loss: 197.6141 - val_mse: 197.6141\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 183.1487 - mse: 183.1487 - val_loss: 205.6202 - val_mse: 205.6202\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 177.6494 - mse: 177.6494 - val_loss: 200.5540 - val_mse: 200.5540\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 180.3205 - mse: 180.3205 - val_loss: 195.5521 - val_mse: 195.5521\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 183.1261 - mse: 183.1261 - val_loss: 199.0424 - val_mse: 199.0424\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 183.1705 - mse: 183.1706 - val_loss: 194.7104 - val_mse: 194.7104\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 185.8871 - mse: 185.8870 - val_loss: 194.0171 - val_mse: 194.0171\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 177.1232 - mse: 177.1233 - val_loss: 199.7762 - val_mse: 199.7763\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 174.0298 - mse: 174.0298 - val_loss: 192.5145 - val_mse: 192.5146\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 176.8625 - mse: 176.8625 - val_loss: 198.7434 - val_mse: 198.7434\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 173.5267 - mse: 173.5267 - val_loss: 191.7179 - val_mse: 191.7179\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 174.0662 - mse: 174.0661 - val_loss: 191.2600 - val_mse: 191.2600\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 182.6447 - mse: 182.6447 - val_loss: 195.8350 - val_mse: 195.8349\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 171.6882 - mse: 171.6881 - val_loss: 187.3629 - val_mse: 187.3629\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 175.7714 - mse: 175.7714 - val_loss: 182.4989 - val_mse: 182.4990\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 210us/sample - loss: 173.8441 - mse: 173.8441 - val_loss: 184.1771 - val_mse: 184.1770\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 177.3976 - mse: 177.3976 - val_loss: 186.1080 - val_mse: 186.1080\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 169.0392 - mse: 169.0392 - val_loss: 187.3490 - val_mse: 187.3490\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 167.5235 - mse: 167.5235 - val_loss: 184.2811 - val_mse: 184.2811\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 175.3768 - mse: 175.3768 - val_loss: 183.2240 - val_mse: 183.2240\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 167.3347 - mse: 167.3347 - val_loss: 186.7935 - val_mse: 186.7935\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 168.9503 - mse: 168.9503 - val_loss: 184.6342 - val_mse: 184.6342\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 170.3260 - mse: 170.3259 - val_loss: 191.0063 - val_mse: 191.0064\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 168.2501 - mse: 168.2501 - val_loss: 184.6198 - val_mse: 184.6198\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 162.0789 - mse: 162.0789 - val_loss: 183.2334 - val_mse: 183.2334\n",
      "[CV] ....................................... nl=0, nn=6, total=  42.6s\n",
      "[CV] nl=0, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 630us/sample - loss: 464.0896 - mse: 464.0894 - val_loss: 653.0208 - val_mse: 653.0208\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 409.5006 - mse: 409.5008 - val_loss: 590.8101 - val_mse: 590.8102\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 355.2608 - mse: 355.2608 - val_loss: 515.4537 - val_mse: 515.4537\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 298.9849 - mse: 298.9848 - val_loss: 422.2828 - val_mse: 422.2828\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 253.2427 - mse: 253.2428 - val_loss: 335.6448 - val_mse: 335.6447\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 185us/sample - loss: 213.5567 - mse: 213.5567 - val_loss: 296.6387 - val_mse: 296.6387\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 187.4391 - mse: 187.4392 - val_loss: 263.8149 - val_mse: 263.8150\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 181.1465 - mse: 181.1465 - val_loss: 248.0560 - val_mse: 248.0561\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 173.1528 - mse: 173.1528 - val_loss: 232.2507 - val_mse: 232.2506\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 164.9463 - mse: 164.9463 - val_loss: 226.7513 - val_mse: 226.7514\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 166.1188 - mse: 166.1188 - val_loss: 215.0559 - val_mse: 215.0559\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 153.5130 - mse: 153.5131 - val_loss: 211.2610 - val_mse: 211.2610\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 153.0275 - mse: 153.0275 - val_loss: 208.1440 - val_mse: 208.1440\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 158.3453 - mse: 158.3453 - val_loss: 201.1958 - val_mse: 201.1958\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 160.1408 - mse: 160.1407 - val_loss: 205.4416 - val_mse: 205.4416\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 155.4570 - mse: 155.4570 - val_loss: 202.3299 - val_mse: 202.3300\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 153.7953 - mse: 153.7952 - val_loss: 201.3105 - val_mse: 201.3105\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 148.4639 - mse: 148.4639 - val_loss: 199.0405 - val_mse: 199.0405\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 148.2794 - mse: 148.2794 - val_loss: 197.2502 - val_mse: 197.2502\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 148.5736 - mse: 148.5737 - val_loss: 195.6038 - val_mse: 195.6039\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 145.6767 - mse: 145.6768 - val_loss: 189.2243 - val_mse: 189.2243\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 146.2112 - mse: 146.2112 - val_loss: 188.0437 - val_mse: 188.0437\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 142.3323 - mse: 142.3322 - val_loss: 185.1177 - val_mse: 185.1177\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 145.7794 - mse: 145.7794 - val_loss: 182.1263 - val_mse: 182.1263\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 137.8783 - mse: 137.8783 - val_loss: 180.9426 - val_mse: 180.9426\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 146.4997 - mse: 146.4997 - val_loss: 185.4128 - val_mse: 185.4128\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 139.0476 - mse: 139.0476 - val_loss: 179.4057 - val_mse: 179.4058\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 136.8982 - mse: 136.8982 - val_loss: 182.5236 - val_mse: 182.5236\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 134.0500 - mse: 134.0500 - val_loss: 180.9090 - val_mse: 180.9090\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 139.8832 - mse: 139.8832 - val_loss: 181.1765 - val_mse: 181.1765\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 138.7656 - mse: 138.7656 - val_loss: 182.3446 - val_mse: 182.3445\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 145.2439 - mse: 145.2439 - val_loss: 186.0223 - val_mse: 186.0224\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 142.2805 - mse: 142.2805 - val_loss: 182.1334 - val_mse: 182.1334\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 142.8117 - mse: 142.8117 - val_loss: 178.7839 - val_mse: 178.7838\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 136.2179 - mse: 136.2179 - val_loss: 178.9598 - val_mse: 178.9598\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 133.5780 - mse: 133.5781 - val_loss: 179.3765 - val_mse: 179.3765\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 134.6945 - mse: 134.6945 - val_loss: 176.9892 - val_mse: 176.9892\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 145.2085 - mse: 145.2085 - val_loss: 182.6052 - val_mse: 182.6052\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 134.0795 - mse: 134.0796 - val_loss: 183.4847 - val_mse: 183.4847\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 136.4225 - mse: 136.4225 - val_loss: 180.7663 - val_mse: 180.7663\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 129.3275 - mse: 129.3276 - val_loss: 180.3597 - val_mse: 180.3597\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 146.4771 - mse: 146.4770 - val_loss: 181.5589 - val_mse: 181.5589\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 129.9219 - mse: 129.9220 - val_loss: 183.3165 - val_mse: 183.3166\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 137.5901 - mse: 137.5901 - val_loss: 181.6798 - val_mse: 181.6798\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 135.9274 - mse: 135.9274 - val_loss: 182.1129 - val_mse: 182.1129\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 137.7206 - mse: 137.7206 - val_loss: 177.4194 - val_mse: 177.4194\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 139.5858 - mse: 139.5858 - val_loss: 179.3397 - val_mse: 179.3396\n",
      "[CV] ....................................... nl=0, nn=6, total=  30.0s\n",
      "[CV] nl=0, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 570us/sample - loss: 519.8522 - mse: 519.8521 - val_loss: 660.4299 - val_mse: 660.4296\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 468.9400 - mse: 468.9400 - val_loss: 583.8802 - val_mse: 583.8803\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 415.5770 - mse: 415.5769 - val_loss: 500.8858 - val_mse: 500.8857\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 361.6762 - mse: 361.6763 - val_loss: 432.3773 - val_mse: 432.3773\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 320.2681 - mse: 320.2680 - val_loss: 390.6032 - val_mse: 390.6031\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 288.4579 - mse: 288.4579 - val_loss: 344.8565 - val_mse: 344.8565\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 273.2148 - mse: 273.2148 - val_loss: 331.5869 - val_mse: 331.5869\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 258.4030 - mse: 258.4030 - val_loss: 310.9443 - val_mse: 310.9444\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 251.6554 - mse: 251.6553 - val_loss: 303.0846 - val_mse: 303.0846\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 246.3949 - mse: 246.3949 - val_loss: 299.5507 - val_mse: 299.5508\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 247.0986 - mse: 247.0985 - val_loss: 297.4518 - val_mse: 297.4520\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 250.5380 - mse: 250.5381 - val_loss: 296.6876 - val_mse: 296.6877\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 245.0090 - mse: 245.0090 - val_loss: 294.3808 - val_mse: 294.3809\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 246.9436 - mse: 246.9436 - val_loss: 295.4602 - val_mse: 295.4602\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 241.2535 - mse: 241.2534 - val_loss: 291.8962 - val_mse: 291.8962\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 243.8412 - mse: 243.8413 - val_loss: 291.8533 - val_mse: 291.8533\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 246.6098 - mse: 246.6098 - val_loss: 290.9738 - val_mse: 290.9737\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 239.7492 - mse: 239.7492 - val_loss: 288.9106 - val_mse: 288.9106\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 235.0484 - mse: 235.0484 - val_loss: 288.3717 - val_mse: 288.3717\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 242.6289 - mse: 242.6290 - val_loss: 287.7673 - val_mse: 287.7673\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 240.3176 - mse: 240.3178 - val_loss: 287.4440 - val_mse: 287.4440\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 241.7186 - mse: 241.7186 - val_loss: 287.7961 - val_mse: 287.7961\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 242.2889 - mse: 242.2889 - val_loss: 287.2215 - val_mse: 287.2214\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 240.2183 - mse: 240.2183 - val_loss: 287.4255 - val_mse: 287.4255\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 240.9561 - mse: 240.9561 - val_loss: 286.9320 - val_mse: 286.9319\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 234.9969 - mse: 234.9968 - val_loss: 284.4220 - val_mse: 284.4220\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 233.4115 - mse: 233.4115 - val_loss: 278.7648 - val_mse: 278.7647\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 240.4372 - mse: 240.4372 - val_loss: 279.5580 - val_mse: 279.5580\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 232.0513 - mse: 232.0513 - val_loss: 275.7503 - val_mse: 275.7503\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 234.2201 - mse: 234.2201 - val_loss: 275.4933 - val_mse: 275.4932\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 230.9907 - mse: 230.9907 - val_loss: 276.2580 - val_mse: 276.2580\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 227.9408 - mse: 227.9407 - val_loss: 272.1579 - val_mse: 272.1579\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 227.1707 - mse: 227.1707 - val_loss: 271.2891 - val_mse: 271.2891\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 228.7193 - mse: 228.7193 - val_loss: 272.9771 - val_mse: 272.9771\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 224.8448 - mse: 224.8447 - val_loss: 272.4852 - val_mse: 272.4852\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 228.3665 - mse: 228.3664 - val_loss: 270.2370 - val_mse: 270.2369\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 220.9667 - mse: 220.9667 - val_loss: 267.9821 - val_mse: 267.9821\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 226.6068 - mse: 226.6067 - val_loss: 279.6715 - val_mse: 279.6715\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 218.4058 - mse: 218.4058 - val_loss: 290.7251 - val_mse: 290.7251\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 212.1676 - mse: 212.1676 - val_loss: 251.2467 - val_mse: 251.2467\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 213.5838 - mse: 213.5838 - val_loss: 249.6014 - val_mse: 249.6014\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 215.2575 - mse: 215.2575 - val_loss: 247.2833 - val_mse: 247.2832\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 207.5450 - mse: 207.5450 - val_loss: 243.8686 - val_mse: 243.8685\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 214.0717 - mse: 214.0717 - val_loss: 244.0448 - val_mse: 244.0447\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 210.9675 - mse: 210.9675 - val_loss: 250.2134 - val_mse: 250.2133\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 204.2712 - mse: 204.2712 - val_loss: 241.6123 - val_mse: 241.6124\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 202.4320 - mse: 202.4320 - val_loss: 236.1805 - val_mse: 236.1805\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 210.2160 - mse: 210.2160 - val_loss: 238.7400 - val_mse: 238.7400\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 199.5495 - mse: 199.5495 - val_loss: 234.4028 - val_mse: 234.4028\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 199.2169 - mse: 199.2170 - val_loss: 232.0100 - val_mse: 232.0099\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 197.7152 - mse: 197.7152 - val_loss: 231.1367 - val_mse: 231.1367\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 268us/sample - loss: 202.3077 - mse: 202.3078 - val_loss: 229.5995 - val_mse: 229.5995\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 195.8419 - mse: 195.8419 - val_loss: 234.2213 - val_mse: 234.2213\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 195.8902 - mse: 195.8902 - val_loss: 231.5433 - val_mse: 231.5434\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 200.1444 - mse: 200.1444 - val_loss: 225.2166 - val_mse: 225.2166\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 193.1197 - mse: 193.1197 - val_loss: 230.3199 - val_mse: 230.3199\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 192.6181 - mse: 192.6181 - val_loss: 224.9112 - val_mse: 224.9111\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 206.2094 - mse: 206.2094 - val_loss: 225.4430 - val_mse: 225.4430\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 195.9245 - mse: 195.9245 - val_loss: 225.3943 - val_mse: 225.3943\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 199.5553 - mse: 199.5553 - val_loss: 224.2619 - val_mse: 224.2619\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 193.6089 - mse: 193.6089 - val_loss: 240.5266 - val_mse: 240.5267\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 186us/sample - loss: 192.5522 - mse: 192.5522 - val_loss: 222.8898 - val_mse: 222.8898\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 186.9905 - mse: 186.9904 - val_loss: 219.4315 - val_mse: 219.4315\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 198.3811 - mse: 198.3811 - val_loss: 230.5887 - val_mse: 230.5887\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 192.0830 - mse: 192.0830 - val_loss: 224.5838 - val_mse: 224.5837\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 191.6647 - mse: 191.6647 - val_loss: 218.1518 - val_mse: 218.1517\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 186.9260 - mse: 186.9259 - val_loss: 215.4944 - val_mse: 215.4943\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 187.7790 - mse: 187.7790 - val_loss: 214.6095 - val_mse: 214.6095\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 181.6905 - mse: 181.6904 - val_loss: 219.3101 - val_mse: 219.3101\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 192.1608 - mse: 192.1607 - val_loss: 217.6679 - val_mse: 217.6679\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 197.9070 - mse: 197.9070 - val_loss: 217.2062 - val_mse: 217.2062\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 192.9097 - mse: 192.9097 - val_loss: 215.4186 - val_mse: 215.4186\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 188.2606 - mse: 188.2606 - val_loss: 226.5329 - val_mse: 226.5329\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 184.0418 - mse: 184.0418 - val_loss: 214.5995 - val_mse: 214.5995\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 187.7116 - mse: 187.7115 - val_loss: 217.4424 - val_mse: 217.4424\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 187.5621 - mse: 187.5622 - val_loss: 212.8074 - val_mse: 212.8074\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 187.0935 - mse: 187.0934 - val_loss: 212.9582 - val_mse: 212.9582\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 193.9277 - mse: 193.9277 - val_loss: 213.5491 - val_mse: 213.5491\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 183.0692 - mse: 183.0692 - val_loss: 221.7949 - val_mse: 221.7949\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 186.9403 - mse: 186.9404 - val_loss: 210.0328 - val_mse: 210.0328\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 197.9149 - mse: 197.9149 - val_loss: 211.7714 - val_mse: 211.7714\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 183.4829 - mse: 183.4828 - val_loss: 215.3058 - val_mse: 215.3058\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 179.6066 - mse: 179.6066 - val_loss: 214.3501 - val_mse: 214.3501\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 186.0793 - mse: 186.0793 - val_loss: 215.5515 - val_mse: 215.5516\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 183.2222 - mse: 183.2222 - val_loss: 218.2349 - val_mse: 218.2349\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 181.6964 - mse: 181.6964 - val_loss: 224.5274 - val_mse: 224.5274\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 181.3950 - mse: 181.3950 - val_loss: 219.1648 - val_mse: 219.1648\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 184.6454 - mse: 184.6454 - val_loss: 205.5382 - val_mse: 205.5382\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 183.1341 - mse: 183.1341 - val_loss: 205.6481 - val_mse: 205.6481\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 186.9310 - mse: 186.9310 - val_loss: 210.3132 - val_mse: 210.3133\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 185.8325 - mse: 185.8325 - val_loss: 206.5912 - val_mse: 206.5911\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 186.3007 - mse: 186.3007 - val_loss: 210.3392 - val_mse: 210.3392\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 179.6752 - mse: 179.6753 - val_loss: 203.8363 - val_mse: 203.8363\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 187.2618 - mse: 187.2618 - val_loss: 227.2345 - val_mse: 227.2345\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 182.0209 - mse: 182.0209 - val_loss: 220.6519 - val_mse: 220.6519\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 180.3866 - mse: 180.3866 - val_loss: 207.4092 - val_mse: 207.4092\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 183.1836 - mse: 183.1836 - val_loss: 211.4182 - val_mse: 211.4182\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 184.9046 - mse: 184.9047 - val_loss: 205.5115 - val_mse: 205.5115\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 185.1495 - mse: 185.1496 - val_loss: 203.7513 - val_mse: 203.7513\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 179.0482 - mse: 179.048 - 1s 207us/sample - loss: 177.2630 - mse: 177.2630 - val_loss: 202.7321 - val_mse: 202.7321\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 181.8616 - mse: 181.8616 - val_loss: 221.4802 - val_mse: 221.4802\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 177.0410 - mse: 177.0409 - val_loss: 199.7607 - val_mse: 199.7607\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 184.0005 - mse: 184.0005 - val_loss: 203.4210 - val_mse: 203.4210\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 181.6424 - mse: 181.6424 - val_loss: 200.3029 - val_mse: 200.3028\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 178.6157 - mse: 178.6156 - val_loss: 205.7411 - val_mse: 205.7411\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 184.9162 - mse: 184.9162 - val_loss: 207.3234 - val_mse: 207.3235\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 176.5943 - mse: 176.5943 - val_loss: 213.2761 - val_mse: 213.2760\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 173.9620 - mse: 173.9619 - val_loss: 198.8694 - val_mse: 198.8694\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 173.7477 - mse: 173.7477 - val_loss: 199.0800 - val_mse: 199.0800\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 177.5767 - mse: 177.5767 - val_loss: 202.9099 - val_mse: 202.9100\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 177.9403 - mse: 177.9403 - val_loss: 200.5753 - val_mse: 200.5753\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 176.2642 - mse: 176.2643 - val_loss: 230.6797 - val_mse: 230.6797\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 180.2809 - mse: 180.2809 - val_loss: 204.1142 - val_mse: 204.1142\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 183.3610 - mse: 183.3610 - val_loss: 198.1535 - val_mse: 198.1535\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 180.5439 - mse: 180.5439 - val_loss: 198.1486 - val_mse: 198.1486\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 171.8096 - mse: 171.8097 - val_loss: 242.9052 - val_mse: 242.9052\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 177.5825 - mse: 177.5825 - val_loss: 196.6290 - val_mse: 196.6290\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 178.2537 - mse: 178.2537 - val_loss: 199.1693 - val_mse: 199.1693\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 180.7821 - mse: 180.7821 - val_loss: 195.7095 - val_mse: 195.7094\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 180.0675 - mse: 180.0676 - val_loss: 201.5457 - val_mse: 201.5457\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 179.5756 - mse: 179.5756 - val_loss: 195.3141 - val_mse: 195.3141\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 183.5463 - mse: 183.5463 - val_loss: 196.9755 - val_mse: 196.9755\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 178.3808 - mse: 178.3808 - val_loss: 195.0327 - val_mse: 195.0327\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 179.4273 - mse: 179.4273 - val_loss: 195.1269 - val_mse: 195.1269\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 172.3731 - mse: 172.3731 - val_loss: 200.0583 - val_mse: 200.0583\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 173.6138 - mse: 173.6137 - val_loss: 197.2788 - val_mse: 197.2788\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 176.5513 - mse: 176.5513 - val_loss: 219.4568 - val_mse: 219.4568\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 177.7961 - mse: 177.7961 - val_loss: 194.2404 - val_mse: 194.2404\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 177.5519 - mse: 177.5519 - val_loss: 194.4323 - val_mse: 194.4323\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 1s 194us/sample - loss: 178.8780 - mse: 178.8780 - val_loss: 215.0395 - val_mse: 215.0395\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 180.1680 - mse: 180.1680 - val_loss: 203.7588 - val_mse: 203.7588\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 181.7442 - mse: 181.7442 - val_loss: 199.8295 - val_mse: 199.8295\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 172.9444 - mse: 172.9444 - val_loss: 203.5936 - val_mse: 203.5936\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 174.0020 - mse: 174.0020 - val_loss: 202.4142 - val_mse: 202.4143\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 177.6833 - mse: 177.6833 - val_loss: 195.4398 - val_mse: 195.4399\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 184.5378 - mse: 184.5378 - val_loss: 193.7742 - val_mse: 193.7742\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 171.9328 - mse: 171.9328 - val_loss: 195.9728 - val_mse: 195.9728\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 176.1358 - mse: 176.1357 - val_loss: 192.9288 - val_mse: 192.9288\n",
      "Epoch 139/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 169.8095 - mse: 169.8096 - val_loss: 192.8965 - val_mse: 192.8965\n",
      "Epoch 140/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 175.4083 - mse: 175.4083 - val_loss: 192.1925 - val_mse: 192.1925\n",
      "Epoch 141/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 175.2684 - mse: 175.2684 - val_loss: 196.3087 - val_mse: 196.3087\n",
      "Epoch 142/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 171.2879 - mse: 171.2880 - val_loss: 213.2306 - val_mse: 213.2306\n",
      "Epoch 143/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 178.3137 - mse: 178.3136 - val_loss: 193.5293 - val_mse: 193.5293\n",
      "Epoch 144/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 174.2203 - mse: 174.2203 - val_loss: 192.2388 - val_mse: 192.2387\n",
      "Epoch 145/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 174.7924 - mse: 174.7923 - val_loss: 193.7632 - val_mse: 193.7632\n",
      "Epoch 146/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 177.6680 - mse: 177.6680 - val_loss: 196.2634 - val_mse: 196.2634\n",
      "Epoch 147/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 172.5085 - mse: 172.5085 - val_loss: 218.8154 - val_mse: 218.8154\n",
      "Epoch 148/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 171.3816 - mse: 171.3816 - val_loss: 191.0851 - val_mse: 191.0851\n",
      "Epoch 149/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 176.0088 - mse: 176.0088 - val_loss: 212.8971 - val_mse: 212.8971\n",
      "Epoch 150/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 172.9172 - mse: 172.9172 - val_loss: 197.7833 - val_mse: 197.7834\n",
      "Epoch 151/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 178.6469 - mse: 178.6469 - val_loss: 194.0756 - val_mse: 194.0755\n",
      "Epoch 152/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 172.9685 - mse: 172.9685 - val_loss: 193.9593 - val_mse: 193.9593\n",
      "Epoch 153/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 176.1977 - mse: 176.1977 - val_loss: 190.5113 - val_mse: 190.5113\n",
      "Epoch 154/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 171.8800 - mse: 171.8800 - val_loss: 197.4092 - val_mse: 197.4092\n",
      "Epoch 155/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 175.5972 - mse: 175.5972 - val_loss: 207.6684 - val_mse: 207.6684\n",
      "Epoch 156/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 169.0044 - mse: 169.0044 - val_loss: 190.2765 - val_mse: 190.2765\n",
      "Epoch 157/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 178.6843 - mse: 178.6843 - val_loss: 191.0884 - val_mse: 191.0885\n",
      "Epoch 158/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 172.6245 - mse: 172.6245 - val_loss: 195.9712 - val_mse: 195.9712\n",
      "Epoch 159/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 172.4164 - mse: 172.4164 - val_loss: 189.3317 - val_mse: 189.3317\n",
      "Epoch 160/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 168.5205 - mse: 168.5205 - val_loss: 276.0413 - val_mse: 276.0413\n",
      "Epoch 161/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 175.2779 - mse: 175.277 - 1s 203us/sample - loss: 174.7534 - mse: 174.7534 - val_loss: 190.6269 - val_mse: 190.6269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 180.7193 - mse: 180.7193 - val_loss: 216.5986 - val_mse: 216.5986\n",
      "Epoch 163/200\n",
      "2858/2858 [==============================] - 1s 186us/sample - loss: 175.9367 - mse: 175.9368 - val_loss: 192.6555 - val_mse: 192.6555\n",
      "Epoch 164/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 175.4857 - mse: 175.4857 - val_loss: 192.7569 - val_mse: 192.7568\n",
      "Epoch 165/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 172.7439 - mse: 172.7439 - val_loss: 189.5736 - val_mse: 189.5736\n",
      "Epoch 166/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 176.4492 - mse: 176.4492 - val_loss: 204.7227 - val_mse: 204.7227\n",
      "Epoch 167/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 173.6185 - mse: 173.6184 - val_loss: 190.7435 - val_mse: 190.7435\n",
      "Epoch 168/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 173.3663 - mse: 173.3663 - val_loss: 193.1127 - val_mse: 193.1127\n",
      "Epoch 169/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 173.2289 - mse: 173.2288 - val_loss: 191.8539 - val_mse: 191.8540\n",
      "[CV] ....................................... nl=0, nn=6, total= 1.7min\n",
      "[CV] nl=0, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 644us/sample - loss: 487.1719 - mse: 487.1719 - val_loss: 540.6205 - val_mse: 540.6205\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 430.3133 - mse: 430.3134 - val_loss: 477.2047 - val_mse: 477.2047\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 371.1357 - mse: 371.1358 - val_loss: 390.0778 - val_mse: 390.0778\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 313.7293 - mse: 313.7293 - val_loss: 333.8275 - val_mse: 333.8275\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 265.2813 - mse: 265.2814 - val_loss: 267.7195 - val_mse: 267.7195\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 234.0050 - mse: 234.0050 - val_loss: 230.8675 - val_mse: 230.8676\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 217.0049 - mse: 217.0050 - val_loss: 211.1077 - val_mse: 211.1078\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 204.3593 - mse: 204.3593 - val_loss: 196.2230 - val_mse: 196.2230\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 200.0369 - mse: 200.0370 - val_loss: 194.0370 - val_mse: 194.0370\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 197.8785 - mse: 197.8785 - val_loss: 186.1418 - val_mse: 186.1418\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 194.8899 - mse: 194.8898 - val_loss: 187.6057 - val_mse: 187.6057\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 192.9481 - mse: 192.9481 - val_loss: 183.2863 - val_mse: 183.2862\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 196.9796 - mse: 196.9796 - val_loss: 180.8339 - val_mse: 180.8339\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 190.7571 - mse: 190.7571 - val_loss: 177.9211 - val_mse: 177.9211\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 193.6118 - mse: 193.6118 - val_loss: 180.0121 - val_mse: 180.0121\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 191.2131 - mse: 191.2131 - val_loss: 176.7809 - val_mse: 176.7808\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 181.2436 - mse: 181.2436 - val_loss: 172.0704 - val_mse: 172.0704\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 193.8844 - mse: 193.8844 - val_loss: 176.1222 - val_mse: 176.1223\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 184.7637 - mse: 184.7637 - val_loss: 172.9237 - val_mse: 172.9237\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 188.1967 - mse: 188.1967 - val_loss: 170.4678 - val_mse: 170.4677\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 176.8041 - mse: 176.8041 - val_loss: 171.4475 - val_mse: 171.4475\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 181.3466 - mse: 181.3466 - val_loss: 166.6867 - val_mse: 166.6867\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 181.9905 - mse: 181.9905 - val_loss: 165.8680 - val_mse: 165.8680\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 172.5130 - mse: 172.5131 - val_loss: 162.4779 - val_mse: 162.4779\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 180.9197 - mse: 180.9197 - val_loss: 161.8837 - val_mse: 161.8837\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 178.5791 - mse: 178.5791 - val_loss: 163.1126 - val_mse: 163.1126\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 178.3146 - mse: 178.3146 - val_loss: 162.0934 - val_mse: 162.0934\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 176.2367 - mse: 176.2367 - val_loss: 160.7296 - val_mse: 160.7296\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 176.0967 - mse: 176.0967 - val_loss: 161.5294 - val_mse: 161.5294\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 173.4349 - mse: 173.4349 - val_loss: 161.6228 - val_mse: 161.6228\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 171.0611 - mse: 171.0611 - val_loss: 160.3875 - val_mse: 160.3875\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 173.3961 - mse: 173.3961 - val_loss: 159.2971 - val_mse: 159.2971\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 170.3519 - mse: 170.3519 - val_loss: 157.2235 - val_mse: 157.2235\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 171.9883 - mse: 171.9883 - val_loss: 155.2764 - val_mse: 155.2764\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 166.1698 - mse: 166.1699 - val_loss: 156.7814 - val_mse: 156.7814\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 172.7716 - mse: 172.7715 - val_loss: 157.6605 - val_mse: 157.6606\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 167.9752 - mse: 167.9752 - val_loss: 153.7652 - val_mse: 153.7652\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 171.0146 - mse: 171.0146 - val_loss: 154.6398 - val_mse: 154.6398\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 164.8421 - mse: 164.8421 - val_loss: 152.2607 - val_mse: 152.2607\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 167.6002 - mse: 167.6002 - val_loss: 152.7809 - val_mse: 152.7809\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 168.7646 - mse: 168.7646 - val_loss: 154.9029 - val_mse: 154.9029\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 172.5666 - mse: 172.5666 - val_loss: 152.7273 - val_mse: 152.7273\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 167.7772 - mse: 167.7771 - val_loss: 152.5646 - val_mse: 152.5646\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 165.7664 - mse: 165.7664 - val_loss: 154.4241 - val_mse: 154.4241\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 163.8539 - mse: 163.8539 - val_loss: 153.1210 - val_mse: 153.1211\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 162.9556 - mse: 162.9556 - val_loss: 151.0677 - val_mse: 151.0677\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 211us/sample - loss: 162.5677 - mse: 162.5677 - val_loss: 150.4941 - val_mse: 150.4941\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 160.2957 - mse: 160.2958 - val_loss: 148.5654 - val_mse: 148.5655\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 161.2925 - mse: 161.2925 - val_loss: 147.7696 - val_mse: 147.7696\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 158.0570 - mse: 158.0570 - val_loss: 147.0802 - val_mse: 147.0802\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 164.2673 - mse: 164.2673 - val_loss: 149.0117 - val_mse: 149.0117\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 166.9103 - mse: 166.9102 - val_loss: 152.4979 - val_mse: 152.4979\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 165.5033 - mse: 165.5033 - val_loss: 150.6884 - val_mse: 150.6884\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 160.8556 - mse: 160.8556 - val_loss: 150.3978 - val_mse: 150.3978\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 163.0491 - mse: 163.0490 - val_loss: 145.8834 - val_mse: 145.8834\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 163.4793 - mse: 163.4792 - val_loss: 143.7064 - val_mse: 143.7065\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 161.3644 - mse: 161.3644 - val_loss: 147.8830 - val_mse: 147.8830\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 157.8728 - mse: 157.8728 - val_loss: 148.2583 - val_mse: 148.2583\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 167.7500 - mse: 167.7500 - val_loss: 146.0410 - val_mse: 146.0409\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 157.7398 - mse: 157.7398 - val_loss: 146.3332 - val_mse: 146.3332\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 161.1161 - mse: 161.1161 - val_loss: 146.6838 - val_mse: 146.6838\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 157.4180 - mse: 157.4180 - val_loss: 145.2111 - val_mse: 145.2111\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 161.6336 - mse: 161.6335 - val_loss: 140.6505 - val_mse: 140.6505\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 159.6326 - mse: 159.6326 - val_loss: 141.7259 - val_mse: 141.7259\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 159.6803 - mse: 159.6803 - val_loss: 141.5148 - val_mse: 141.5148\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 155.6983 - mse: 155.6983 - val_loss: 141.1026 - val_mse: 141.1026\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 158.0500 - mse: 158.0500 - val_loss: 140.0239 - val_mse: 140.0239\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 160.2052 - mse: 160.2052 - val_loss: 142.1742 - val_mse: 142.1741\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 157.9242 - mse: 157.9242 - val_loss: 141.6950 - val_mse: 141.6950\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 162.2108 - mse: 162.2108 - val_loss: 139.9434 - val_mse: 139.9434\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 159.9047 - mse: 159.9047 - val_loss: 140.5206 - val_mse: 140.5205\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 155.7773 - mse: 155.7773 - val_loss: 140.8147 - val_mse: 140.8148\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 159.8947 - mse: 159.8947 - val_loss: 143.2639 - val_mse: 143.2639\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 151.3950 - mse: 151.3950 - val_loss: 142.9154 - val_mse: 142.9153\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 164.7754 - mse: 164.7753 - val_loss: 141.7212 - val_mse: 141.7212\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 157.4620 - mse: 157.4620 - val_loss: 141.0582 - val_mse: 141.0582\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 158.2063 - mse: 158.2063 - val_loss: 141.5828 - val_mse: 141.5828\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 190us/sample - loss: 154.5159 - mse: 154.5159 - val_loss: 140.1788 - val_mse: 140.1788\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 149.9822 - mse: 149.9823 - val_loss: 144.1543 - val_mse: 144.1543\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 154.7637 - mse: 154.7637 - val_loss: 144.4932 - val_mse: 144.4932\n",
      "[CV] ....................................... nl=0, nn=6, total=  52.7s\n",
      "[CV] nl=0, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 634us/sample - loss: 471.3551 - mse: 471.3550 - val_loss: 635.4722 - val_mse: 635.4721\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 426.4875 - mse: 426.4876 - val_loss: 578.1383 - val_mse: 578.1382\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 366.0012 - mse: 366.0010 - val_loss: 481.3040 - val_mse: 481.3039\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 310.0985 - mse: 310.0985 - val_loss: 401.3295 - val_mse: 401.3296\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 266.4231 - mse: 266.4231 - val_loss: 346.5191 - val_mse: 346.5192\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 234.5287 - mse: 234.5288 - val_loss: 301.7720 - val_mse: 301.7719\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 211.0015 - mse: 211.0014 - val_loss: 281.6558 - val_mse: 281.6558\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 203.0790 - mse: 203.0790 - val_loss: 266.9484 - val_mse: 266.9485\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 190.9318 - mse: 190.9318 - val_loss: 252.5648 - val_mse: 252.5647\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 188.8925 - mse: 188.8925 - val_loss: 243.2393 - val_mse: 243.2393\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 184.1906 - mse: 184.1906 - val_loss: 242.1941 - val_mse: 242.1942\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 180.4514 - mse: 180.4514 - val_loss: 236.9054 - val_mse: 236.9054\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 183.4964 - mse: 183.4964 - val_loss: 237.5541 - val_mse: 237.5540\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 185.8108 - mse: 185.8108 - val_loss: 229.3091 - val_mse: 229.3091\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 184.6324 - mse: 184.6324 - val_loss: 229.3150 - val_mse: 229.3150\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 180.9561 - mse: 180.9561 - val_loss: 226.4105 - val_mse: 226.4105\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 176.3791 - mse: 176.3792 - val_loss: 224.7109 - val_mse: 224.7109\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 175.7881 - mse: 175.7881 - val_loss: 219.7378 - val_mse: 219.7379\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 177.9463 - mse: 177.9463 - val_loss: 218.6195 - val_mse: 218.6194\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 173.4310 - mse: 173.4309 - val_loss: 215.6051 - val_mse: 215.6052\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 174.6691 - mse: 174.6691 - val_loss: 221.0979 - val_mse: 221.0979\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 172.5576 - mse: 172.5576 - val_loss: 213.5847 - val_mse: 213.5847\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 172.8772 - mse: 172.8772 - val_loss: 210.0287 - val_mse: 210.0287\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 166.2597 - mse: 166.2597 - val_loss: 208.6243 - val_mse: 208.6243\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 161.8796 - mse: 161.879 - 1s 253us/sample - loss: 168.2809 - mse: 168.2809 - val_loss: 205.2198 - val_mse: 205.2198\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 164.4706 - mse: 164.4706 - val_loss: 202.8452 - val_mse: 202.8452\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 168.0812 - mse: 168.0812 - val_loss: 203.3424 - val_mse: 203.3425\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 163.2663 - mse: 163.2663 - val_loss: 202.8610 - val_mse: 202.8611\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 169.4929 - mse: 169.4929 - val_loss: 197.8589 - val_mse: 197.8589\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 159.4439 - mse: 159.4439 - val_loss: 198.1723 - val_mse: 198.1723\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 163.6088 - mse: 163.6088 - val_loss: 196.6660 - val_mse: 196.6661\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 156.3514 - mse: 156.3513 - val_loss: 194.6170 - val_mse: 194.6170\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 155.8792 - mse: 155.8792 - val_loss: 193.1557 - val_mse: 193.1557\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 164.4183 - mse: 164.4183 - val_loss: 189.6621 - val_mse: 189.6621\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 163.3779 - mse: 163.3779 - val_loss: 190.8121 - val_mse: 190.8121\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 306us/sample - loss: 152.2854 - mse: 152.2855 - val_loss: 188.5622 - val_mse: 188.5622\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 278us/sample - loss: 155.3159 - mse: 155.3159 - val_loss: 188.1255 - val_mse: 188.1256\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 154.6172 - mse: 154.6172 - val_loss: 186.0817 - val_mse: 186.0817\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 292us/sample - loss: 155.1412 - mse: 155.1411 - val_loss: 183.8270 - val_mse: 183.8270\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 157.4531 - mse: 157.4531 - val_loss: 182.6513 - val_mse: 182.6513\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 167.2256 - mse: 167.225 - 1s 288us/sample - loss: 162.9076 - mse: 162.9075 - val_loss: 183.5169 - val_mse: 183.5169\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 303us/sample - loss: 157.7984 - mse: 157.7984 - val_loss: 183.4763 - val_mse: 183.4763\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 154.7844 - mse: 154.7844 - val_loss: 181.1978 - val_mse: 181.1978\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 299us/sample - loss: 149.7018 - mse: 149.7018 - val_loss: 180.8802 - val_mse: 180.8802\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 157.6647 - mse: 157.6647 - val_loss: 178.5099 - val_mse: 178.5099\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 151.2497 - mse: 151.2497 - val_loss: 180.9951 - val_mse: 180.9951\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 156.9800 - mse: 156.9800 - val_loss: 176.9700 - val_mse: 176.9700\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 153.1633 - mse: 153.1632 - val_loss: 177.3246 - val_mse: 177.3247\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 297us/sample - loss: 145.5716 - mse: 145.5716 - val_loss: 176.3503 - val_mse: 176.3503\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 152.2952 - mse: 152.2952 - val_loss: 176.0040 - val_mse: 176.0040\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 150.7841 - mse: 150.7841 - val_loss: 175.0817 - val_mse: 175.0817\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 146.3482 - mse: 146.3482 - val_loss: 175.6493 - val_mse: 175.6493\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 146.6445 - mse: 146.6445 - val_loss: 174.5729 - val_mse: 174.5729\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 151.5801 - mse: 151.5801 - val_loss: 173.3245 - val_mse: 173.3245\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 149.0307 - mse: 149.0307 - val_loss: 175.5672 - val_mse: 175.5672\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 148.7337 - mse: 148.7337 - val_loss: 173.6654 - val_mse: 173.6653\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 148.8247 - mse: 148.8247 - val_loss: 173.8317 - val_mse: 173.8317\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 148.9786 - mse: 148.9786 - val_loss: 172.9395 - val_mse: 172.9395\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 149.0846 - mse: 149.0846 - val_loss: 172.2592 - val_mse: 172.2592\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 154.8758 - mse: 154.8758 - val_loss: 173.1300 - val_mse: 173.1299\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 148.0828 - mse: 148.0828 - val_loss: 171.3294 - val_mse: 171.3294\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 144.9423 - mse: 144.9423 - val_loss: 172.6553 - val_mse: 172.6553\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 148.8575 - mse: 148.8576 - val_loss: 172.9173 - val_mse: 172.9173\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 146.7572 - mse: 146.7572 - val_loss: 171.4703 - val_mse: 171.4703\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 139.8224 - mse: 139.8224 - val_loss: 170.4297 - val_mse: 170.4297\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 143.4368 - mse: 143.4368 - val_loss: 169.6021 - val_mse: 169.6021\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 142.8544 - mse: 142.8544 - val_loss: 169.6809 - val_mse: 169.6809\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 149.1210 - mse: 149.1210 - val_loss: 168.8560 - val_mse: 168.8559\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 146.5049 - mse: 146.5049 - val_loss: 170.0702 - val_mse: 170.0703\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 142.1391 - mse: 142.1391 - val_loss: 170.4717 - val_mse: 170.4717\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 148.8314 - mse: 148.8314 - val_loss: 169.9132 - val_mse: 169.9132\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 150.6072 - mse: 150.6072 - val_loss: 168.9997 - val_mse: 168.9996\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 146.2951 - mse: 146.2950 - val_loss: 169.1466 - val_mse: 169.1466\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 147.5015 - mse: 147.5015 - val_loss: 169.4873 - val_mse: 169.4874\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 143.3611 - mse: 143.3611 - val_loss: 170.0692 - val_mse: 170.0692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 137.8868 - mse: 137.8868 - val_loss: 170.2173 - val_mse: 170.2174\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 146.2665 - mse: 146.2665 - val_loss: 169.6480 - val_mse: 169.6480\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 299us/sample - loss: 142.6804 - mse: 142.6805 - val_loss: 171.4531 - val_mse: 171.4532\n",
      "[CV] ....................................... nl=0, nn=6, total=  60.0s\n",
      "[CV] nl=0, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 671us/sample - loss: 512.7731 - mse: 512.7732 - val_loss: 664.7039 - val_mse: 664.7037\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 436.8551 - mse: 436.8550 - val_loss: 584.7737 - val_mse: 584.7737\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 378.0798 - mse: 378.0798 - val_loss: 472.0710 - val_mse: 472.0711\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 326.4090 - mse: 326.4091 - val_loss: 397.9372 - val_mse: 397.9370\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 276.9763 - mse: 276.9763 - val_loss: 342.2930 - val_mse: 342.2931\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 255.2082 - mse: 255.2083 - val_loss: 297.0341 - val_mse: 297.0341\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 229.5499 - mse: 229.5499 - val_loss: 270.9651 - val_mse: 270.9651\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 219.9885 - mse: 219.9885 - val_loss: 260.1146 - val_mse: 260.1146\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 209.3724 - mse: 209.3724 - val_loss: 253.5254 - val_mse: 253.5254\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 209.7391 - mse: 209.7392 - val_loss: 244.7830 - val_mse: 244.7830\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 309us/sample - loss: 205.9032 - mse: 205.9032 - val_loss: 239.6407 - val_mse: 239.6407\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 202.2088 - mse: 202.2088 - val_loss: 238.2635 - val_mse: 238.2635\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 289us/sample - loss: 202.8612 - mse: 202.8611 - val_loss: 230.6509 - val_mse: 230.6509\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 205.6146 - mse: 205.6146 - val_loss: 230.5766 - val_mse: 230.5766\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 297us/sample - loss: 204.3078 - mse: 204.3078 - val_loss: 230.7163 - val_mse: 230.7163\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 197.4726 - mse: 197.4727 - val_loss: 227.8255 - val_mse: 227.8255\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 201.3078 - mse: 201.3079 - val_loss: 229.0790 - val_mse: 229.0789\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 192.3331 - mse: 192.3331 - val_loss: 223.5315 - val_mse: 223.5315\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 196.6633 - mse: 196.6633 - val_loss: 219.2167 - val_mse: 219.2167\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 200.1871 - mse: 200.1871 - val_loss: 217.6289 - val_mse: 217.6289\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 191.3260 - mse: 191.3260 - val_loss: 216.6518 - val_mse: 216.6518\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 183.8824 - mse: 183.8824 - val_loss: 211.5542 - val_mse: 211.5543\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 191.8848 - mse: 191.8849 - val_loss: 211.4372 - val_mse: 211.4371\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 185.3710 - mse: 185.3710 - val_loss: 206.8856 - val_mse: 206.8856\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 183.8237 - mse: 183.8237 - val_loss: 206.3518 - val_mse: 206.3518\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 177.3183 - mse: 177.3184 - val_loss: 203.0725 - val_mse: 203.0725\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 194.2824 - mse: 194.2824 - val_loss: 201.3997 - val_mse: 201.3997\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 176.9520 - mse: 176.9520 - val_loss: 199.8660 - val_mse: 199.8660\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 177.7115 - mse: 177.7115 - val_loss: 201.3669 - val_mse: 201.3669\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 176.1471 - mse: 176.1471 - val_loss: 200.1022 - val_mse: 200.1022\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 181.3442 - mse: 181.3441 - val_loss: 198.7481 - val_mse: 198.7481\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 175.4848 - mse: 175.4848 - val_loss: 196.1040 - val_mse: 196.1040\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 174.4424 - mse: 174.4424 - val_loss: 194.1441 - val_mse: 194.1441\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 170.2694 - mse: 170.2694 - val_loss: 193.3919 - val_mse: 193.3919\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 180.1862 - mse: 180.1862 - val_loss: 194.4112 - val_mse: 194.4113\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 174.7258 - mse: 174.7258 - val_loss: 190.3391 - val_mse: 190.3391\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 167.0319 - mse: 167.0319 - val_loss: 189.3459 - val_mse: 189.3459\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 172.2664 - mse: 172.2664 - val_loss: 186.6424 - val_mse: 186.6424\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 167.7309 - mse: 167.7309 - val_loss: 187.2313 - val_mse: 187.2314\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 168.6316 - mse: 168.6315 - val_loss: 183.1211 - val_mse: 183.1211\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 157.5542 - mse: 157.5542 - val_loss: 188.1998 - val_mse: 188.1998\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 177.2109 - mse: 177.2109 - val_loss: 186.0144 - val_mse: 186.0144\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 164.6348 - mse: 164.6349 - val_loss: 185.0623 - val_mse: 185.0623\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 162.8142 - mse: 162.8142 - val_loss: 181.4355 - val_mse: 181.4355\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 161.3155 - mse: 161.3155 - val_loss: 184.1015 - val_mse: 184.1015\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 165.3240 - mse: 165.3240 - val_loss: 180.7293 - val_mse: 180.7292\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 166.8096 - mse: 166.8096 - val_loss: 177.9384 - val_mse: 177.9384\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 160.0367 - mse: 160.0367 - val_loss: 178.3263 - val_mse: 178.3264\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 161.0840 - mse: 161.0840 - val_loss: 177.4752 - val_mse: 177.4752\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 169.4641 - mse: 169.4642 - val_loss: 179.6298 - val_mse: 179.6297\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 159.4421 - mse: 159.4421 - val_loss: 175.1025 - val_mse: 175.1025\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 164.3181 - mse: 164.3181 - val_loss: 172.5162 - val_mse: 172.5162\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 162.4019 - mse: 162.4019 - val_loss: 171.5982 - val_mse: 171.5981\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 155.5506 - mse: 155.5506 - val_loss: 172.0141 - val_mse: 172.0142\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 157.7488 - mse: 157.7488 - val_loss: 170.3371 - val_mse: 170.3371\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 163.2000 - mse: 163.2000 - val_loss: 171.9274 - val_mse: 171.9274\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 165.0556 - mse: 165.0556 - val_loss: 172.1352 - val_mse: 172.1351\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 156.5195 - mse: 156.5195 - val_loss: 168.4577 - val_mse: 168.4576\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 158.2877 - mse: 158.2877 - val_loss: 170.7384 - val_mse: 170.7384\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 159.5055 - mse: 159.5055 - val_loss: 170.8011 - val_mse: 170.8011\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 157.2740 - mse: 157.2740 - val_loss: 173.9284 - val_mse: 173.9283\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 152.8528 - mse: 152.8528 - val_loss: 166.4417 - val_mse: 166.4417\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 150.1586 - mse: 150.1585 - val_loss: 168.9585 - val_mse: 168.9585\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 150.5651 - mse: 150.5650 - val_loss: 169.6499 - val_mse: 169.6499\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 169.0680 - mse: 169.0680 - val_loss: 168.7166 - val_mse: 168.7166\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 147.4264 - mse: 147.4264 - val_loss: 168.0361 - val_mse: 168.0361\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 149.2477 - mse: 149.2478 - val_loss: 166.3999 - val_mse: 166.3999\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.3893 - mse: 147.3893 - val_loss: 165.7907 - val_mse: 165.7907\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 157.4394 - mse: 157.4394 - val_loss: 167.6582 - val_mse: 167.6582\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 151.6331 - mse: 151.6332 - val_loss: 167.4502 - val_mse: 167.4502\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 151.0532 - mse: 151.0532 - val_loss: 169.7394 - val_mse: 169.7394\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 148.6051 - mse: 148.6051 - val_loss: 170.2440 - val_mse: 170.2440\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 149.5059 - mse: 149.5059 - val_loss: 169.3492 - val_mse: 169.3493\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 150.6397 - mse: 150.6397 - val_loss: 169.2965 - val_mse: 169.2965\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 149.0704 - mse: 149.0704 - val_loss: 168.1206 - val_mse: 168.1206\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 143.2473 - mse: 143.2474 - val_loss: 167.7824 - val_mse: 167.7824\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 150.0762 - mse: 150.0762 - val_loss: 168.3910 - val_mse: 168.3910\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.9290 - mse: 147.9290 - val_loss: 165.4962 - val_mse: 165.4962\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 134.3474 - mse: 134.3474 - val_loss: 167.8758 - val_mse: 167.8759\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 147.7538 - mse: 147.7538 - val_loss: 165.3687 - val_mse: 165.3687\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 151.6491 - mse: 151.6491 - val_loss: 165.0312 - val_mse: 165.0311\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 152.7100 - mse: 152.7099 - val_loss: 163.8442 - val_mse: 163.8442\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 148.6932 - mse: 148.6932 - val_loss: 165.4286 - val_mse: 165.4286\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 145.6275 - mse: 145.6275 - val_loss: 166.8079 - val_mse: 166.8079\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 149.4129 - mse: 149.4129 - val_loss: 165.1058 - val_mse: 165.1058\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 152.1343 - mse: 152.1343 - val_loss: 165.7620 - val_mse: 165.7619\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 151.4714 - mse: 151.4714 - val_loss: 167.4664 - val_mse: 167.4664\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 147.1887 - mse: 147.1887 - val_loss: 166.5309 - val_mse: 166.5309\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 153.0903 - mse: 153.0903 - val_loss: 162.4470 - val_mse: 162.4470\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 137.6633 - mse: 137.6632 - val_loss: 164.0659 - val_mse: 164.0659\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 152.0038 - mse: 152.0038 - val_loss: 163.1390 - val_mse: 163.1389\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 143.2999 - mse: 143.2999 - val_loss: 163.4772 - val_mse: 163.4772\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 145.4460 - mse: 145.4460 - val_loss: 161.8970 - val_mse: 161.8970\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 144.4932 - mse: 144.4931 - val_loss: 163.3071 - val_mse: 163.3071\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 145.8192 - mse: 145.8192 - val_loss: 164.4307 - val_mse: 164.4307\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 145.4459 - mse: 145.4458 - val_loss: 166.7477 - val_mse: 166.7476\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 137.1387 - mse: 137.1387 - val_loss: 163.3004 - val_mse: 163.3004\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 142.8363 - mse: 142.8362 - val_loss: 162.7209 - val_mse: 162.7209\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 145.6401 - mse: 145.6402 - val_loss: 164.8565 - val_mse: 164.8565\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 144.9290 - mse: 144.9290 - val_loss: 162.1704 - val_mse: 162.1705\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 141.6702 - mse: 141.6702 - val_loss: 161.6999 - val_mse: 161.6999\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 146.2732 - mse: 146.2732 - val_loss: 165.2894 - val_mse: 165.2894\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 144.0536 - mse: 144.0536 - val_loss: 162.9114 - val_mse: 162.9114\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 143.9862 - mse: 143.9862 - val_loss: 161.9654 - val_mse: 161.9653\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 142.4738 - mse: 142.4739 - val_loss: 164.7667 - val_mse: 164.7667\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 144.5418 - mse: 144.5418 - val_loss: 162.3922 - val_mse: 162.3922\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 254us/sample - loss: 139.6451 - mse: 139.6451 - val_loss: 164.5904 - val_mse: 164.5903\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 141.3706 - mse: 141.3706 - val_loss: 165.6364 - val_mse: 165.6364\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 139.9283 - mse: 139.9284 - val_loss: 164.1822 - val_mse: 164.1823\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 136.4857 - mse: 136.4857 - val_loss: 160.9997 - val_mse: 160.9997\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 143.6663 - mse: 143.6662 - val_loss: 163.3401 - val_mse: 163.3401\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 139.6357 - mse: 139.6356 - val_loss: 164.6828 - val_mse: 164.6828\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 142.0967 - mse: 142.0967 - val_loss: 163.5348 - val_mse: 163.5349\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 137.4997 - mse: 137.4997 - val_loss: 162.2995 - val_mse: 162.2995\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 133.4575 - mse: 133.4575 - val_loss: 160.1663 - val_mse: 160.1662\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 142.4861 - mse: 142.4861 - val_loss: 159.6429 - val_mse: 159.6429\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 139.0780 - mse: 139.0780 - val_loss: 164.7680 - val_mse: 164.7680\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 140.3821 - mse: 140.3821 - val_loss: 159.1248 - val_mse: 159.1248\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 137.4634 - mse: 137.4634 - val_loss: 161.9490 - val_mse: 161.9490\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 134.3517 - mse: 134.3517 - val_loss: 162.8619 - val_mse: 162.8620\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 137.1980 - mse: 137.1980 - val_loss: 166.0191 - val_mse: 166.0191\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 137.4739 - mse: 137.4739 - val_loss: 165.0931 - val_mse: 165.0931\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 142.3906 - mse: 142.3907 - val_loss: 162.9823 - val_mse: 162.9823\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 139.7796 - mse: 139.7796 - val_loss: 159.5004 - val_mse: 159.5004\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 135.1718 - mse: 135.1719 - val_loss: 161.9495 - val_mse: 161.9495\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 130.4664 - mse: 130.4664 - val_loss: 164.3544 - val_mse: 164.3545\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 139.2755 - mse: 139.2754 - val_loss: 162.5624 - val_mse: 162.5624\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 133.1947 - mse: 133.1947 - val_loss: 160.9680 - val_mse: 160.9680\n",
      "[CV] ...................................... nl=0, nn=12, total= 1.6min\n",
      "[CV] nl=0, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 668us/sample - loss: 458.7933 - mse: 458.7932 - val_loss: 659.7521 - val_mse: 659.7523\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 408.9763 - mse: 408.9762 - val_loss: 587.7957 - val_mse: 587.7957\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 352.6576 - mse: 352.6576 - val_loss: 503.4904 - val_mse: 503.4904\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 298.3461 - mse: 298.3462 - val_loss: 428.3863 - val_mse: 428.3863\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 253.7302 - mse: 253.7302 - val_loss: 369.3165 - val_mse: 369.3165\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 228.4912 - mse: 228.4912 - val_loss: 338.4420 - val_mse: 338.4420\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 210.2465 - mse: 210.2465 - val_loss: 311.9088 - val_mse: 311.9088\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 201.2420 - mse: 201.2420 - val_loss: 294.6914 - val_mse: 294.6914\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 191.7622 - mse: 191.7622 - val_loss: 286.7961 - val_mse: 286.7961\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 187.7431 - mse: 187.7431 - val_loss: 281.6819 - val_mse: 281.6820\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 185.5649 - mse: 185.5650 - val_loss: 274.1354 - val_mse: 274.1354\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 181.5178 - mse: 181.5178 - val_loss: 267.0889 - val_mse: 267.0889\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 188.2487 - mse: 188.2487 - val_loss: 266.3653 - val_mse: 266.3653\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 186.6381 - mse: 186.6382 - val_loss: 262.2965 - val_mse: 262.2965\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 179.4634 - mse: 179.4634 - val_loss: 258.5307 - val_mse: 258.5307\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 178.9452 - mse: 178.9452 - val_loss: 257.9931 - val_mse: 257.9931\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 180.1691 - mse: 180.1691 - val_loss: 255.7008 - val_mse: 255.7008\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 181.7731 - mse: 181.7731 - val_loss: 252.4624 - val_mse: 252.4623\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 174.9744 - mse: 174.9745 - val_loss: 252.0344 - val_mse: 252.0344\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 169.8295 - mse: 169.8295 - val_loss: 251.4252 - val_mse: 251.4251\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 171.6893 - mse: 171.6894 - val_loss: 250.2626 - val_mse: 250.2626\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 169.8224 - mse: 169.8225 - val_loss: 244.3885 - val_mse: 244.3885\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 171.6861 - mse: 171.6861 - val_loss: 248.9402 - val_mse: 248.9403\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 171.4920 - mse: 171.4920 - val_loss: 244.4832 - val_mse: 244.4833\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 171.6353 - mse: 171.6353 - val_loss: 238.8242 - val_mse: 238.8242\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 166.4795 - mse: 166.4795 - val_loss: 240.6689 - val_mse: 240.6690\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 175.0887 - mse: 175.0888 - val_loss: 238.4317 - val_mse: 238.4318\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 172.2635 - mse: 172.2635 - val_loss: 241.6330 - val_mse: 241.6330\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 166.6688 - mse: 166.6688 - val_loss: 237.3296 - val_mse: 237.3296\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 171.8813 - mse: 171.8813 - val_loss: 235.7470 - val_mse: 235.7469\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 172.8385 - mse: 172.8385 - val_loss: 234.9218 - val_mse: 234.9218\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 166.3617 - mse: 166.3617 - val_loss: 231.7053 - val_mse: 231.7054\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 200us/sample - loss: 172.3790 - mse: 172.3791 - val_loss: 230.8317 - val_mse: 230.8317\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 172.1771 - mse: 172.1771 - val_loss: 232.3630 - val_mse: 232.3630\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 172.5988 - mse: 172.5988 - val_loss: 237.9558 - val_mse: 237.9558\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 171.5212 - mse: 171.5212 - val_loss: 228.1738 - val_mse: 228.1738\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 166.9322 - mse: 166.9321 - val_loss: 225.9862 - val_mse: 225.9861\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 166.6230 - mse: 166.6230 - val_loss: 230.4157 - val_mse: 230.4158\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 168.4867 - mse: 168.4867 - val_loss: 233.4820 - val_mse: 233.4820\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 167.2476 - mse: 167.2476 - val_loss: 222.5473 - val_mse: 222.5473\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 163.3437 - mse: 163.3437 - val_loss: 228.1990 - val_mse: 228.1990\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 168.5874 - mse: 168.5873 - val_loss: 222.6381 - val_mse: 222.6381\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 165.8844 - mse: 165.8844 - val_loss: 225.0635 - val_mse: 225.0635\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 169.0666 - mse: 169.0665 - val_loss: 224.8999 - val_mse: 224.8999\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 167.5097 - mse: 167.5097 - val_loss: 221.4642 - val_mse: 221.4641\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 159.9631 - mse: 159.9631 - val_loss: 229.6483 - val_mse: 229.6484\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 161.7878 - mse: 161.7877 - val_loss: 214.9191 - val_mse: 214.9191\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 164.9901 - mse: 164.9901 - val_loss: 216.0262 - val_mse: 216.0262\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 166.3009 - mse: 166.3010 - val_loss: 216.2918 - val_mse: 216.2917\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 161.0683 - mse: 161.0683 - val_loss: 213.9584 - val_mse: 213.9583\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 166.9642 - mse: 166.9641 - val_loss: 214.7880 - val_mse: 214.7881\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 162.6039 - mse: 162.6039 - val_loss: 211.3982 - val_mse: 211.3982\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 159.1586 - mse: 159.1586 - val_loss: 216.0551 - val_mse: 216.0552\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 158.5507 - mse: 158.5508 - val_loss: 210.0455 - val_mse: 210.0456\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 166.9409 - mse: 166.940 - 1s 249us/sample - loss: 162.4945 - mse: 162.4946 - val_loss: 205.3934 - val_mse: 205.3934\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 158.4109 - mse: 158.4109 - val_loss: 207.0806 - val_mse: 207.0806\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 161.8939 - mse: 161.8939 - val_loss: 200.9558 - val_mse: 200.9558\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 158.3112 - mse: 158.3112 - val_loss: 204.3306 - val_mse: 204.3306\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 156.8528 - mse: 156.8528 - val_loss: 200.8488 - val_mse: 200.8487\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 158.3605 - mse: 158.3606 - val_loss: 205.4231 - val_mse: 205.4231\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 157.6706 - mse: 157.6706 - val_loss: 200.1111 - val_mse: 200.1111\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 156.1964 - mse: 156.1964 - val_loss: 202.8915 - val_mse: 202.8914\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 153.6970 - mse: 153.6969 - val_loss: 199.7247 - val_mse: 199.7248\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 157.0243 - mse: 157.0242 - val_loss: 195.7101 - val_mse: 195.7101\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 153.8759 - mse: 153.8759 - val_loss: 204.7345 - val_mse: 204.7345\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 150.4231 - mse: 150.4231 - val_loss: 195.0684 - val_mse: 195.0684\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 151.9085 - mse: 151.9085 - val_loss: 193.8294 - val_mse: 193.8295\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 154.1972 - mse: 154.1971 - val_loss: 199.7128 - val_mse: 199.7128\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 148.5225 - mse: 148.5224 - val_loss: 196.6398 - val_mse: 196.6398\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 148.7344 - mse: 148.7343 - val_loss: 191.8618 - val_mse: 191.8618\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 148.1685 - mse: 148.1685 - val_loss: 195.4994 - val_mse: 195.4994\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 157.1474 - mse: 157.1474 - val_loss: 194.5009 - val_mse: 194.5009\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 151.5437 - mse: 151.5436 - val_loss: 195.2654 - val_mse: 195.2654\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 153.1895 - mse: 153.1895 - val_loss: 199.7739 - val_mse: 199.7739\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 152.4049 - mse: 152.4049 - val_loss: 191.5040 - val_mse: 191.5041\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 146.6094 - mse: 146.6094 - val_loss: 194.2366 - val_mse: 194.2366\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 153.9937 - mse: 153.9937 - val_loss: 191.9953 - val_mse: 191.9953\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 150.2268 - mse: 150.2268 - val_loss: 195.9240 - val_mse: 195.9240\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 303us/sample - loss: 149.1358 - mse: 149.1359 - val_loss: 186.0213 - val_mse: 186.0213\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 148.0523 - mse: 148.0523 - val_loss: 194.4295 - val_mse: 194.4294\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 153.4020 - mse: 153.4021 - val_loss: 187.8997 - val_mse: 187.8997\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 152.1616 - mse: 152.1616 - val_loss: 185.8267 - val_mse: 185.8267\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 147.0565 - mse: 147.0565 - val_loss: 188.9702 - val_mse: 188.9702\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 155.1141 - mse: 155.1140 - val_loss: 190.6164 - val_mse: 190.6165\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 145.5560 - mse: 145.5560 - val_loss: 187.1048 - val_mse: 187.1048\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 154.1960 - mse: 154.1960 - val_loss: 186.1175 - val_mse: 186.1175\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 145.1021 - mse: 145.1021 - val_loss: 189.9267 - val_mse: 189.9267\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 150.6031 - mse: 150.6031 - val_loss: 186.7463 - val_mse: 186.7462\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 149.9719 - mse: 149.9719 - val_loss: 186.5294 - val_mse: 186.5294\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 148.1958 - mse: 148.1958 - val_loss: 190.2423 - val_mse: 190.2423\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 148.4444 - mse: 148.4444 - val_loss: 201.5441 - val_mse: 201.5441\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.1970 - mse: 147.1970 - val_loss: 190.1339 - val_mse: 190.1339\n",
      "[CV] ...................................... nl=0, nn=12, total= 1.1min\n",
      "[CV] nl=0, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 615us/sample - loss: 540.6069 - mse: 540.6069 - val_loss: 655.9452 - val_mse: 655.9453\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 464.5772 - mse: 464.5771 - val_loss: 585.4123 - val_mse: 585.4122\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 397.2409 - mse: 397.2408 - val_loss: 469.5887 - val_mse: 469.5887\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 341.2648 - mse: 341.2648 - val_loss: 385.3310 - val_mse: 385.3309\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 292.0438 - mse: 292.0439 - val_loss: 333.3257 - val_mse: 333.3258\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 260.0274 - mse: 260.0274 - val_loss: 305.9900 - val_mse: 305.9900\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 238.0580 - mse: 238.0581 - val_loss: 268.2621 - val_mse: 268.2621\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 222.2426 - mse: 222.2426 - val_loss: 251.3711 - val_mse: 251.3710\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 207.4746 - mse: 207.4745 - val_loss: 238.4444 - val_mse: 238.4445\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 202.2878 - mse: 202.2878 - val_loss: 230.1645 - val_mse: 230.1645\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 212.5376 - mse: 212.5376 - val_loss: 222.0067 - val_mse: 222.0067\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 202.9493 - mse: 202.9492 - val_loss: 216.7768 - val_mse: 216.7768\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 201.5747 - mse: 201.5747 - val_loss: 218.5025 - val_mse: 218.5025\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 202.5256 - mse: 202.5256 - val_loss: 214.5449 - val_mse: 214.5448\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 198.1691 - mse: 198.1690 - val_loss: 209.9076 - val_mse: 209.9076\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 195.4342 - mse: 195.4341 - val_loss: 203.8321 - val_mse: 203.8321\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 186.1536 - mse: 186.1536 - val_loss: 203.2806 - val_mse: 203.2806\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 184.9087 - mse: 184.9088 - val_loss: 199.2998 - val_mse: 199.2998\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 188.6502 - mse: 188.6502 - val_loss: 195.2995 - val_mse: 195.2995\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 185.2704 - mse: 185.2705 - val_loss: 197.1561 - val_mse: 197.1561\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 183.2574 - mse: 183.2575 - val_loss: 193.6919 - val_mse: 193.6919\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 185.2651 - mse: 185.2651 - val_loss: 191.8398 - val_mse: 191.8398\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 175.7469 - mse: 175.746 - 1s 252us/sample - loss: 180.4518 - mse: 180.4518 - val_loss: 188.9097 - val_mse: 188.9096\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 179.9896 - mse: 179.9896 - val_loss: 186.2862 - val_mse: 186.2862\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 173.4226 - mse: 173.4226 - val_loss: 187.1003 - val_mse: 187.1003\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 182.6600 - mse: 182.6600 - val_loss: 187.3613 - val_mse: 187.3613\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 172.3260 - mse: 172.3260 - val_loss: 184.8082 - val_mse: 184.8082\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 171.7991 - mse: 171.7991 - val_loss: 186.1961 - val_mse: 186.1960\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 177.9966 - mse: 177.9966 - val_loss: 180.2653 - val_mse: 180.2653\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 173.4208 - mse: 173.4207 - val_loss: 180.6840 - val_mse: 180.6839\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 169.2378 - mse: 169.2378 - val_loss: 178.4013 - val_mse: 178.4013\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 168.0310 - mse: 168.0310 - val_loss: 179.7638 - val_mse: 179.7638\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 165.2922 - mse: 165.2923 - val_loss: 180.7994 - val_mse: 180.7994\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 169.5278 - mse: 169.5278 - val_loss: 179.3590 - val_mse: 179.3590\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 177.4520 - mse: 177.4520 - val_loss: 181.3243 - val_mse: 181.3242\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 169.6282 - mse: 169.6282 - val_loss: 177.8440 - val_mse: 177.8440\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 174.9024 - mse: 174.9024 - val_loss: 177.3938 - val_mse: 177.3938\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 170.1357 - mse: 170.1357 - val_loss: 178.6555 - val_mse: 178.6555\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 170.9953 - mse: 170.9953 - val_loss: 176.6968 - val_mse: 176.6968\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 178.0969 - mse: 178.0969 - val_loss: 178.2469 - val_mse: 178.2469\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 169.1547 - mse: 169.1546 - val_loss: 179.5395 - val_mse: 179.5395\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 173.7290 - mse: 173.7290 - val_loss: 175.5515 - val_mse: 175.5515\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 174.7136 - mse: 174.7135 - val_loss: 176.2030 - val_mse: 176.2030\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 161.4466 - mse: 161.4466 - val_loss: 177.9790 - val_mse: 177.9790\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 171.2485 - mse: 171.2485 - val_loss: 173.2667 - val_mse: 173.2668\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 165.5389 - mse: 165.5389 - val_loss: 173.2186 - val_mse: 173.2186\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 164.8887 - mse: 164.8887 - val_loss: 176.8254 - val_mse: 176.8254\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 166.1641 - mse: 166.1641 - val_loss: 172.1022 - val_mse: 172.1022\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 172.1132 - mse: 172.1132 - val_loss: 175.8482 - val_mse: 175.8482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 165.9756 - mse: 165.9756 - val_loss: 172.5330 - val_mse: 172.5330\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 170.9275 - mse: 170.9275 - val_loss: 173.5345 - val_mse: 173.5345\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 167.3050 - mse: 167.3050 - val_loss: 174.1993 - val_mse: 174.1993\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 156.6836 - mse: 156.6836 - val_loss: 172.0419 - val_mse: 172.0419\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 171.5412 - mse: 171.5411 - val_loss: 169.4056 - val_mse: 169.4055\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 161.5199 - mse: 161.5199 - val_loss: 167.8992 - val_mse: 167.8992\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 169.8856 - mse: 169.8856 - val_loss: 169.4132 - val_mse: 169.4132\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 161.8397 - mse: 161.8397 - val_loss: 168.6551 - val_mse: 168.6551\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 160.7258 - mse: 160.7259 - val_loss: 167.1480 - val_mse: 167.1481\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 168.8288 - mse: 168.8289 - val_loss: 165.6703 - val_mse: 165.6703\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 167.0975 - mse: 167.0974 - val_loss: 163.3505 - val_mse: 163.3505\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 160.5758 - mse: 160.5758 - val_loss: 165.2072 - val_mse: 165.2072\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 164.1136 - mse: 164.1136 - val_loss: 166.7769 - val_mse: 166.7769\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 158.1736 - mse: 158.1736 - val_loss: 166.4378 - val_mse: 166.4378\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 161.7623 - mse: 161.7623 - val_loss: 164.9235 - val_mse: 164.9235\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 166.0032 - mse: 166.0033 - val_loss: 163.5892 - val_mse: 163.5892\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 162.5502 - mse: 162.5502 - val_loss: 164.1090 - val_mse: 164.1090\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 157.8981 - mse: 157.8981 - val_loss: 163.4796 - val_mse: 163.4795\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 159.8295 - mse: 159.8295 - val_loss: 163.0067 - val_mse: 163.0067\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 167.3393 - mse: 167.3393 - val_loss: 163.0539 - val_mse: 163.0539\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 159.4353 - mse: 159.4353 - val_loss: 161.3734 - val_mse: 161.3734\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 154.4390 - mse: 154.4390 - val_loss: 161.6686 - val_mse: 161.6686\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 162.4484 - mse: 162.4483 - val_loss: 162.6702 - val_mse: 162.6703\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 155.7992 - mse: 155.7993 - val_loss: 160.5088 - val_mse: 160.5088\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 160.9758 - mse: 160.9758 - val_loss: 160.7378 - val_mse: 160.7378\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 154.9555 - mse: 154.9556 - val_loss: 162.3138 - val_mse: 162.3138\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 161.1831 - mse: 161.1831 - val_loss: 161.5648 - val_mse: 161.5648\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 159.7512 - mse: 159.7512 - val_loss: 160.6128 - val_mse: 160.6128\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 156.1881 - mse: 156.1882 - val_loss: 159.3136 - val_mse: 159.3135\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 159.7317 - mse: 159.7318 - val_loss: 158.0406 - val_mse: 158.0406\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 162.6043 - mse: 162.6043 - val_loss: 159.8392 - val_mse: 159.8392\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 158.5523 - mse: 158.5524 - val_loss: 158.2040 - val_mse: 158.2040\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 152.7139 - mse: 152.7138 - val_loss: 158.9207 - val_mse: 158.9207\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 160.5683 - mse: 160.5683 - val_loss: 155.7557 - val_mse: 155.7557\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 165.9240 - mse: 165.9240 - val_loss: 154.5535 - val_mse: 154.5535\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 164.1050 - mse: 164.1050 - val_loss: 155.6645 - val_mse: 155.6645\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 157.0692 - mse: 157.0692 - val_loss: 157.0211 - val_mse: 157.0211\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 151.7242 - mse: 151.7242 - val_loss: 156.0410 - val_mse: 156.0410\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 157.5981 - mse: 157.5981 - val_loss: 156.0067 - val_mse: 156.0066\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 154.0181 - mse: 154.0181 - val_loss: 155.9681 - val_mse: 155.9681\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 160.4126 - mse: 160.4126 - val_loss: 156.3743 - val_mse: 156.3743\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 152.7927 - mse: 152.7927 - val_loss: 157.1817 - val_mse: 157.1817\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 159.9552 - mse: 159.9553 - val_loss: 156.1824 - val_mse: 156.1824\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 164.2335 - mse: 164.2335 - val_loss: 154.5031 - val_mse: 154.5031\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 146.8439 - mse: 146.8439 - val_loss: 155.8296 - val_mse: 155.8296\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 154.5244 - mse: 154.5244 - val_loss: 155.6134 - val_mse: 155.6134\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 155.9352 - mse: 155.9352 - val_loss: 151.5047 - val_mse: 151.5047\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 156.8785 - mse: 156.8785 - val_loss: 155.5277 - val_mse: 155.5278\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 164.8397 - mse: 164.8397 - val_loss: 151.9666 - val_mse: 151.9666\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 159.4154 - mse: 159.4154 - val_loss: 158.8158 - val_mse: 158.8159\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 157.7616 - mse: 157.7616 - val_loss: 151.5600 - val_mse: 151.5600\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 149.0034 - mse: 149.0034 - val_loss: 152.5599 - val_mse: 152.5600\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 155.2035 - mse: 155.2035 - val_loss: 151.4782 - val_mse: 151.4782\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 150.5410 - mse: 150.5410 - val_loss: 153.0498 - val_mse: 153.0498\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 157.0907 - mse: 157.0906 - val_loss: 151.9925 - val_mse: 151.9925\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 162.1771 - mse: 162.1771 - val_loss: 151.5042 - val_mse: 151.5043\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.9494 - mse: 147.9494 - val_loss: 151.6378 - val_mse: 151.6378\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 146.9825 - mse: 146.9825 - val_loss: 148.8680 - val_mse: 148.8680\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 150.1249 - mse: 150.1249 - val_loss: 152.3382 - val_mse: 152.3382\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 160.0883 - mse: 160.0883 - val_loss: 150.4378 - val_mse: 150.4378\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 150.1200 - mse: 150.1200 - val_loss: 148.6711 - val_mse: 148.6711\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 153.5679 - mse: 153.5680 - val_loss: 145.9414 - val_mse: 145.9413\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 147.4079 - mse: 147.4079 - val_loss: 151.0757 - val_mse: 151.0757\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 156.9688 - mse: 156.9688 - val_loss: 152.5274 - val_mse: 152.5274\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 154.3233 - mse: 154.3233 - val_loss: 150.9156 - val_mse: 150.9156\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 146.7732 - mse: 146.7732 - val_loss: 153.0184 - val_mse: 153.0183\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 149.0660 - mse: 149.0660 - val_loss: 151.8865 - val_mse: 151.8864\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 150.8310 - mse: 150.8310 - val_loss: 151.7091 - val_mse: 151.7091\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 144.4818 - mse: 144.4818 - val_loss: 146.6577 - val_mse: 146.6577\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 148.4794 - mse: 148.4794 - val_loss: 149.2885 - val_mse: 149.2885\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 138.6689 - mse: 138.6689 - val_loss: 148.6353 - val_mse: 148.6353\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 151.4905 - mse: 151.4905 - val_loss: 147.1048 - val_mse: 147.1048\n",
      "[CV] ...................................... nl=0, nn=12, total= 1.5min\n",
      "[CV] nl=0, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 672us/sample - loss: 491.3098 - mse: 491.3097 - val_loss: 535.0655 - val_mse: 535.0657\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 433.8254 - mse: 433.8254 - val_loss: 496.4122 - val_mse: 496.4123\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 377.2598 - mse: 377.2598 - val_loss: 413.5747 - val_mse: 413.5747\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 319.5182 - mse: 319.5181 - val_loss: 324.1715 - val_mse: 324.1715\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 263.9930 - mse: 263.9930 - val_loss: 262.7200 - val_mse: 262.7199\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 229.5940 - mse: 229.5940 - val_loss: 213.5014 - val_mse: 213.5014\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 205.0194 - mse: 205.0194 - val_loss: 184.9752 - val_mse: 184.9753\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 187.8499 - mse: 187.8499 - val_loss: 171.1517 - val_mse: 171.1517\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 180.1190 - mse: 180.1190 - val_loss: 162.5115 - val_mse: 162.5115\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 175.1547 - mse: 175.1547 - val_loss: 156.9500 - val_mse: 156.9500\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 173.4051 - mse: 173.4051 - val_loss: 150.1181 - val_mse: 150.1180\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 172.0555 - mse: 172.0555 - val_loss: 146.3723 - val_mse: 146.3723\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 169.2838 - mse: 169.2839 - val_loss: 143.3130 - val_mse: 143.3130\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 169.2324 - mse: 169.2324 - val_loss: 141.0750 - val_mse: 141.0750\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 170.3705 - mse: 170.3704 - val_loss: 140.2090 - val_mse: 140.2090\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 161.8041 - mse: 161.8041 - val_loss: 136.3515 - val_mse: 136.3514\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 165.8107 - mse: 165.8107 - val_loss: 135.6736 - val_mse: 135.6736\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 162.2178 - mse: 162.2179 - val_loss: 134.1717 - val_mse: 134.1717\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 161.6748 - mse: 161.6748 - val_loss: 135.8498 - val_mse: 135.8498\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 163.7133 - mse: 163.7134 - val_loss: 133.0007 - val_mse: 133.0007\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 158.5889 - mse: 158.5889 - val_loss: 130.5890 - val_mse: 130.5890\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 162.5904 - mse: 162.5904 - val_loss: 131.5798 - val_mse: 131.5798\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 152.9826 - mse: 152.9826 - val_loss: 130.7713 - val_mse: 130.7713\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 158.2373 - mse: 158.2374 - val_loss: 126.9145 - val_mse: 126.9145\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 157.2590 - mse: 157.2590 - val_loss: 127.3252 - val_mse: 127.3252\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 153.8551 - mse: 153.8551 - val_loss: 126.5845 - val_mse: 126.5845\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 155.4931 - mse: 155.4931 - val_loss: 125.7990 - val_mse: 125.7990\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 153.9246 - mse: 153.9246 - val_loss: 127.1842 - val_mse: 127.1842\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 152.7647 - mse: 152.7646 - val_loss: 126.8896 - val_mse: 126.8896\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 158.7102 - mse: 158.7103 - val_loss: 124.6215 - val_mse: 124.6216\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 160.4433 - mse: 160.4434 - val_loss: 125.7547 - val_mse: 125.7547\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 153.8082 - mse: 153.8082 - val_loss: 124.3741 - val_mse: 124.3742\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 148.8771 - mse: 148.8771 - val_loss: 123.6906 - val_mse: 123.6906\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 147.3166 - mse: 147.3166 - val_loss: 124.2326 - val_mse: 124.2326\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 151.1565 - mse: 151.1565 - val_loss: 122.8101 - val_mse: 122.8101\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 157.2479 - mse: 157.2479 - val_loss: 122.6698 - val_mse: 122.6697\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 152.6475 - mse: 152.6475 - val_loss: 124.9398 - val_mse: 124.9398\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 254us/sample - loss: 151.1746 - mse: 151.1746 - val_loss: 123.2820 - val_mse: 123.2821\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 150.7140 - mse: 150.7140 - val_loss: 122.3479 - val_mse: 122.3479\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 146.8079 - mse: 146.8078 - val_loss: 122.8826 - val_mse: 122.8826\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 152.6522 - mse: 152.6522 - val_loss: 123.4931 - val_mse: 123.4931\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 143.9136 - mse: 143.9135 - val_loss: 123.1651 - val_mse: 123.1651\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 150.0489 - mse: 150.0490 - val_loss: 122.9567 - val_mse: 122.9567\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 145.7762 - mse: 145.7763 - val_loss: 124.5038 - val_mse: 124.5038\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 149.2867 - mse: 149.2868 - val_loss: 121.7725 - val_mse: 121.7725\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 149.9091 - mse: 149.9091 - val_loss: 122.5120 - val_mse: 122.5120\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 145.6294 - mse: 145.6295 - val_loss: 121.9389 - val_mse: 121.9390\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 148.4222 - mse: 148.4222 - val_loss: 121.5442 - val_mse: 121.5442\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 137.4066 - mse: 137.4066 - val_loss: 121.7594 - val_mse: 121.7594\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 149.7718 - mse: 149.7718 - val_loss: 121.5860 - val_mse: 121.5860\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 153.9723 - mse: 153.9723 - val_loss: 121.4542 - val_mse: 121.4542\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 145.2298 - mse: 145.2298 - val_loss: 122.1062 - val_mse: 122.1061\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 152.7474 - mse: 152.7474 - val_loss: 122.6292 - val_mse: 122.6292\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 141.3889 - mse: 141.3890 - val_loss: 121.3719 - val_mse: 121.3719\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 140.4462 - mse: 140.4461 - val_loss: 120.9804 - val_mse: 120.9804\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 146.1789 - mse: 146.1789 - val_loss: 122.5959 - val_mse: 122.5959\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 143.9088 - mse: 143.9087 - val_loss: 122.4502 - val_mse: 122.4502\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 142.9372 - mse: 142.9372 - val_loss: 121.0520 - val_mse: 121.0520\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 142.7871 - mse: 142.7871 - val_loss: 123.2704 - val_mse: 123.2704\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 147.0924 - mse: 147.0924 - val_loss: 121.6657 - val_mse: 121.6657\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 147.2602 - mse: 147.2602 - val_loss: 120.8152 - val_mse: 120.8152\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 143.1076 - mse: 143.1076 - val_loss: 120.8791 - val_mse: 120.8791\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 143.1198 - mse: 143.1198 - val_loss: 120.4119 - val_mse: 120.4119\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 138.2317 - mse: 138.2317 - val_loss: 120.4071 - val_mse: 120.4071\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 139.5279 - mse: 139.5279 - val_loss: 120.9809 - val_mse: 120.9808\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 140.4404 - mse: 140.4404 - val_loss: 120.7114 - val_mse: 120.7114\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 139.3388 - mse: 139.3388 - val_loss: 120.3286 - val_mse: 120.3286\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 136.3857 - mse: 136.3857 - val_loss: 122.3383 - val_mse: 122.3383\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 139.7870 - mse: 139.7870 - val_loss: 121.6085 - val_mse: 121.6085\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 134.8582 - mse: 134.8582 - val_loss: 121.9137 - val_mse: 121.9137\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 140.7713 - mse: 140.7713 - val_loss: 122.9802 - val_mse: 122.9802\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 137.1556 - mse: 137.1556 - val_loss: 121.6240 - val_mse: 121.6239\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 138.1539 - mse: 138.1539 - val_loss: 122.6314 - val_mse: 122.6314\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 133.4251 - mse: 133.4250 - val_loss: 123.1922 - val_mse: 123.1921\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 136.4026 - mse: 136.4026 - val_loss: 124.9122 - val_mse: 124.9122\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 137.6043 - mse: 137.6043 - val_loss: 124.0340 - val_mse: 124.0340\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 134.0845 - mse: 134.0845 - val_loss: 122.9388 - val_mse: 122.9389\n",
      "[CV] ...................................... nl=0, nn=12, total=  56.9s\n",
      "[CV] nl=0, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 647us/sample - loss: 464.6928 - mse: 464.6928 - val_loss: 626.2046 - val_mse: 626.2047\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 419.1049 - mse: 419.1050 - val_loss: 568.0329 - val_mse: 568.0330\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 360.3160 - mse: 360.3160 - val_loss: 508.2976 - val_mse: 508.2977\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 293.8576 - mse: 293.8576 - val_loss: 394.2196 - val_mse: 394.2195\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 257.8332 - mse: 257.8332 - val_loss: 327.6869 - val_mse: 327.6869\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 228.1714 - mse: 228.1714 - val_loss: 298.9814 - val_mse: 298.9814\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 211.7054 - mse: 211.7054 - val_loss: 276.6987 - val_mse: 276.6988\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 207.3819 - mse: 207.3820 - val_loss: 264.9591 - val_mse: 264.9592\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 203.2729 - mse: 203.2728 - val_loss: 258.1151 - val_mse: 258.1152\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 192.4698 - mse: 192.4698 - val_loss: 251.4110 - val_mse: 251.4111\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 192.1647 - mse: 192.1647 - val_loss: 249.5105 - val_mse: 249.5105\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 186.6108 - mse: 186.6109 - val_loss: 243.8355 - val_mse: 243.8355\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 195.4313 - mse: 195.4314 - val_loss: 242.4197 - val_mse: 242.4196\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 189.3001 - mse: 189.3001 - val_loss: 239.6607 - val_mse: 239.6607\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 187.6830 - mse: 187.6830 - val_loss: 240.1827 - val_mse: 240.1828\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 183.8948 - mse: 183.8949 - val_loss: 233.9729 - val_mse: 233.9729\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 181.7089 - mse: 181.7088 - val_loss: 231.9852 - val_mse: 231.9852\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 181.4356 - mse: 181.4356 - val_loss: 228.2665 - val_mse: 228.2665\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 175.4918 - mse: 175.4918 - val_loss: 230.8162 - val_mse: 230.8162\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 179.3529 - mse: 179.3530 - val_loss: 226.5615 - val_mse: 226.5615\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 175.9882 - mse: 175.9882 - val_loss: 225.0488 - val_mse: 225.0488\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 170.0816 - mse: 170.0816 - val_loss: 222.1292 - val_mse: 222.1292\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 174.3470 - mse: 174.3470 - val_loss: 221.3155 - val_mse: 221.3154\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 176.2926 - mse: 176.2926 - val_loss: 221.7431 - val_mse: 221.7431\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 173.9708 - mse: 173.9707 - val_loss: 223.2966 - val_mse: 223.2966\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 175.2190 - mse: 175.2190 - val_loss: 218.4648 - val_mse: 218.4648\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 173.2765 - mse: 173.2766 - val_loss: 218.3754 - val_mse: 218.3754\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 169.2863 - mse: 169.2863 - val_loss: 214.3886 - val_mse: 214.3885\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 171.8063 - mse: 171.8063 - val_loss: 217.4881 - val_mse: 217.4881\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 172.6737 - mse: 172.6737 - val_loss: 211.9775 - val_mse: 211.9774\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 177.9184 - mse: 177.9184 - val_loss: 212.1906 - val_mse: 212.1906\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 165.8979 - mse: 165.8978 - val_loss: 214.1003 - val_mse: 214.1003\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 172.3877 - mse: 172.3877 - val_loss: 209.3387 - val_mse: 209.3387\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 172.1931 - mse: 172.1931 - val_loss: 208.0611 - val_mse: 208.0611\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 164.3978 - mse: 164.3977 - val_loss: 208.2387 - val_mse: 208.2387\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 167.9201 - mse: 167.9201 - val_loss: 205.0738 - val_mse: 205.0738\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 168.5872 - mse: 168.5872 - val_loss: 206.8714 - val_mse: 206.8713\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 166.3197 - mse: 166.3198 - val_loss: 207.3230 - val_mse: 207.3230\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 169.5187 - mse: 169.5187 - val_loss: 200.5704 - val_mse: 200.5704\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 167.7028 - mse: 167.7028 - val_loss: 204.1471 - val_mse: 204.1470\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 164.8527 - mse: 164.8527 - val_loss: 204.4532 - val_mse: 204.4532\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 165.7627 - mse: 165.7627 - val_loss: 200.8026 - val_mse: 200.8027\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 163.1117 - mse: 163.1117 - val_loss: 201.2752 - val_mse: 201.2752\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 161.8252 - mse: 161.8252 - val_loss: 200.0997 - val_mse: 200.0997\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 169.7894 - mse: 169.7894 - val_loss: 200.5995 - val_mse: 200.5995\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 165.9942 - mse: 165.9942 - val_loss: 197.9192 - val_mse: 197.9192\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 167.4928 - mse: 167.4929 - val_loss: 198.5234 - val_mse: 198.5234\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 168.9173 - mse: 168.9173 - val_loss: 206.9026 - val_mse: 206.9026\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 164.8638 - mse: 164.8638 - val_loss: 197.1461 - val_mse: 197.1461\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 163.8474 - mse: 163.8474 - val_loss: 192.9730 - val_mse: 192.9730\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 162.0683 - mse: 162.0683 - val_loss: 193.0469 - val_mse: 193.0469\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 171.0766 - mse: 171.0766 - val_loss: 195.0977 - val_mse: 195.0977\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 157.5629 - mse: 157.563 - 1s 199us/sample - loss: 160.8427 - mse: 160.8427 - val_loss: 192.9367 - val_mse: 192.9367\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 162.2293 - mse: 162.2293 - val_loss: 195.5934 - val_mse: 195.5934\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 164.6833 - mse: 164.6833 - val_loss: 193.3533 - val_mse: 193.3533\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 162.8796 - mse: 162.8797 - val_loss: 195.9734 - val_mse: 195.9734\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 158.1838 - mse: 158.1838 - val_loss: 191.0117 - val_mse: 191.0117\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 158.3630 - mse: 158.3629 - val_loss: 192.6982 - val_mse: 192.6983\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 157.8552 - mse: 157.8552 - val_loss: 189.7114 - val_mse: 189.7114\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 158.4382 - mse: 158.4382 - val_loss: 192.8662 - val_mse: 192.8662\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 158.0712 - mse: 158.0712 - val_loss: 189.3071 - val_mse: 189.3071\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 158.3361 - mse: 158.3361 - val_loss: 188.4390 - val_mse: 188.4391\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 161.8230 - mse: 161.8230 - val_loss: 188.0240 - val_mse: 188.0240\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 161.6181 - mse: 161.6181 - val_loss: 190.5285 - val_mse: 190.5285\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 154.2975 - mse: 154.2975 - val_loss: 187.4371 - val_mse: 187.4371\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 158.3606 - mse: 158.3606 - val_loss: 187.7702 - val_mse: 187.7702\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 155.5583 - mse: 155.5583 - val_loss: 187.2104 - val_mse: 187.2104\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 156.7184 - mse: 156.7183 - val_loss: 186.8834 - val_mse: 186.8835\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 160.7631 - mse: 160.7631 - val_loss: 186.4643 - val_mse: 186.4644\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 214us/sample - loss: 154.7252 - mse: 154.7252 - val_loss: 185.6431 - val_mse: 185.6431\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 155.5224 - mse: 155.5224 - val_loss: 191.9578 - val_mse: 191.9578\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 161.3759 - mse: 161.3759 - val_loss: 186.5186 - val_mse: 186.5186\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 159.1685 - mse: 159.1685 - val_loss: 185.7376 - val_mse: 185.7376\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 158.4051 - mse: 158.4051 - val_loss: 184.5935 - val_mse: 184.5935\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 157.3993 - mse: 157.3994 - val_loss: 185.6591 - val_mse: 185.6591\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 159.7015 - mse: 159.7015 - val_loss: 186.6678 - val_mse: 186.6678\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 152.9402 - mse: 152.9402 - val_loss: 185.8769 - val_mse: 185.8770\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 152.5299 - mse: 152.5299 - val_loss: 186.6834 - val_mse: 186.6834\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 157.7359 - mse: 157.7359 - val_loss: 186.6682 - val_mse: 186.6682\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 158.1040 - mse: 158.1041 - val_loss: 185.3964 - val_mse: 185.3965\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 153.4861 - mse: 153.4861 - val_loss: 187.1880 - val_mse: 187.1880\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 154.1266 - mse: 154.1266 - val_loss: 185.3796 - val_mse: 185.3795\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 157.8460 - mse: 157.8460 - val_loss: 186.6538 - val_mse: 186.6537\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 151.9082 - mse: 151.9082 - val_loss: 183.8561 - val_mse: 183.8561\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 156.9679 - mse: 156.9679 - val_loss: 182.8694 - val_mse: 182.8694\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 160.0159 - mse: 160.015 - 1s 197us/sample - loss: 158.0304 - mse: 158.0304 - val_loss: 183.2282 - val_mse: 183.2282\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 153.9672 - mse: 153.9672 - val_loss: 186.6342 - val_mse: 186.6342\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 155.4020 - mse: 155.4020 - val_loss: 185.4183 - val_mse: 185.4183\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 152.1839 - mse: 152.1839 - val_loss: 184.5453 - val_mse: 184.5453\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 151.4545 - mse: 151.4545 - val_loss: 185.0881 - val_mse: 185.0882\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 151.1568 - mse: 151.1568 - val_loss: 189.4070 - val_mse: 189.4070\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 154.0059 - mse: 154.0059 - val_loss: 183.8488 - val_mse: 183.8488\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 155.1252 - mse: 155.1252 - val_loss: 186.3322 - val_mse: 186.3322\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 157.4893 - mse: 157.4893 - val_loss: 185.0273 - val_mse: 185.0273\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 154.9714 - mse: 154.9714 - val_loss: 183.8881 - val_mse: 183.8881\n",
      "[CV] ...................................... nl=0, nn=12, total= 1.0min\n",
      "[CV] nl=0, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 661us/sample - loss: 505.6944 - mse: 505.6944 - val_loss: 656.7923 - val_mse: 656.7924\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 464.5987 - mse: 464.5987 - val_loss: 594.5522 - val_mse: 594.5522\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 419.2429 - mse: 419.2428 - val_loss: 519.5256 - val_mse: 519.5256\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 370.1657 - mse: 370.1657 - val_loss: 462.1828 - val_mse: 462.1829\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 328.2856 - mse: 328.2856 - val_loss: 412.6250 - val_mse: 412.6249\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 295.8825 - mse: 295.8824 - val_loss: 367.0290 - val_mse: 367.0290\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 251.9110 - mse: 251.911 - 1s 200us/sample - loss: 275.5301 - mse: 275.5301 - val_loss: 344.8125 - val_mse: 344.8125\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 260.1180 - mse: 260.1180 - val_loss: 324.2330 - val_mse: 324.2329\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 250.4491 - mse: 250.4491 - val_loss: 310.5135 - val_mse: 310.5134\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 237.9443 - mse: 237.9444 - val_loss: 295.6068 - val_mse: 295.6069\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 237.2950 - mse: 237.2950 - val_loss: 290.7276 - val_mse: 290.7276\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 228.9791 - mse: 228.9790 - val_loss: 277.6038 - val_mse: 277.6039\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 222.6320 - mse: 222.6320 - val_loss: 265.9725 - val_mse: 265.9726\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 216.4963 - mse: 216.4963 - val_loss: 259.2034 - val_mse: 259.2034\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 212.5524 - mse: 212.5524 - val_loss: 249.5583 - val_mse: 249.5583\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 204.6886 - mse: 204.6886 - val_loss: 254.4067 - val_mse: 254.4067\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 205.0253 - mse: 205.0252 - val_loss: 245.1268 - val_mse: 245.1268\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 206.5273 - mse: 206.5273 - val_loss: 234.9122 - val_mse: 234.9122\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 203.3665 - mse: 203.3665 - val_loss: 238.3349 - val_mse: 238.3349\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 201.6957 - mse: 201.6957 - val_loss: 232.1855 - val_mse: 232.1855\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 202.5542 - mse: 202.5542 - val_loss: 231.5323 - val_mse: 231.5323\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 206.4996 - mse: 206.4996 - val_loss: 226.8663 - val_mse: 226.8663\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 198.0218 - mse: 198.0219 - val_loss: 222.4999 - val_mse: 222.5000\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 205.5588 - mse: 205.5588 - val_loss: 225.6100 - val_mse: 225.6100\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 194.3822 - mse: 194.3823 - val_loss: 221.6044 - val_mse: 221.6044\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 197.7755 - mse: 197.7755 - val_loss: 215.2349 - val_mse: 215.2350\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 190.1590 - mse: 190.1590 - val_loss: 213.9702 - val_mse: 213.9702\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 195.1915 - mse: 195.1915 - val_loss: 221.0145 - val_mse: 221.0144\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 193.1895 - mse: 193.1894 - val_loss: 212.8411 - val_mse: 212.8411\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 191.6874 - mse: 191.6874 - val_loss: 210.4581 - val_mse: 210.4581\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 191.1220 - mse: 191.1221 - val_loss: 210.9635 - val_mse: 210.9635\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 196.5664 - mse: 196.5664 - val_loss: 208.3825 - val_mse: 208.3824\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 188.2329 - mse: 188.2328 - val_loss: 200.9330 - val_mse: 200.9330\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 193.3164 - mse: 193.3164 - val_loss: 202.7124 - val_mse: 202.7124\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 194.2113 - mse: 194.2113 - val_loss: 202.1279 - val_mse: 202.1279\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 198.4855 - mse: 198.4854 - val_loss: 198.8545 - val_mse: 198.8545\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 186.5309 - mse: 186.5310 - val_loss: 199.7895 - val_mse: 199.7894\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 190.3618 - mse: 190.3618 - val_loss: 198.5601 - val_mse: 198.5601\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 187.6553 - mse: 187.6553 - val_loss: 195.5732 - val_mse: 195.5732\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 184.4960 - mse: 184.4960 - val_loss: 199.7041 - val_mse: 199.7041\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 185.5715 - mse: 185.5715 - val_loss: 197.6122 - val_mse: 197.6122\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.1139 - mse: 185.1140 - val_loss: 191.8713 - val_mse: 191.8714\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 180.5803 - mse: 180.5803 - val_loss: 199.3527 - val_mse: 199.3527\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 183.4910 - mse: 183.4910 - val_loss: 190.3235 - val_mse: 190.3236\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 186.4554 - mse: 186.4555 - val_loss: 195.0720 - val_mse: 195.0720\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 184.4026 - mse: 184.4026 - val_loss: 195.5466 - val_mse: 195.5466\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 182.8337 - mse: 182.8337 - val_loss: 192.7172 - val_mse: 192.7172\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 180.2860 - mse: 180.2860 - val_loss: 193.7515 - val_mse: 193.7515\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 182.3190 - mse: 182.3191 - val_loss: 194.8433 - val_mse: 194.8433\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 177.6086 - mse: 177.6087 - val_loss: 190.4445 - val_mse: 190.4445\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 184.8186 - mse: 184.8186 - val_loss: 193.1319 - val_mse: 193.1319\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 181.6625 - mse: 181.6625 - val_loss: 192.5969 - val_mse: 192.5970\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 182.2658 - mse: 182.2658 - val_loss: 188.2121 - val_mse: 188.2121\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 182.2527 - mse: 182.2527 - val_loss: 200.1962 - val_mse: 200.1962\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 176.8992 - mse: 176.8993 - val_loss: 185.8332 - val_mse: 185.8332\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 183.0394 - mse: 183.0394 - val_loss: 186.0816 - val_mse: 186.0815\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 183.2798 - mse: 183.2798 - val_loss: 188.2695 - val_mse: 188.2694\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 179.1588 - mse: 179.1588 - val_loss: 186.3175 - val_mse: 186.3175\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 177.1909 - mse: 177.1909 - val_loss: 188.8119 - val_mse: 188.8119\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 179.9775 - mse: 179.9775 - val_loss: 182.7781 - val_mse: 182.7781\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 182.5481 - mse: 182.548 - 1s 251us/sample - loss: 179.2405 - mse: 179.2405 - val_loss: 187.6993 - val_mse: 187.6993\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 174.2110 - mse: 174.2110 - val_loss: 191.7172 - val_mse: 191.7172\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 182.4532 - mse: 182.4532 - val_loss: 186.7044 - val_mse: 186.7044\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 172.4501 - mse: 172.4501 - val_loss: 185.6435 - val_mse: 185.6434\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 170.2961 - mse: 170.2961 - val_loss: 185.6413 - val_mse: 185.6413\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 178.3142 - mse: 178.3142 - val_loss: 186.5962 - val_mse: 186.5962\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 178.9902 - mse: 178.9902 - val_loss: 182.6129 - val_mse: 182.6128\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 176.2051 - mse: 176.2051 - val_loss: 184.3264 - val_mse: 184.3264\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 175.4675 - mse: 175.4674 - val_loss: 181.9513 - val_mse: 181.9513\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 174.9746 - mse: 174.9745 - val_loss: 181.7601 - val_mse: 181.7602\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 170.1312 - mse: 170.1313 - val_loss: 184.1086 - val_mse: 184.1085\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 171.5097 - mse: 171.5097 - val_loss: 182.0271 - val_mse: 182.0272\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 169.9421 - mse: 169.9422 - val_loss: 187.8864 - val_mse: 187.8863\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 166.5085 - mse: 166.5085 - val_loss: 187.4762 - val_mse: 187.4762\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 167.6323 - mse: 167.6323 - val_loss: 183.9859 - val_mse: 183.9860\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 172.8904 - mse: 172.8903 - val_loss: 181.4717 - val_mse: 181.4716\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 168.8733 - mse: 168.8732 - val_loss: 180.9011 - val_mse: 180.9011\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 170.4955 - mse: 170.4955 - val_loss: 186.8146 - val_mse: 186.8146\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 177.6563 - mse: 177.6563 - val_loss: 187.2890 - val_mse: 187.2890\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 168.7006 - mse: 168.7005 - val_loss: 181.6519 - val_mse: 181.6519\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 172.6814 - mse: 172.6814 - val_loss: 180.1077 - val_mse: 180.1077\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 167.1771 - mse: 167.1771 - val_loss: 184.0462 - val_mse: 184.0462\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 211us/sample - loss: 161.6160 - mse: 161.6160 - val_loss: 180.5814 - val_mse: 180.5814\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 165.6914 - mse: 165.6914 - val_loss: 181.1743 - val_mse: 181.1743\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 169.1883 - mse: 169.1882 - val_loss: 181.5154 - val_mse: 181.5154\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 169.7474 - mse: 169.7473 - val_loss: 183.5089 - val_mse: 183.5089\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 165.8690 - mse: 165.8690 - val_loss: 181.3674 - val_mse: 181.3674\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 164.2773 - mse: 164.2772 - val_loss: 182.4650 - val_mse: 182.4650\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 167.4920 - mse: 167.4920 - val_loss: 183.6628 - val_mse: 183.6627\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 165.5953 - mse: 165.5954 - val_loss: 181.6913 - val_mse: 181.6914\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 168.2737 - mse: 168.2737 - val_loss: 178.9734 - val_mse: 178.9734\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 163.0013 - mse: 163.0013 - val_loss: 178.7090 - val_mse: 178.7090\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 170.4169 - mse: 170.4170 - val_loss: 178.8014 - val_mse: 178.8013\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 164.3353 - mse: 164.3353 - val_loss: 181.7690 - val_mse: 181.7690\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 167.6548 - mse: 167.6548 - val_loss: 178.7687 - val_mse: 178.7686\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 168.9452 - mse: 168.9452 - val_loss: 181.8552 - val_mse: 181.8552\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 163.5179 - mse: 163.5179 - val_loss: 180.1531 - val_mse: 180.1531\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 171.4079 - mse: 171.4079 - val_loss: 188.7352 - val_mse: 188.7352\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 162.4465 - mse: 162.4465 - val_loss: 182.9507 - val_mse: 182.9507\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 164.6518 - mse: 164.6518 - val_loss: 178.4563 - val_mse: 178.4563\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 164.2593 - mse: 164.2593 - val_loss: 180.2801 - val_mse: 180.2801\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 166.7575 - mse: 166.7575 - val_loss: 178.9317 - val_mse: 178.9317\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 167.5371 - mse: 167.5371 - val_loss: 177.3667 - val_mse: 177.3667\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 169.6585 - mse: 169.6585 - val_loss: 178.2660 - val_mse: 178.2660\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 168.0900 - mse: 168.0900 - val_loss: 181.2294 - val_mse: 181.2294\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 162.9670 - mse: 162.9670 - val_loss: 179.3340 - val_mse: 179.3339\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 167.1354 - mse: 167.1354 - val_loss: 185.2368 - val_mse: 185.2368\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 162.8113 - mse: 162.8113 - val_loss: 181.9908 - val_mse: 181.9908\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 158.4538 - mse: 158.4537 - val_loss: 180.1705 - val_mse: 180.1705\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 164.0090 - mse: 164.0090 - val_loss: 180.2759 - val_mse: 180.2758\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 161.1347 - mse: 161.1347 - val_loss: 178.7704 - val_mse: 178.7704\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 164.6330 - mse: 164.6330 - val_loss: 181.7051 - val_mse: 181.7050\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 159.9277 - mse: 159.9277 - val_loss: 180.4919 - val_mse: 180.4919\n",
      "[CV] ...................................... nl=0, nn=24, total= 1.2min\n",
      "[CV] nl=0, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 581us/sample - loss: 436.0326 - mse: 436.0326 - val_loss: 652.1951 - val_mse: 652.1949\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 385.8764 - mse: 385.8764 - val_loss: 569.6983 - val_mse: 569.6982\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 334.8613 - mse: 334.8614 - val_loss: 484.2278 - val_mse: 484.2278\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 290.6508 - mse: 290.6508 - val_loss: 420.8719 - val_mse: 420.8719\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 256.2676 - mse: 256.2675 - val_loss: 378.4782 - val_mse: 378.4783\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 229.9449 - mse: 229.9449 - val_loss: 350.0886 - val_mse: 350.0887\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 218.3405 - mse: 218.3404 - val_loss: 331.8328 - val_mse: 331.8328\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 213.5520 - mse: 213.5520 - val_loss: 323.3330 - val_mse: 323.3330\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 209.5254 - mse: 209.5254 - val_loss: 316.7644 - val_mse: 316.7645\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 203.2609 - mse: 203.2610 - val_loss: 311.3002 - val_mse: 311.3001\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 207.1180 - mse: 207.1179 - val_loss: 307.0264 - val_mse: 307.0265\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 210.7508 - mse: 210.7508 - val_loss: 306.5894 - val_mse: 306.5894\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 203.9835 - mse: 203.9836 - val_loss: 303.5245 - val_mse: 303.5245\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 204.4069 - mse: 204.4069 - val_loss: 302.2056 - val_mse: 302.2055\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 203.4334 - mse: 203.4334 - val_loss: 301.2984 - val_mse: 301.2984\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 208.4264 - mse: 208.4263 - val_loss: 300.8684 - val_mse: 300.8683\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 207.5524 - mse: 207.5523 - val_loss: 301.0736 - val_mse: 301.0735\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 205.7301 - mse: 205.7301 - val_loss: 301.4242 - val_mse: 301.4242\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 190us/sample - loss: 199.9342 - mse: 199.9342 - val_loss: 298.8361 - val_mse: 298.8360\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 201.4475 - mse: 201.4475 - val_loss: 299.8331 - val_mse: 299.8330\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 197.5090 - mse: 197.5090 - val_loss: 297.6624 - val_mse: 297.6624\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 203.6070 - mse: 203.6070 - val_loss: 298.0456 - val_mse: 298.0455\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 201.9907 - mse: 201.9908 - val_loss: 297.1808 - val_mse: 297.1808\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 200.3329 - mse: 200.3328 - val_loss: 297.8056 - val_mse: 297.8055\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 202.1242 - mse: 202.1242 - val_loss: 297.1966 - val_mse: 297.1966\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 210.4089 - mse: 210.409 - 1s 217us/sample - loss: 202.0436 - mse: 202.0436 - val_loss: 298.1435 - val_mse: 298.1435\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 190us/sample - loss: 198.2591 - mse: 198.2591 - val_loss: 299.2147 - val_mse: 299.2147\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 199.1203 - mse: 199.1203 - val_loss: 297.2049 - val_mse: 297.2049\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 186us/sample - loss: 205.8472 - mse: 205.8472 - val_loss: 297.8218 - val_mse: 297.8218\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 198.1788 - mse: 198.1788 - val_loss: 297.0134 - val_mse: 297.0135\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 200.0792 - mse: 200.0792 - val_loss: 296.1778 - val_mse: 296.1779\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 194.7709 - mse: 194.7709 - val_loss: 296.0659 - val_mse: 296.0659\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 195.1113 - mse: 195.1113 - val_loss: 295.2969 - val_mse: 295.2969\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 198.3920 - mse: 198.3920 - val_loss: 295.9148 - val_mse: 295.9148\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 198.9151 - mse: 198.9152 - val_loss: 294.6610 - val_mse: 294.6610\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 195.4618 - mse: 195.4618 - val_loss: 296.4162 - val_mse: 296.4162\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 192.0841 - mse: 192.0842 - val_loss: 295.1279 - val_mse: 295.1279\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 195.7691 - mse: 195.7691 - val_loss: 295.3678 - val_mse: 295.3676\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 195.8136 - mse: 195.8136 - val_loss: 294.8130 - val_mse: 294.8130\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 196.2614 - mse: 196.2614 - val_loss: 296.5020 - val_mse: 296.5020\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 195.8716 - mse: 195.8716 - val_loss: 293.9865 - val_mse: 293.9865\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 199.1368 - mse: 199.1368 - val_loss: 297.3312 - val_mse: 297.3312\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 197.9572 - mse: 197.9572 - val_loss: 295.2091 - val_mse: 295.2091\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 194.7099 - mse: 194.7099 - val_loss: 295.0736 - val_mse: 295.0736\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 190.7447 - mse: 190.7448 - val_loss: 295.1252 - val_mse: 295.1251\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 195.5922 - mse: 195.5922 - val_loss: 294.6829 - val_mse: 294.6829\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 196.5595 - mse: 196.5595 - val_loss: 295.4256 - val_mse: 295.4256\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 194.4214 - mse: 194.4214 - val_loss: 296.3585 - val_mse: 296.3585\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 197.1857 - mse: 197.1857 - val_loss: 294.8987 - val_mse: 294.8985\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 197.5252 - mse: 197.5251 - val_loss: 295.8643 - val_mse: 295.8643\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 195.8060 - mse: 195.8060 - val_loss: 295.5729 - val_mse: 295.5729\n",
      "[CV] ...................................... nl=0, nn=24, total=  32.3s\n",
      "[CV] nl=0, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 543us/sample - loss: 540.8130 - mse: 540.8129 - val_loss: 664.4227 - val_mse: 664.4229\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 485.5377 - mse: 485.5378 - val_loss: 598.7319 - val_mse: 598.7319\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 190us/sample - loss: 428.5903 - mse: 428.5902 - val_loss: 508.6600 - val_mse: 508.6600\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 370.0563 - mse: 370.0562 - val_loss: 435.9105 - val_mse: 435.9104\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 314.8350 - mse: 314.8350 - val_loss: 372.8677 - val_mse: 372.8676\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 267.2817 - mse: 267.2817 - val_loss: 312.8863 - val_mse: 312.8863\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 243.1008 - mse: 243.1009 - val_loss: 276.9853 - val_mse: 276.9854\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 221.4571 - mse: 221.4571 - val_loss: 253.5722 - val_mse: 253.5721\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 214.3278 - mse: 214.3278 - val_loss: 238.9299 - val_mse: 238.9299\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 205.6578 - mse: 205.6578 - val_loss: 231.1426 - val_mse: 231.1426\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 213.1931 - mse: 213.1931 - val_loss: 225.6708 - val_mse: 225.6708\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 197.7916 - mse: 197.7916 - val_loss: 223.6799 - val_mse: 223.6799\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 192.2630 - mse: 192.2630 - val_loss: 214.7495 - val_mse: 214.7495\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 189.0883 - mse: 189.0883 - val_loss: 208.7161 - val_mse: 208.7161\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 198.3928 - mse: 198.3929 - val_loss: 213.0659 - val_mse: 213.0659\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 200.6490 - mse: 200.6490 - val_loss: 210.9596 - val_mse: 210.9596\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 196.4919 - mse: 196.4919 - val_loss: 201.1012 - val_mse: 201.1012\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 196.3293 - mse: 196.3293 - val_loss: 203.5850 - val_mse: 203.5850\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 191.0134 - mse: 191.0133 - val_loss: 202.8464 - val_mse: 202.8464\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 192.8157 - mse: 192.8157 - val_loss: 204.0726 - val_mse: 204.0726\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 181.1314 - mse: 181.1313 - val_loss: 197.1605 - val_mse: 197.1605\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 184.8901 - mse: 184.8901 - val_loss: 191.9519 - val_mse: 191.9519\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 179.9268 - mse: 179.9269 - val_loss: 189.1547 - val_mse: 189.1547\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 171.1518 - mse: 171.1519 - val_loss: 185.1239 - val_mse: 185.1239\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 179.7237 - mse: 179.7237 - val_loss: 188.8174 - val_mse: 188.8174\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 175.1767 - mse: 175.1767 - val_loss: 182.1553 - val_mse: 182.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 174.9603 - mse: 174.9603 - val_loss: 182.3592 - val_mse: 182.3592\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 172.2394 - mse: 172.2394 - val_loss: 182.2649 - val_mse: 182.2648\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 170.2878 - mse: 170.2878 - val_loss: 181.5237 - val_mse: 181.5236\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 176.6430 - mse: 176.6430 - val_loss: 178.9018 - val_mse: 178.9017\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 175.4508 - mse: 175.4508 - val_loss: 177.4790 - val_mse: 177.4789\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 167.0719 - mse: 167.0719 - val_loss: 177.7922 - val_mse: 177.7922\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 171.9260 - mse: 171.9260 - val_loss: 177.0983 - val_mse: 177.0983\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 169.2474 - mse: 169.2474 - val_loss: 176.4597 - val_mse: 176.4597\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 170.0459 - mse: 170.0459 - val_loss: 173.2086 - val_mse: 173.2086\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 165.1588 - mse: 165.1588 - val_loss: 174.3969 - val_mse: 174.3969\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 172.3146 - mse: 172.3146 - val_loss: 171.1742 - val_mse: 171.1742\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 157.4403 - mse: 157.4403 - val_loss: 172.9025 - val_mse: 172.9025\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 164.3114 - mse: 164.3114 - val_loss: 174.7534 - val_mse: 174.7533\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 166.2538 - mse: 166.2538 - val_loss: 174.0143 - val_mse: 174.0143\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 167.6329 - mse: 167.6329 - val_loss: 170.9273 - val_mse: 170.9273\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 161.9979 - mse: 161.9979 - val_loss: 170.3856 - val_mse: 170.3856\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 169.1665 - mse: 169.1665 - val_loss: 171.8426 - val_mse: 171.8426\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 165.7440 - mse: 165.7441 - val_loss: 172.6318 - val_mse: 172.6319\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 158.5253 - mse: 158.5253 - val_loss: 172.4071 - val_mse: 172.4071\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 163.3834 - mse: 163.3835 - val_loss: 168.1233 - val_mse: 168.1233\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 162.1915 - mse: 162.1915 - val_loss: 170.5668 - val_mse: 170.5668\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 155.7150 - mse: 155.7150 - val_loss: 169.2179 - val_mse: 169.2179\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 156.6655 - mse: 156.6655 - val_loss: 167.2232 - val_mse: 167.2232\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 162.0470 - mse: 162.0470 - val_loss: 168.1039 - val_mse: 168.1039\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 158.4192 - mse: 158.4192 - val_loss: 167.5043 - val_mse: 167.5043\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 159.4729 - mse: 159.4729 - val_loss: 165.9003 - val_mse: 165.9003\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 162.8053 - mse: 162.8052 - val_loss: 166.0288 - val_mse: 166.0289\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 157.2361 - mse: 157.2361 - val_loss: 163.5363 - val_mse: 163.5363\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 157.8744 - mse: 157.8744 - val_loss: 163.2722 - val_mse: 163.2723\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 160.6134 - mse: 160.6134 - val_loss: 162.5825 - val_mse: 162.5825\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 187us/sample - loss: 163.2080 - mse: 163.2080 - val_loss: 164.1353 - val_mse: 164.1353\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 162.5392 - mse: 162.5392 - val_loss: 162.9419 - val_mse: 162.9419\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 158.5736 - mse: 158.5735 - val_loss: 162.5349 - val_mse: 162.5349\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 155.2638 - mse: 155.2637 - val_loss: 161.6084 - val_mse: 161.6084\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 157.2413 - mse: 157.2413 - val_loss: 165.3669 - val_mse: 165.3669\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 162.2498 - mse: 162.2499 - val_loss: 167.2561 - val_mse: 167.2561\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 151.4978 - mse: 151.4979 - val_loss: 164.3188 - val_mse: 164.3188\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 163.2070 - mse: 163.2070 - val_loss: 162.8367 - val_mse: 162.8367\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 156.4561 - mse: 156.4561 - val_loss: 159.0476 - val_mse: 159.0476\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 157.9733 - mse: 157.9733 - val_loss: 159.2976 - val_mse: 159.2977\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 162.9944 - mse: 162.9944 - val_loss: 158.0808 - val_mse: 158.0808\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 149.7977 - mse: 149.7977 - val_loss: 160.3569 - val_mse: 160.3569\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 163.1288 - mse: 163.1288 - val_loss: 159.0044 - val_mse: 159.0044\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 156.8039 - mse: 156.8039 - val_loss: 156.4126 - val_mse: 156.4126\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 153.8165 - mse: 153.816 - 1s 199us/sample - loss: 155.1627 - mse: 155.1627 - val_loss: 156.2137 - val_mse: 156.2137\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 157.5789 - mse: 157.5788 - val_loss: 156.1910 - val_mse: 156.1910\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 158.6622 - mse: 158.6622 - val_loss: 154.9648 - val_mse: 154.9648\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 148.2792 - mse: 148.2792 - val_loss: 159.7872 - val_mse: 159.7872\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 151.7547 - mse: 151.7547 - val_loss: 155.2461 - val_mse: 155.2461\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 154.9036 - mse: 154.9037 - val_loss: 154.0094 - val_mse: 154.0094\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 153.7936 - mse: 153.7935 - val_loss: 153.7772 - val_mse: 153.7772\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 153.2046 - mse: 153.2046 - val_loss: 153.1573 - val_mse: 153.1572\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 158.7230 - mse: 158.7230 - val_loss: 157.4398 - val_mse: 157.4398\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 149.1335 - mse: 149.1334 - val_loss: 155.4194 - val_mse: 155.4194\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 152.2927 - mse: 152.2927 - val_loss: 154.0770 - val_mse: 154.0770\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 198us/sample - loss: 158.7266 - mse: 158.7266 - val_loss: 151.6866 - val_mse: 151.6866\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 157.9948 - mse: 157.9948 - val_loss: 149.9481 - val_mse: 149.9482\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 149.6252 - mse: 149.6252 - val_loss: 153.1650 - val_mse: 153.1649\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 166.1751 - mse: 166.1751 - val_loss: 152.6602 - val_mse: 152.6602\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 147.1640 - mse: 147.1640 - val_loss: 153.5588 - val_mse: 153.5588\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 147.9430 - mse: 147.9430 - val_loss: 153.6205 - val_mse: 153.6205\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 142.9040 - mse: 142.9040 - val_loss: 150.8149 - val_mse: 150.8149\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 148.6950 - mse: 148.6950 - val_loss: 150.1270 - val_mse: 150.1270\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 158.7150 - mse: 158.7150 - val_loss: 150.1610 - val_mse: 150.1610\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 149.4164 - mse: 149.4164 - val_loss: 149.6425 - val_mse: 149.6426\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 152.6926 - mse: 152.6926 - val_loss: 148.7915 - val_mse: 148.7915\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 148.9515 - mse: 148.9515 - val_loss: 147.3640 - val_mse: 147.3640\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 148.1037 - mse: 148.1038 - val_loss: 148.7102 - val_mse: 148.7103\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 147.1939 - mse: 147.1939 - val_loss: 151.6703 - val_mse: 151.6703\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 151.4235 - mse: 151.4235 - val_loss: 150.3822 - val_mse: 150.3822\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 154.1886 - mse: 154.1886 - val_loss: 149.7637 - val_mse: 149.7637\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 149.2063 - mse: 149.2063 - val_loss: 148.6612 - val_mse: 148.6613\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 147.5007 - mse: 147.5007 - val_loss: 149.1622 - val_mse: 149.1622\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 142.2651 - mse: 142.2651 - val_loss: 147.4492 - val_mse: 147.4492\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 151.7051 - mse: 151.7051 - val_loss: 146.0826 - val_mse: 146.0826\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 146.8763 - mse: 146.8763 - val_loss: 148.1581 - val_mse: 148.1581\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 141.1150 - mse: 141.1150 - val_loss: 147.9447 - val_mse: 147.9447\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 146.8972 - mse: 146.8971 - val_loss: 148.4029 - val_mse: 148.4029\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 140.4672 - mse: 140.4671 - val_loss: 148.4923 - val_mse: 148.4923\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 152.4807 - mse: 152.4807 - val_loss: 150.1400 - val_mse: 150.1400\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 146.8859 - mse: 146.8858 - val_loss: 147.2860 - val_mse: 147.2860\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 152.0046 - mse: 152.0047 - val_loss: 147.0265 - val_mse: 147.0265\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.0365 - mse: 141.0365 - val_loss: 149.6749 - val_mse: 149.6749\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 150.7854 - mse: 150.7854 - val_loss: 150.1875 - val_mse: 150.1875\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 145.0003 - mse: 145.0003 - val_loss: 150.7587 - val_mse: 150.7587\n",
      "[CV] ...................................... nl=0, nn=24, total= 1.2min\n",
      "[CV] nl=0, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 549us/sample - loss: 471.3275 - mse: 471.3273 - val_loss: 531.3367 - val_mse: 531.3368\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 408.2955 - mse: 408.2956 - val_loss: 474.8355 - val_mse: 474.8355\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 340.2993 - mse: 340.2993 - val_loss: 360.7826 - val_mse: 360.7826\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 276.0754 - mse: 276.0754 - val_loss: 263.1743 - val_mse: 263.1743\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 226.7837 - mse: 226.7838 - val_loss: 205.2767 - val_mse: 205.2766\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 202.6585 - mse: 202.6584 - val_loss: 177.9788 - val_mse: 177.9788\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 182.6182 - mse: 182.6182 - val_loss: 161.1558 - val_mse: 161.1558\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 178.7478 - mse: 178.7477 - val_loss: 157.0974 - val_mse: 157.0974\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 171.5706 - mse: 171.5706 - val_loss: 152.2736 - val_mse: 152.2735\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 171.9456 - mse: 171.9455 - val_loss: 150.4226 - val_mse: 150.4226\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 168.9624 - mse: 168.9624 - val_loss: 147.2321 - val_mse: 147.2321\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 168.2433 - mse: 168.2433 - val_loss: 144.6725 - val_mse: 144.6725\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 172.6288 - mse: 172.6288 - val_loss: 143.0310 - val_mse: 143.0310\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 169.8154 - mse: 169.8154 - val_loss: 142.5664 - val_mse: 142.5663\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 173.0108 - mse: 173.0108 - val_loss: 139.3228 - val_mse: 139.3228\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 166.7712 - mse: 166.7712 - val_loss: 139.2216 - val_mse: 139.2216\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 161.3032 - mse: 161.3032 - val_loss: 137.8000 - val_mse: 137.8000\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 194us/sample - loss: 162.0415 - mse: 162.0415 - val_loss: 138.9768 - val_mse: 138.9768\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 162.3296 - mse: 162.3296 - val_loss: 139.8843 - val_mse: 139.8843\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 166.0378 - mse: 166.0378 - val_loss: 135.6498 - val_mse: 135.6498\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 159.1999 - mse: 159.1999 - val_loss: 132.6485 - val_mse: 132.6485\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 156.7255 - mse: 156.7255 - val_loss: 133.2675 - val_mse: 133.2675\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 162.7148 - mse: 162.7148 - val_loss: 130.4538 - val_mse: 130.4538\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 159.0789 - mse: 159.0789 - val_loss: 129.7457 - val_mse: 129.7457\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 160.0474 - mse: 160.0475 - val_loss: 130.1029 - val_mse: 130.1028\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 161.3769 - mse: 161.3769 - val_loss: 129.5253 - val_mse: 129.5253\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 160.1949 - mse: 160.1948 - val_loss: 129.8665 - val_mse: 129.8665\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 155.7191 - mse: 155.7191 - val_loss: 129.9882 - val_mse: 129.9882\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 191us/sample - loss: 148.9350 - mse: 148.9350 - val_loss: 127.8499 - val_mse: 127.8499\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 151.2240 - mse: 151.2240 - val_loss: 128.9925 - val_mse: 128.9925\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 157.7838 - mse: 157.7838 - val_loss: 127.6781 - val_mse: 127.6781\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 146.0258 - mse: 146.0259 - val_loss: 127.9890 - val_mse: 127.9890\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 158.1661 - mse: 158.1661 - val_loss: 126.5170 - val_mse: 126.5170\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 151.6929 - mse: 151.6929 - val_loss: 126.1531 - val_mse: 126.1531\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 151.0867 - mse: 151.0867 - val_loss: 128.6276 - val_mse: 128.6276\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 153.6811 - mse: 153.6810 - val_loss: 125.6007 - val_mse: 125.6007\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 151.2142 - mse: 151.2142 - val_loss: 125.6382 - val_mse: 125.6382\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 145.6861 - mse: 145.6861 - val_loss: 127.1435 - val_mse: 127.1435\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 150.8955 - mse: 150.8955 - val_loss: 125.4184 - val_mse: 125.4184\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 153.5857 - mse: 153.5857 - val_loss: 125.6324 - val_mse: 125.6324\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 151.8987 - mse: 151.8988 - val_loss: 124.6049 - val_mse: 124.6049\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 146.2224 - mse: 146.2224 - val_loss: 124.7449 - val_mse: 124.7448\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 155.8952 - mse: 155.8952 - val_loss: 124.6137 - val_mse: 124.6137\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 149.4811 - mse: 149.4811 - val_loss: 124.6307 - val_mse: 124.6307\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 150.2123 - mse: 150.2123 - val_loss: 124.6317 - val_mse: 124.6316\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 153.8709 - mse: 153.8709 - val_loss: 125.1757 - val_mse: 125.1756\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 185us/sample - loss: 153.5411 - mse: 153.5411 - val_loss: 125.3745 - val_mse: 125.3745\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 165.0434 - mse: 165.0434 - val_loss: 125.1638 - val_mse: 125.1639\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 146.6987 - mse: 146.6988 - val_loss: 124.3167 - val_mse: 124.3167\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 156.9596 - mse: 156.9595 - val_loss: 124.0072 - val_mse: 124.0072\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 148.7220 - mse: 148.7220 - val_loss: 123.6587 - val_mse: 123.6587\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 149.5944 - mse: 149.5944 - val_loss: 124.8395 - val_mse: 124.8395\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 150.3286 - mse: 150.3286 - val_loss: 122.9557 - val_mse: 122.9557\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 141.9830 - mse: 141.9831 - val_loss: 123.1483 - val_mse: 123.1483\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 155.0011 - mse: 155.0011 - val_loss: 124.4459 - val_mse: 124.4460\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 146.7806 - mse: 146.7807 - val_loss: 123.2390 - val_mse: 123.2390\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 151.3731 - mse: 151.3731 - val_loss: 122.7531 - val_mse: 122.7531\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.2952 - mse: 141.2952 - val_loss: 123.5962 - val_mse: 123.5962\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 148.6043 - mse: 148.6042 - val_loss: 123.5773 - val_mse: 123.5773\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 148.0089 - mse: 148.0089 - val_loss: 122.5236 - val_mse: 122.5236\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 151.0248 - mse: 151.0249 - val_loss: 123.7682 - val_mse: 123.7682\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 142.2842 - mse: 142.2842 - val_loss: 122.1733 - val_mse: 122.1734\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 150.5976 - mse: 150.5975 - val_loss: 122.2659 - val_mse: 122.2659\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 145.7302 - mse: 145.7303 - val_loss: 121.7026 - val_mse: 121.7026\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 149.9094 - mse: 149.9094 - val_loss: 123.1361 - val_mse: 123.1360\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 146.7973 - mse: 146.7973 - val_loss: 123.0526 - val_mse: 123.0526\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 145.1943 - mse: 145.1944 - val_loss: 122.7746 - val_mse: 122.7747\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 151.7952 - mse: 151.7952 - val_loss: 121.0275 - val_mse: 121.0275\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 311us/sample - loss: 149.9557 - mse: 149.9557 - val_loss: 121.8873 - val_mse: 121.8873\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 139.4414 - mse: 139.4415 - val_loss: 122.3006 - val_mse: 122.3006\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 145.0274 - mse: 145.0274 - val_loss: 121.4856 - val_mse: 121.4857\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 147.1385 - mse: 147.1385 - val_loss: 120.4169 - val_mse: 120.4169\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 143.7604 - mse: 143.7604 - val_loss: 120.9008 - val_mse: 120.9008\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 141.4287 - mse: 141.4287 - val_loss: 120.9561 - val_mse: 120.9561\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 141.1435 - mse: 141.1435 - val_loss: 121.3470 - val_mse: 121.3470\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 145.7802 - mse: 145.7802 - val_loss: 123.1926 - val_mse: 123.1926\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 137.7504 - mse: 137.7504 - val_loss: 121.6823 - val_mse: 121.6823\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 194us/sample - loss: 142.0408 - mse: 142.0408 - val_loss: 121.8560 - val_mse: 121.8560\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 141.8001 - mse: 141.8001 - val_loss: 121.1789 - val_mse: 121.1788\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 207us/sample - loss: 141.2217 - mse: 141.2217 - val_loss: 123.4247 - val_mse: 123.4247\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 148.6224 - mse: 148.6224 - val_loss: 121.8014 - val_mse: 121.8014\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 147.6032 - mse: 147.6032 - val_loss: 122.5363 - val_mse: 122.5362\n",
      "[CV] ...................................... nl=0, nn=24, total=  51.6s\n",
      "[CV] nl=0, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 552us/sample - loss: 459.9576 - mse: 459.9577 - val_loss: 628.2689 - val_mse: 628.2689\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 404.8781 - mse: 404.8781 - val_loss: 538.2082 - val_mse: 538.2081\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 345.8911 - mse: 345.8911 - val_loss: 440.0345 - val_mse: 440.0345\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 290.0170 - mse: 290.0169 - val_loss: 366.0739 - val_mse: 366.0739\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 243.3163 - mse: 243.3163 - val_loss: 300.8139 - val_mse: 300.8139\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 221.3893 - mse: 221.3893 - val_loss: 267.2067 - val_mse: 267.2067\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 189us/sample - loss: 192.4958 - mse: 192.4958 - val_loss: 243.0871 - val_mse: 243.0871\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 185.3213 - mse: 185.3213 - val_loss: 230.7277 - val_mse: 230.7277\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 173.6093 - mse: 173.6093 - val_loss: 222.4625 - val_mse: 222.4626\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 182.3818 - mse: 182.3818 - val_loss: 215.4291 - val_mse: 215.4291\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 168.2184 - mse: 168.2184 - val_loss: 210.7327 - val_mse: 210.7327\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 169.1528 - mse: 169.1528 - val_loss: 209.6270 - val_mse: 209.6270\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 163.1200 - mse: 163.1200 - val_loss: 210.8120 - val_mse: 210.8120\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 166.4135 - mse: 166.4135 - val_loss: 203.6331 - val_mse: 203.6331\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 167.3022 - mse: 167.3021 - val_loss: 205.7185 - val_mse: 205.7186\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 164.4963 - mse: 164.4963 - val_loss: 203.3394 - val_mse: 203.3394\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 168.3274 - mse: 168.3274 - val_loss: 199.0510 - val_mse: 199.0510\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 159.7353 - mse: 159.7352 - val_loss: 199.6770 - val_mse: 199.6770\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 162.5379 - mse: 162.5378 - val_loss: 197.4897 - val_mse: 197.4897\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 164.5598 - mse: 164.5598 - val_loss: 193.4596 - val_mse: 193.4596\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 166.8824 - mse: 166.8824 - val_loss: 194.5517 - val_mse: 194.5517\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 160.0843 - mse: 160.0843 - val_loss: 191.5390 - val_mse: 191.5389\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 156.0632 - mse: 156.0632 - val_loss: 191.1750 - val_mse: 191.1750\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 156.7285 - mse: 156.7286 - val_loss: 188.2387 - val_mse: 188.2387\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 156.7244 - mse: 156.7244 - val_loss: 190.3546 - val_mse: 190.3546\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 156.5814 - mse: 156.5813 - val_loss: 188.5391 - val_mse: 188.5391\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 158.3802 - mse: 158.3801 - val_loss: 186.3148 - val_mse: 186.3148\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 160.8527 - mse: 160.8526 - val_loss: 185.0619 - val_mse: 185.0619\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 160.3970 - mse: 160.3970 - val_loss: 183.7895 - val_mse: 183.7896\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 153.8012 - mse: 153.8013 - val_loss: 184.1078 - val_mse: 184.1078\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 153.4156 - mse: 153.4156 - val_loss: 182.5947 - val_mse: 182.5947\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 151.8734 - mse: 151.8735 - val_loss: 182.8486 - val_mse: 182.8486\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 158.6051 - mse: 158.6051 - val_loss: 182.2602 - val_mse: 182.2602\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 148.7966 - mse: 148.7966 - val_loss: 181.7096 - val_mse: 181.7096\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 153.0260 - mse: 153.0260 - val_loss: 180.9652 - val_mse: 180.9652\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 156.6768 - mse: 156.6767 - val_loss: 182.5890 - val_mse: 182.5890\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 146.3717 - mse: 146.3717 - val_loss: 179.2399 - val_mse: 179.2399\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 147.4231 - mse: 147.4231 - val_loss: 178.9218 - val_mse: 178.9218\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 153.4532 - mse: 153.4532 - val_loss: 180.3035 - val_mse: 180.3035\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 150.8763 - mse: 150.8764 - val_loss: 178.7558 - val_mse: 178.7558\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 150.3304 - mse: 150.3305 - val_loss: 177.5010 - val_mse: 177.5010\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 151.2291 - mse: 151.2291 - val_loss: 177.3363 - val_mse: 177.3363\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 147.2802 - mse: 147.2803 - val_loss: 176.4446 - val_mse: 176.4446\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 150.5336 - mse: 150.5336 - val_loss: 177.2269 - val_mse: 177.2269\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 142.9609 - mse: 142.9609 - val_loss: 175.6638 - val_mse: 175.6638\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 149.0606 - mse: 149.0606 - val_loss: 175.7349 - val_mse: 175.7349\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 147.0122 - mse: 147.0122 - val_loss: 177.3707 - val_mse: 177.3707\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 148.1506 - mse: 148.1505 - val_loss: 174.9033 - val_mse: 174.9033\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 145.3740 - mse: 145.3740 - val_loss: 176.7653 - val_mse: 176.7653\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 149.5680 - mse: 149.5680 - val_loss: 175.9414 - val_mse: 175.9414\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 147.4471 - mse: 147.4471 - val_loss: 174.7563 - val_mse: 174.7563\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 145.1882 - mse: 145.1882 - val_loss: 173.9253 - val_mse: 173.9253\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 145.2071 - mse: 145.2071 - val_loss: 174.0487 - val_mse: 174.0488\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 144.6333 - mse: 144.6333 - val_loss: 175.1431 - val_mse: 175.1431\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 147.5861 - mse: 147.5862 - val_loss: 173.6871 - val_mse: 173.6871\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 148.5840 - mse: 148.5841 - val_loss: 173.5677 - val_mse: 173.5677\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 194us/sample - loss: 140.2982 - mse: 140.2982 - val_loss: 173.9531 - val_mse: 173.9531\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 145.2442 - mse: 145.2442 - val_loss: 173.6221 - val_mse: 173.6221\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 152.0641 - mse: 152.0641 - val_loss: 172.9004 - val_mse: 172.9003\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 141.0354 - mse: 141.0354 - val_loss: 173.6297 - val_mse: 173.6297\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 146.0601 - mse: 146.0601 - val_loss: 172.9347 - val_mse: 172.9347\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 140.6294 - mse: 140.6295 - val_loss: 173.6102 - val_mse: 173.6102\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 143.1524 - mse: 143.1524 - val_loss: 173.4137 - val_mse: 173.4137\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 148.3243 - mse: 148.3243 - val_loss: 172.5713 - val_mse: 172.5713\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 143.8669 - mse: 143.8668 - val_loss: 172.4815 - val_mse: 172.4816\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 145.7468 - mse: 145.7468 - val_loss: 171.0679 - val_mse: 171.0679\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 140.2891 - mse: 140.2891 - val_loss: 169.8022 - val_mse: 169.8022\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 153.2321 - mse: 153.2321 - val_loss: 170.3349 - val_mse: 170.3350\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 139.1322 - mse: 139.1322 - val_loss: 169.7899 - val_mse: 169.7899\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 141.7614 - mse: 141.7614 - val_loss: 169.0656 - val_mse: 169.0656\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 140.9314 - mse: 140.9314 - val_loss: 168.6912 - val_mse: 168.6912\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 141.3879 - mse: 141.3879 - val_loss: 168.3990 - val_mse: 168.3990\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.8171 - mse: 141.8171 - val_loss: 169.9082 - val_mse: 169.9082\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 137.4864 - mse: 137.4864 - val_loss: 171.2351 - val_mse: 171.2351\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 146.7116 - mse: 146.7116 - val_loss: 168.6251 - val_mse: 168.6251\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 144.5493 - mse: 144.5492 - val_loss: 168.1964 - val_mse: 168.1964\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 136.8432 - mse: 136.8433 - val_loss: 169.4226 - val_mse: 169.4226\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 140.5671 - mse: 140.5671 - val_loss: 169.0225 - val_mse: 169.0224\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 141.9737 - mse: 141.9738 - val_loss: 168.6446 - val_mse: 168.6446\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 140.7661 - mse: 140.7661 - val_loss: 167.9979 - val_mse: 167.9979\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 143.5627 - mse: 143.5627 - val_loss: 168.0469 - val_mse: 168.0469\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 138.0106 - mse: 138.0107 - val_loss: 167.6708 - val_mse: 167.6709\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 141.6087 - mse: 141.608 - 1s 199us/sample - loss: 141.2623 - mse: 141.2623 - val_loss: 168.5130 - val_mse: 168.5130\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 143.0631 - mse: 143.0632 - val_loss: 172.7407 - val_mse: 172.7407\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 135.8952 - mse: 135.8952 - val_loss: 169.7958 - val_mse: 169.7958\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 141.6648 - mse: 141.6649 - val_loss: 167.0209 - val_mse: 167.0209\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 135.3197 - mse: 135.3197 - val_loss: 167.0322 - val_mse: 167.0322\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 137.8245 - mse: 137.8244 - val_loss: 166.0661 - val_mse: 166.0661\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 138.9969 - mse: 138.9970 - val_loss: 167.0038 - val_mse: 167.0037\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 146.5543 - mse: 146.5543 - val_loss: 167.7977 - val_mse: 167.7978\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 140.8253 - mse: 140.8253 - val_loss: 167.8427 - val_mse: 167.8428\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 135.3628 - mse: 135.3628 - val_loss: 169.9799 - val_mse: 169.9798\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 134.5690 - mse: 134.5690 - val_loss: 166.7915 - val_mse: 166.7915\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 135.6426 - mse: 135.6425 - val_loss: 165.9927 - val_mse: 165.9927\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 135.9218 - mse: 135.9218 - val_loss: 166.9099 - val_mse: 166.9099\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 140.9113 - mse: 140.9113 - val_loss: 165.5017 - val_mse: 165.5017\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 143.3140 - mse: 143.3140 - val_loss: 164.8216 - val_mse: 164.8216\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 133.4799 - mse: 133.4799 - val_loss: 165.5057 - val_mse: 165.5056\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 134.8266 - mse: 134.8266 - val_loss: 165.0023 - val_mse: 165.0023\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 136.0115 - mse: 136.0115 - val_loss: 165.9853 - val_mse: 165.9853\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 134.6618 - mse: 134.6618 - val_loss: 166.2614 - val_mse: 166.2614\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 135.8439 - mse: 135.8439 - val_loss: 164.5337 - val_mse: 164.5337\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 136.7157 - mse: 136.7157 - val_loss: 166.0097 - val_mse: 166.0097\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 135.5094 - mse: 135.5094 - val_loss: 168.9459 - val_mse: 168.9459\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 135.4247 - mse: 135.4247 - val_loss: 166.9273 - val_mse: 166.9272\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 128.0739 - mse: 128.0739 - val_loss: 165.2643 - val_mse: 165.2643\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 200us/sample - loss: 135.4138 - mse: 135.4138 - val_loss: 164.6794 - val_mse: 164.6794\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 140.2235 - mse: 140.2235 - val_loss: 165.8316 - val_mse: 165.8316\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 133.5925 - mse: 133.5925 - val_loss: 164.5498 - val_mse: 164.5498\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 135.5912 - mse: 135.5912 - val_loss: 164.5042 - val_mse: 164.5042\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 138.9087 - mse: 138.9087 - val_loss: 164.5760 - val_mse: 164.5760\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 131.8587 - mse: 131.8588 - val_loss: 166.5365 - val_mse: 166.5365\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 136.1764 - mse: 136.1764 - val_loss: 163.2113 - val_mse: 163.2113\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 137.8137 - mse: 137.8137 - val_loss: 164.7428 - val_mse: 164.7428\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 131.3418 - mse: 131.3418 - val_loss: 162.7106 - val_mse: 162.7106\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 138.8863 - mse: 138.8862 - val_loss: 165.6852 - val_mse: 165.6853\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 130.7113 - mse: 130.7113 - val_loss: 163.1165 - val_mse: 163.1165\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 130.0730 - mse: 130.0730 - val_loss: 167.2508 - val_mse: 167.2508\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 133.1670 - mse: 133.1670 - val_loss: 161.7414 - val_mse: 161.7414\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 132.3663 - mse: 132.3662 - val_loss: 160.9377 - val_mse: 160.9377\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 126.3124 - mse: 126.3124 - val_loss: 160.5851 - val_mse: 160.5851\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 131.0308 - mse: 131.0309 - val_loss: 160.5248 - val_mse: 160.5248\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 124.3901 - mse: 124.3901 - val_loss: 160.4001 - val_mse: 160.4001\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 137.3703 - mse: 137.3702 - val_loss: 159.6485 - val_mse: 159.6485\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 124.5210 - mse: 124.5210 - val_loss: 160.7337 - val_mse: 160.7336\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 125.9095 - mse: 125.9095 - val_loss: 159.2026 - val_mse: 159.2026\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 124.5054 - mse: 124.5054 - val_loss: 158.7130 - val_mse: 158.7130\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 130.8669 - mse: 130.8670 - val_loss: 158.3101 - val_mse: 158.3101\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 134.6662 - mse: 134.6663 - val_loss: 159.8418 - val_mse: 159.8418\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 136.7966 - mse: 136.7966 - val_loss: 160.7589 - val_mse: 160.7589\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 129.0170 - mse: 129.0170 - val_loss: 158.9777 - val_mse: 158.9778\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 128.2981 - mse: 128.2982 - val_loss: 160.5793 - val_mse: 160.5793\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 126.7306 - mse: 126.7307 - val_loss: 157.8377 - val_mse: 157.8377\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 126.3533 - mse: 126.3533 - val_loss: 165.0402 - val_mse: 165.0402\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 125.1812 - mse: 125.1812 - val_loss: 157.6872 - val_mse: 157.6872\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 123.0611 - mse: 123.0611 - val_loss: 156.5241 - val_mse: 156.5241\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 127.9893 - mse: 127.9894 - val_loss: 160.5604 - val_mse: 160.5604\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 128.6648 - mse: 128.6648 - val_loss: 156.7372 - val_mse: 156.7372\n",
      "Epoch 139/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 128.1419 - mse: 128.1418 - val_loss: 157.0728 - val_mse: 157.0728\n",
      "Epoch 140/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 125.2093 - mse: 125.2093 - val_loss: 157.0661 - val_mse: 157.0660\n",
      "Epoch 141/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 122.6185 - mse: 122.6185 - val_loss: 155.9695 - val_mse: 155.9696\n",
      "Epoch 142/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 120.6269 - mse: 120.626 - 1s 203us/sample - loss: 120.2674 - mse: 120.2674 - val_loss: 155.9176 - val_mse: 155.9176\n",
      "Epoch 143/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 122.1419 - mse: 122.1419 - val_loss: 155.9869 - val_mse: 155.9869\n",
      "Epoch 144/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 119.9219 - mse: 119.9219 - val_loss: 155.9099 - val_mse: 155.9099\n",
      "Epoch 145/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 124.3291 - mse: 124.3291 - val_loss: 155.6252 - val_mse: 155.6252\n",
      "Epoch 146/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 118.6517 - mse: 118.6517 - val_loss: 158.5689 - val_mse: 158.5689\n",
      "Epoch 147/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 126.3060 - mse: 126.3060 - val_loss: 156.1489 - val_mse: 156.1489\n",
      "Epoch 148/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 124.8461 - mse: 124.8461 - val_loss: 155.7796 - val_mse: 155.7796\n",
      "Epoch 149/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 124.5694 - mse: 124.5693 - val_loss: 161.6339 - val_mse: 161.6339\n",
      "Epoch 150/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 125.5843 - mse: 125.5843 - val_loss: 163.5622 - val_mse: 163.5622\n",
      "Epoch 151/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 126.6657 - mse: 126.6657 - val_loss: 156.7063 - val_mse: 156.7063\n",
      "Epoch 152/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 126.5714 - mse: 126.5714 - val_loss: 156.8133 - val_mse: 156.8133\n",
      "Epoch 153/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 121.5661 - mse: 121.5661 - val_loss: 157.5421 - val_mse: 157.5421\n",
      "Epoch 154/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 123.6608 - mse: 123.6608 - val_loss: 164.2217 - val_mse: 164.2217\n",
      "Epoch 155/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 124.3215 - mse: 124.3215 - val_loss: 156.6485 - val_mse: 156.6485\n",
      "[CV] ...................................... nl=0, nn=24, total= 1.6min\n",
      "[CV] nl=1, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 683us/sample - loss: 532.7617 - mse: 532.7617 - val_loss: 676.6752 - val_mse: 676.6751\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 504.8641 - mse: 504.8642 - val_loss: 645.4394 - val_mse: 645.4392\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 477.4264 - mse: 477.4264 - val_loss: 599.4735 - val_mse: 599.4733\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 447.2339 - mse: 447.2340 - val_loss: 553.9719 - val_mse: 553.9719\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 413.6537 - mse: 413.6537 - val_loss: 508.3510 - val_mse: 508.3510\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 382.0049 - mse: 382.0050 - val_loss: 477.7334 - val_mse: 477.7333\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 352.9062 - mse: 352.9062 - val_loss: 434.2079 - val_mse: 434.2079\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 324.3959 - mse: 324.3959 - val_loss: 404.5138 - val_mse: 404.5138\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 302.3893 - mse: 302.3893 - val_loss: 391.1156 - val_mse: 391.1155\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 282.6904 - mse: 282.6904 - val_loss: 359.7080 - val_mse: 359.7080\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 270.9714 - mse: 270.9714 - val_loss: 336.1670 - val_mse: 336.1671\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 257.2748 - mse: 257.2747 - val_loss: 323.2859 - val_mse: 323.2859\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 246.5863 - mse: 246.5863 - val_loss: 312.6391 - val_mse: 312.6392\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 240.5610 - mse: 240.5611 - val_loss: 285.1241 - val_mse: 285.1241\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 227.2015 - mse: 227.2014 - val_loss: 291.5194 - val_mse: 291.5195\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 226.5810 - mse: 226.5810 - val_loss: 273.9536 - val_mse: 273.9536\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 222.0326 - mse: 222.0326 - val_loss: 267.6753 - val_mse: 267.6753\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 214.4496 - mse: 214.4496 - val_loss: 268.8962 - val_mse: 268.8962\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 212.3543 - mse: 212.3544 - val_loss: 244.3076 - val_mse: 244.3075\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 208.5926 - mse: 208.5926 - val_loss: 248.1645 - val_mse: 248.1646\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 207.7355 - mse: 207.7355 - val_loss: 233.7280 - val_mse: 233.7281\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 206.7399 - mse: 206.7399 - val_loss: 242.7145 - val_mse: 242.7145\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 200.8217 - mse: 200.8216 - val_loss: 228.4153 - val_mse: 228.4153\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 199.9828 - mse: 199.9829 - val_loss: 229.6943 - val_mse: 229.6943\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 200.2891 - mse: 200.2891 - val_loss: 224.1808 - val_mse: 224.1808\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 195.4261 - mse: 195.4261 - val_loss: 231.6888 - val_mse: 231.6888\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 195.3953 - mse: 195.3953 - val_loss: 210.8110 - val_mse: 210.8110\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 192.0039 - mse: 192.0039 - val_loss: 208.8290 - val_mse: 208.8289\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 194.2518 - mse: 194.2518 - val_loss: 211.0804 - val_mse: 211.0804\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 196.3457 - mse: 196.3457 - val_loss: 214.0006 - val_mse: 214.0006\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 188.6540 - mse: 188.6541 - val_loss: 206.4942 - val_mse: 206.4942\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 199.6566 - mse: 199.6566 - val_loss: 207.9829 - val_mse: 207.9828\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 189.1531 - mse: 189.1531 - val_loss: 217.1515 - val_mse: 217.1515\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 185.8471 - mse: 185.8472 - val_loss: 202.4575 - val_mse: 202.4575\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 197.8052 - mse: 197.8052 - val_loss: 205.3838 - val_mse: 205.3838\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 187.8951 - mse: 187.8951 - val_loss: 207.3116 - val_mse: 207.3116\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 185.4770 - mse: 185.4770 - val_loss: 211.3287 - val_mse: 211.3288\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 189.3600 - mse: 189.3600 - val_loss: 205.2277 - val_mse: 205.2277\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 188.1409 - mse: 188.1409 - val_loss: 219.2325 - val_mse: 219.2325\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 192.4245 - mse: 192.4245 - val_loss: 214.0690 - val_mse: 214.0690\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 184.1191 - mse: 184.1191 - val_loss: 201.9925 - val_mse: 201.9925\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 184.0314 - mse: 184.0313 - val_loss: 211.8538 - val_mse: 211.8538\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 183.8191 - mse: 183.8191 - val_loss: 226.1444 - val_mse: 226.1444\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 182.0862 - mse: 182.0862 - val_loss: 202.5686 - val_mse: 202.5686\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 179.5417 - mse: 179.5417 - val_loss: 208.8892 - val_mse: 208.8893\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 180.6842 - mse: 180.6842 - val_loss: 223.1363 - val_mse: 223.1363\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 181.0664 - mse: 181.0664 - val_loss: 208.1843 - val_mse: 208.1843\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 182.3771 - mse: 182.3770 - val_loss: 221.9322 - val_mse: 221.9322\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 178.4408 - mse: 178.4407 - val_loss: 211.9611 - val_mse: 211.9610\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 174.3473 - mse: 174.3473 - val_loss: 211.7356 - val_mse: 211.7357\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 183.8246 - mse: 183.8246 - val_loss: 212.7815 - val_mse: 212.7815\n",
      "[CV] ....................................... nl=1, nn=2, total=  33.6s\n",
      "[CV] nl=1, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 591us/sample - loss: 484.1873 - mse: 484.1873 - val_loss: 675.2947 - val_mse: 675.2946\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 469.6568 - mse: 469.6566 - val_loss: 658.7438 - val_mse: 658.7438\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 452.7933 - mse: 452.7933 - val_loss: 638.2792 - val_mse: 638.2793\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 433.4231 - mse: 433.4231 - val_loss: 615.9560 - val_mse: 615.9559\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 413.6205 - mse: 413.6205 - val_loss: 593.6592 - val_mse: 593.6591\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 395.2767 - mse: 395.2768 - val_loss: 573.9029 - val_mse: 573.9029\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 379.5275 - mse: 379.5276 - val_loss: 557.2679 - val_mse: 557.2681\n",
      "Epoch 8/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 214us/sample - loss: 367.1076 - mse: 367.1076 - val_loss: 543.9263 - val_mse: 543.9262\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 358.1813 - mse: 358.1813 - val_loss: 534.3257 - val_mse: 534.3257\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 352.0947 - mse: 352.0946 - val_loss: 527.8257 - val_mse: 527.8256\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 348.2372 - mse: 348.2373 - val_loss: 523.2429 - val_mse: 523.2429\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 345.9765 - mse: 345.9763 - val_loss: 520.4888 - val_mse: 520.4888\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 344.7429 - mse: 344.7428 - val_loss: 518.6115 - val_mse: 518.6116\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 344.0947 - mse: 344.0947 - val_loss: 517.5677 - val_mse: 517.5677\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 343.7661 - mse: 343.7662 - val_loss: 517.0162 - val_mse: 517.0162\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 343.6014 - mse: 343.6013 - val_loss: 516.4730 - val_mse: 516.4730\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 343.5044 - mse: 343.5044 - val_loss: 516.0796 - val_mse: 516.0795\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 343.4729 - mse: 343.4729 - val_loss: 515.9768 - val_mse: 515.9766\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 343.4533 - mse: 343.4534 - val_loss: 515.7502 - val_mse: 515.7502\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 343.4481 - mse: 343.4481 - val_loss: 515.7243 - val_mse: 515.7243\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 343.4565 - mse: 343.4564 - val_loss: 515.7613 - val_mse: 515.7612\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 343.4675 - mse: 343.4674 - val_loss: 515.7158 - val_mse: 515.7158\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 343.4646 - mse: 343.4646 - val_loss: 515.6580 - val_mse: 515.6581\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 192us/sample - loss: 343.4501 - mse: 343.4501 - val_loss: 515.6808 - val_mse: 515.6808\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 343.4533 - mse: 343.4533 - val_loss: 515.7049 - val_mse: 515.7049\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 343.4577 - mse: 343.4577 - val_loss: 515.7187 - val_mse: 515.7187\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 343.4507 - mse: 343.4507 - val_loss: 515.6803 - val_mse: 515.6802\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 343.4486 - mse: 343.4486 - val_loss: 515.7192 - val_mse: 515.7192\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 343.4431 - mse: 343.4431 - val_loss: 515.6624 - val_mse: 515.6625\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 343.4456 - mse: 343.4457 - val_loss: 515.6581 - val_mse: 515.6581\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 343.4585 - mse: 343.4585 - val_loss: 515.7129 - val_mse: 515.7128\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 343.4544 - mse: 343.4543 - val_loss: 515.5743 - val_mse: 515.5743\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 343.4783 - mse: 343.4782 - val_loss: 515.5866 - val_mse: 515.5865\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 343.4548 - mse: 343.4550 - val_loss: 515.5949 - val_mse: 515.5949\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 343.4606 - mse: 343.4606 - val_loss: 515.6719 - val_mse: 515.6719\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 343.4408 - mse: 343.4407 - val_loss: 515.7150 - val_mse: 515.7150\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 343.4647 - mse: 343.4647 - val_loss: 515.6497 - val_mse: 515.6498\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 343.4487 - mse: 343.4487 - val_loss: 515.6961 - val_mse: 515.6961\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 343.4770 - mse: 343.4771 - val_loss: 515.6218 - val_mse: 515.6220\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 343.4520 - mse: 343.4519 - val_loss: 515.6869 - val_mse: 515.6869\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 343.4701 - mse: 343.4701 - val_loss: 515.7390 - val_mse: 515.7391\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 343.4515 - mse: 343.4515 - val_loss: 515.7302 - val_mse: 515.7303\n",
      "[CV] ....................................... nl=1, nn=2, total=  29.3s\n",
      "[CV] nl=1, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 696us/sample - loss: 556.1784 - mse: 556.1785 - val_loss: 678.1712 - val_mse: 678.1713\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 517.2434 - mse: 517.2434 - val_loss: 654.2898 - val_mse: 654.2897\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 480.7289 - mse: 480.7289 - val_loss: 576.8151 - val_mse: 576.8151\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 442.2543 - mse: 442.2544 - val_loss: 530.5026 - val_mse: 530.5026\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 401.0203 - mse: 401.0204 - val_loss: 456.8038 - val_mse: 456.8039\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 368.4204 - mse: 368.4202 - val_loss: 426.3288 - val_mse: 426.3289\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 330.7154 - mse: 330.7154 - val_loss: 390.9823 - val_mse: 390.9824\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 302.8073 - mse: 302.8074 - val_loss: 350.0615 - val_mse: 350.0616\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 283.4471 - mse: 283.4471 - val_loss: 304.6304 - val_mse: 304.6304\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 260.6969 - mse: 260.6970 - val_loss: 303.7548 - val_mse: 303.7547\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 243.9510 - mse: 243.9511 - val_loss: 263.6472 - val_mse: 263.6472\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 229.3559 - mse: 229.3560 - val_loss: 251.3603 - val_mse: 251.3603\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 227.1768 - mse: 227.1767 - val_loss: 237.1917 - val_mse: 237.1917\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 212.3387 - mse: 212.3387 - val_loss: 225.6458 - val_mse: 225.6458\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 213.4852 - mse: 213.4852 - val_loss: 228.4757 - val_mse: 228.4757\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 208.6376 - mse: 208.6377 - val_loss: 222.9430 - val_mse: 222.9430\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 206.5538 - mse: 206.5537 - val_loss: 205.2658 - val_mse: 205.2658\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 206.8739 - mse: 206.8740 - val_loss: 206.8655 - val_mse: 206.8654\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 200.0255 - mse: 200.0255 - val_loss: 206.3459 - val_mse: 206.3459\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 198.9056 - mse: 198.9054 - val_loss: 196.8338 - val_mse: 196.8338\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 201.0285 - mse: 201.0285 - val_loss: 194.8032 - val_mse: 194.8031\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 198.7185 - mse: 198.7184 - val_loss: 196.2749 - val_mse: 196.2748\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 198.6373 - mse: 198.6373 - val_loss: 197.0891 - val_mse: 197.0891\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 196.5659 - mse: 196.5659 - val_loss: 199.3298 - val_mse: 199.3299\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 196.4549 - mse: 196.4549 - val_loss: 191.1201 - val_mse: 191.1201\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 190.6739 - mse: 190.6739 - val_loss: 186.0682 - val_mse: 186.0682\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 195.2744 - mse: 195.2743 - val_loss: 192.9631 - val_mse: 192.9631\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 201.9190 - mse: 201.9190 - val_loss: 188.8306 - val_mse: 188.8306\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 191.6351 - mse: 191.6351 - val_loss: 185.3025 - val_mse: 185.3026\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 194.9500 - mse: 194.9500 - val_loss: 190.0017 - val_mse: 190.0017\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 187.7369 - mse: 187.7369 - val_loss: 186.5128 - val_mse: 186.5128\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 191.8775 - mse: 191.8775 - val_loss: 187.5422 - val_mse: 187.5421\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 191.7733 - mse: 191.7733 - val_loss: 189.7995 - val_mse: 189.7995\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 191.2366 - mse: 191.2366 - val_loss: 188.6449 - val_mse: 188.6449\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 202.5345 - mse: 202.5345 - val_loss: 194.7551 - val_mse: 194.7551\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 188.9183 - mse: 188.9184 - val_loss: 185.3303 - val_mse: 185.3304\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 184.3432 - mse: 184.3433 - val_loss: 182.5152 - val_mse: 182.5152\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 181.0979 - mse: 181.0979 - val_loss: 186.6667 - val_mse: 186.6667\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 185.8362 - mse: 185.8363 - val_loss: 185.2735 - val_mse: 185.2735\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 189.3338 - mse: 189.3338 - val_loss: 179.4432 - val_mse: 179.4432\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 181.2672 - mse: 181.2672 - val_loss: 187.8911 - val_mse: 187.8911\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 184.8950 - mse: 184.8951 - val_loss: 196.6574 - val_mse: 196.6574\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 178.6749 - mse: 178.6749 - val_loss: 179.3792 - val_mse: 179.3792\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 181.8296 - mse: 181.8296 - val_loss: 180.5953 - val_mse: 180.5954\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 188.1482 - mse: 188.1482 - val_loss: 180.6629 - val_mse: 180.6629\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 190.6553 - mse: 190.6553 - val_loss: 180.8543 - val_mse: 180.8543\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 181.9376 - mse: 181.9376 - val_loss: 185.7342 - val_mse: 185.7342\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 180.7423 - mse: 180.7423 - val_loss: 177.1063 - val_mse: 177.1062\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 178.1975 - mse: 178.1974 - val_loss: 180.2037 - val_mse: 180.2038\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 188.4596 - mse: 188.4595 - val_loss: 180.5605 - val_mse: 180.5605\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 183.9607 - mse: 183.9607 - val_loss: 190.1347 - val_mse: 190.1347\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 190.9574 - mse: 190.9574 - val_loss: 184.7000 - val_mse: 184.7000\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 183.2309 - mse: 183.2309 - val_loss: 189.5439 - val_mse: 189.5439\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 179.2233 - mse: 179.2233 - val_loss: 186.2486 - val_mse: 186.2486\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 174.9029 - mse: 174.9029 - val_loss: 184.5960 - val_mse: 184.5959\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 188.1803 - mse: 188.1804 - val_loss: 185.0831 - val_mse: 185.0832\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 177.2676 - mse: 177.2676 - val_loss: 179.7432 - val_mse: 179.7432\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 175.9330 - mse: 175.9330 - val_loss: 179.2955 - val_mse: 179.2955\n",
      "[CV] ....................................... nl=1, nn=2, total=  38.4s\n",
      "[CV] nl=1, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 645us/sample - loss: 456.3907 - mse: 456.3907 - val_loss: 535.4707 - val_mse: 535.4705\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 413.2606 - mse: 413.2606 - val_loss: 488.3353 - val_mse: 488.3353\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 372.9590 - mse: 372.9589 - val_loss: 404.9129 - val_mse: 404.9130\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 340.7420 - mse: 340.7419 - val_loss: 349.8082 - val_mse: 349.8082\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 308.6000 - mse: 308.6001 - val_loss: 315.3757 - val_mse: 315.3757\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 276.0198 - mse: 276.0198 - val_loss: 289.6810 - val_mse: 289.6810\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 249.8255 - mse: 249.8256 - val_loss: 231.2504 - val_mse: 231.2504\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 228.8026 - mse: 228.8026 - val_loss: 213.5083 - val_mse: 213.5082\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 210.6890 - mse: 210.6890 - val_loss: 214.3678 - val_mse: 214.3679\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 197.0385 - mse: 197.0385 - val_loss: 172.6839 - val_mse: 172.6839\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 189.6446 - mse: 189.6446 - val_loss: 169.6053 - val_mse: 169.6053\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 184.4908 - mse: 184.4908 - val_loss: 168.2205 - val_mse: 168.2205\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 173.1477 - mse: 173.1477 - val_loss: 156.8865 - val_mse: 156.8866\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 175.3945 - mse: 175.3946 - val_loss: 152.6427 - val_mse: 152.6427\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 169.6173 - mse: 169.6172 - val_loss: 155.4713 - val_mse: 155.4713\n",
      "Epoch 16/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 269us/sample - loss: 168.5444 - mse: 168.5445 - val_loss: 148.1204 - val_mse: 148.1204\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 169.8923 - mse: 169.8923 - val_loss: 150.7237 - val_mse: 150.7238\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 165.8317 - mse: 165.8317 - val_loss: 145.9023 - val_mse: 145.9023\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 170.6430 - mse: 170.6431 - val_loss: 140.2008 - val_mse: 140.2008\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 163.5870 - mse: 163.5870 - val_loss: 141.7176 - val_mse: 141.7176\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 176.7221 - mse: 176.7221 - val_loss: 141.6698 - val_mse: 141.6698\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 162.9229 - mse: 162.9229 - val_loss: 140.6495 - val_mse: 140.6495\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 166.9928 - mse: 166.9928 - val_loss: 135.4682 - val_mse: 135.4682\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 161.5455 - mse: 161.5455 - val_loss: 141.8408 - val_mse: 141.8408\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 162.3521 - mse: 162.3521 - val_loss: 137.1770 - val_mse: 137.1770\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 164.0176 - mse: 164.0176 - val_loss: 136.8917 - val_mse: 136.8917\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 162.8458 - mse: 162.8458 - val_loss: 133.0639 - val_mse: 133.0639\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 166.2508 - mse: 166.2507 - val_loss: 132.8197 - val_mse: 132.8197\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 161.2823 - mse: 161.2823 - val_loss: 133.9156 - val_mse: 133.9155\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 154.4976 - mse: 154.4976 - val_loss: 139.0751 - val_mse: 139.0751\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 159.5580 - mse: 159.5580 - val_loss: 132.8638 - val_mse: 132.8638\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 164.3467 - mse: 164.3467 - val_loss: 130.6014 - val_mse: 130.6015\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 162.4872 - mse: 162.4872 - val_loss: 131.1911 - val_mse: 131.1910\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 161.4979 - mse: 161.4979 - val_loss: 132.9063 - val_mse: 132.9063\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 159.0388 - mse: 159.0388 - val_loss: 131.4071 - val_mse: 131.4071\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 156.3162 - mse: 156.3161 - val_loss: 137.5948 - val_mse: 137.5948\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 159.5495 - mse: 159.5494 - val_loss: 130.2791 - val_mse: 130.2791\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 150.1773 - mse: 150.1773 - val_loss: 132.0346 - val_mse: 132.0346\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 161.5595 - mse: 161.5595 - val_loss: 130.7616 - val_mse: 130.7616\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 155.8924 - mse: 155.8923 - val_loss: 136.9543 - val_mse: 136.9543\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 149.9565 - mse: 149.9564 - val_loss: 127.7419 - val_mse: 127.7419\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 155.8555 - mse: 155.8555 - val_loss: 127.2155 - val_mse: 127.2155\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 161.2587 - mse: 161.2587 - val_loss: 130.5839 - val_mse: 130.5839\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 151.2322 - mse: 151.2322 - val_loss: 129.2219 - val_mse: 129.2219\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 153.9157 - mse: 153.9156 - val_loss: 129.8733 - val_mse: 129.8733\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 153.9957 - mse: 153.9957 - val_loss: 129.4629 - val_mse: 129.4629\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 150.5059 - mse: 150.5059 - val_loss: 127.9808 - val_mse: 127.9808\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 154.5168 - mse: 154.5168 - val_loss: 124.0122 - val_mse: 124.0122\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 154.2319 - mse: 154.2320 - val_loss: 124.9997 - val_mse: 124.9998\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 155.5976 - mse: 155.5975 - val_loss: 125.7239 - val_mse: 125.7239\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 157.2413 - mse: 157.2413 - val_loss: 124.2590 - val_mse: 124.2590\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 158.9521 - mse: 158.9521 - val_loss: 124.1963 - val_mse: 124.1963\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 150.0569 - mse: 150.0569 - val_loss: 131.6020 - val_mse: 131.6020\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 150.1656 - mse: 150.1655 - val_loss: 125.6524 - val_mse: 125.6524\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 155.1163 - mse: 155.1164 - val_loss: 125.4614 - val_mse: 125.4614\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 156.9643 - mse: 156.9643 - val_loss: 123.6740 - val_mse: 123.6740\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 151.2486 - mse: 151.2487 - val_loss: 126.9617 - val_mse: 126.9617\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 150.0498 - mse: 150.0498 - val_loss: 123.8086 - val_mse: 123.8086\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 256us/sample - loss: 151.6646 - mse: 151.6646 - val_loss: 124.0693 - val_mse: 124.0693\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 159.0834 - mse: 159.0834 - val_loss: 123.7111 - val_mse: 123.7111\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 150.8102 - mse: 150.8102 - val_loss: 128.4608 - val_mse: 128.4608\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 148.9406 - mse: 148.9405 - val_loss: 122.5464 - val_mse: 122.5464\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 150.3131 - mse: 150.3131 - val_loss: 124.3849 - val_mse: 124.3849\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 138.0700 - mse: 138.0700 - val_loss: 126.4781 - val_mse: 126.4781\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 148.2548 - mse: 148.2548 - val_loss: 123.5049 - val_mse: 123.5049\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 147.2468 - mse: 147.2468 - val_loss: 130.2536 - val_mse: 130.2536\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 150.7045 - mse: 150.7045 - val_loss: 129.4761 - val_mse: 129.4761\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 151.8575 - mse: 151.8575 - val_loss: 125.4708 - val_mse: 125.4708\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 146.0614 - mse: 146.0614 - val_loss: 125.4762 - val_mse: 125.4762\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 142.8898 - mse: 142.8898 - val_loss: 123.9176 - val_mse: 123.9177\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 148.6934 - mse: 148.6933 - val_loss: 123.9773 - val_mse: 123.9773\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 145.3669 - mse: 145.3669 - val_loss: 126.9638 - val_mse: 126.9637\n",
      "[CV] ....................................... nl=1, nn=2, total=  47.1s\n",
      "[CV] nl=1, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 702us/sample - loss: 474.2588 - mse: 474.2590 - val_loss: 635.5090 - val_mse: 635.5090\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 452.1107 - mse: 452.1107 - val_loss: 602.4885 - val_mse: 602.4885\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 425.8833 - mse: 425.8833 - val_loss: 556.2642 - val_mse: 556.2643\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 398.2280 - mse: 398.2281 - val_loss: 504.7932 - val_mse: 504.7931\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 371.3475 - mse: 371.3474 - val_loss: 487.1617 - val_mse: 487.1616\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 342.2462 - mse: 342.2463 - val_loss: 439.9531 - val_mse: 439.9531\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 315.1488 - mse: 315.1488 - val_loss: 411.1176 - val_mse: 411.1176\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 289.6582 - mse: 289.6583 - val_loss: 370.3505 - val_mse: 370.3505\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 271.6020 - mse: 271.6021 - val_loss: 362.8502 - val_mse: 362.8502\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 258.4115 - mse: 258.4115 - val_loss: 331.1976 - val_mse: 331.1976\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 245.5158 - mse: 245.5157 - val_loss: 318.5580 - val_mse: 318.5579\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 229.8019 - mse: 229.8019 - val_loss: 291.9470 - val_mse: 291.9470\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 218.0659 - mse: 218.065 - 1s 198us/sample - loss: 223.3420 - mse: 223.3420 - val_loss: 279.2854 - val_mse: 279.2854\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 214.0728 - mse: 214.0727 - val_loss: 277.4621 - val_mse: 277.4621\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 211.6389 - mse: 211.6389 - val_loss: 262.5217 - val_mse: 262.5217\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 205.0138 - mse: 205.0138 - val_loss: 259.8696 - val_mse: 259.8697\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 199.4489 - mse: 199.4489 - val_loss: 253.1550 - val_mse: 253.1550\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 197.0212 - mse: 197.0212 - val_loss: 231.9391 - val_mse: 231.9392\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 197.7356 - mse: 197.7356 - val_loss: 248.3108 - val_mse: 248.3108\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 196.5719 - mse: 196.5719 - val_loss: 229.8133 - val_mse: 229.8133\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 188.2976 - mse: 188.2976 - val_loss: 228.1270 - val_mse: 228.1270\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 182.1080 - mse: 182.1080 - val_loss: 226.2062 - val_mse: 226.2062\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 185.1378 - mse: 185.1377 - val_loss: 225.9716 - val_mse: 225.9716\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 187.5819 - mse: 187.5819 - val_loss: 214.2069 - val_mse: 214.2070\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 182.9043 - mse: 182.9043 - val_loss: 211.8373 - val_mse: 211.8373\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 186.2222 - mse: 186.2222 - val_loss: 217.3561 - val_mse: 217.3561\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 181.8209 - mse: 181.8209 - val_loss: 217.1485 - val_mse: 217.1485\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 184.1491 - mse: 184.1491 - val_loss: 216.2908 - val_mse: 216.2908\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 180.6939 - mse: 180.6940 - val_loss: 209.5847 - val_mse: 209.5847\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 178.4298 - mse: 178.4297 - val_loss: 205.2910 - val_mse: 205.2910\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.7381 - mse: 185.7381 - val_loss: 209.0834 - val_mse: 209.0833\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 182.6840 - mse: 182.6840 - val_loss: 204.8104 - val_mse: 204.8104\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 179.8101 - mse: 179.8102 - val_loss: 203.6950 - val_mse: 203.6950\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 179.0424 - mse: 179.0423 - val_loss: 204.0104 - val_mse: 204.0104\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 177.0173 - mse: 177.0173 - val_loss: 200.2023 - val_mse: 200.2024\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 183.7027 - mse: 183.7027 - val_loss: 204.9986 - val_mse: 204.9986\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 181.8336 - mse: 181.8336 - val_loss: 196.9228 - val_mse: 196.9228\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 175.5522 - mse: 175.5522 - val_loss: 200.3032 - val_mse: 200.3032\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 176.7257 - mse: 176.7257 - val_loss: 199.4320 - val_mse: 199.4320\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 174.1003 - mse: 174.1004 - val_loss: 209.4182 - val_mse: 209.4182\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 172.8499 - mse: 172.8500 - val_loss: 201.6317 - val_mse: 201.6317\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 180.9333 - mse: 180.9333 - val_loss: 204.0446 - val_mse: 204.0446\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 179.2433 - mse: 179.2433 - val_loss: 210.7627 - val_mse: 210.7627\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 175.8880 - mse: 175.8880 - val_loss: 199.2432 - val_mse: 199.2432\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 173.1963 - mse: 173.1963 - val_loss: 200.5077 - val_mse: 200.5077\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 174.2533 - mse: 174.2534 - val_loss: 202.0988 - val_mse: 202.0988\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 178.9568 - mse: 178.9568 - val_loss: 193.6825 - val_mse: 193.6825\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 173.0210 - mse: 173.0211 - val_loss: 193.9690 - val_mse: 193.9690\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 173.0310 - mse: 173.0310 - val_loss: 196.6196 - val_mse: 196.6196\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 167.1952 - mse: 167.1952 - val_loss: 203.3904 - val_mse: 203.3904\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 172.1835 - mse: 172.1835 - val_loss: 198.9580 - val_mse: 198.9580\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 171.6851 - mse: 171.6851 - val_loss: 195.1384 - val_mse: 195.1384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 168.8996 - mse: 168.8995 - val_loss: 195.6441 - val_mse: 195.6441\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 168.2840 - mse: 168.2840 - val_loss: 193.2959 - val_mse: 193.2959\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 172.5810 - mse: 172.5810 - val_loss: 194.6276 - val_mse: 194.6276\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 174.5128 - mse: 174.5127 - val_loss: 191.1965 - val_mse: 191.1965\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 172.0953 - mse: 172.0953 - val_loss: 197.5512 - val_mse: 197.5512\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 168.8486 - mse: 168.8486 - val_loss: 196.2014 - val_mse: 196.2014\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 172.5881 - mse: 172.5880 - val_loss: 206.3757 - val_mse: 206.3757\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 174.4047 - mse: 174.4047 - val_loss: 192.2105 - val_mse: 192.2105\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 165.9205 - mse: 165.9205 - val_loss: 190.5932 - val_mse: 190.5932\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 158.9674 - mse: 158.9675 - val_loss: 193.3478 - val_mse: 193.3478\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 167.4935 - mse: 167.4935 - val_loss: 204.8139 - val_mse: 204.8139\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 166.5561 - mse: 166.5561 - val_loss: 197.7688 - val_mse: 197.7688\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 160.8338 - mse: 160.8338 - val_loss: 201.1696 - val_mse: 201.1695\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 172.1757 - mse: 172.1757 - val_loss: 191.9481 - val_mse: 191.9480\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 161.3641 - mse: 161.3641 - val_loss: 200.8611 - val_mse: 200.8611\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 163.9793 - mse: 163.9793 - val_loss: 202.4870 - val_mse: 202.4870\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 160.9785 - mse: 160.9785 - val_loss: 201.7345 - val_mse: 201.7345\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 168.4203 - mse: 168.4203 - val_loss: 190.5603 - val_mse: 190.5603\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 161.9979 - mse: 161.9978 - val_loss: 191.2417 - val_mse: 191.2417\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 165.1602 - mse: 165.1602 - val_loss: 193.5133 - val_mse: 193.5133\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 159.6858 - mse: 159.6857 - val_loss: 197.5659 - val_mse: 197.5659\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 159.0124 - mse: 159.0124 - val_loss: 191.4382 - val_mse: 191.4382\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 166.9495 - mse: 166.9495 - val_loss: 191.0699 - val_mse: 191.0699\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 162.6293 - mse: 162.6293 - val_loss: 197.7842 - val_mse: 197.7842\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 154.2828 - mse: 154.2828 - val_loss: 194.8434 - val_mse: 194.8434\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 153.4662 - mse: 153.4662 - val_loss: 195.5678 - val_mse: 195.5678\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 152.3389 - mse: 152.3389 - val_loss: 189.1227 - val_mse: 189.1227\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 161.9304 - mse: 161.9303 - val_loss: 187.4384 - val_mse: 187.4384\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 160.7335 - mse: 160.7335 - val_loss: 196.1209 - val_mse: 196.1208\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 156.9268 - mse: 156.9268 - val_loss: 191.1069 - val_mse: 191.1069\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 154.3639 - mse: 154.3640 - val_loss: 189.3687 - val_mse: 189.3687\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 158.9630 - mse: 158.9631 - val_loss: 186.9807 - val_mse: 186.9807\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 154.4293 - mse: 154.4293 - val_loss: 187.6286 - val_mse: 187.6286\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 161.2187 - mse: 161.2187 - val_loss: 189.2212 - val_mse: 189.2212\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 158.5605 - mse: 158.5605 - val_loss: 186.3265 - val_mse: 186.3264\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 155.7917 - mse: 155.7917 - val_loss: 211.7138 - val_mse: 211.7138\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 168.6218 - mse: 168.6218 - val_loss: 188.3318 - val_mse: 188.3318\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 156.1334 - mse: 156.1334 - val_loss: 191.8051 - val_mse: 191.8051\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 158.6011 - mse: 158.6011 - val_loss: 187.6143 - val_mse: 187.6143\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 157.0004 - mse: 157.0004 - val_loss: 185.3296 - val_mse: 185.3296\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 152.5734 - mse: 152.5734 - val_loss: 185.2243 - val_mse: 185.2243\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 157.5465 - mse: 157.5465 - val_loss: 212.6878 - val_mse: 212.6879\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 152.6164 - mse: 152.6165 - val_loss: 197.2714 - val_mse: 197.2714\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 0s 149us/sample - loss: 153.8949 - mse: 153.8949 - val_loss: 185.2171 - val_mse: 185.2170\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 188us/sample - loss: 160.6380 - mse: 160.6380 - val_loss: 185.6240 - val_mse: 185.6240\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 0s 173us/sample - loss: 153.1270 - mse: 153.1270 - val_loss: 186.3182 - val_mse: 186.3183\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 176us/sample - loss: 154.5675 - mse: 154.5676 - val_loss: 187.7447 - val_mse: 187.7447\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 155.1420 - mse: 155.1420 - val_loss: 184.6669 - val_mse: 184.6669\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 156.1958 - mse: 156.1958 - val_loss: 187.2371 - val_mse: 187.2371\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 0s 154us/sample - loss: 155.0045 - mse: 155.0045 - val_loss: 186.9880 - val_mse: 186.9880\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 178us/sample - loss: 156.7349 - mse: 156.7350 - val_loss: 191.0204 - val_mse: 191.0204\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 0s 144us/sample - loss: 154.9877 - mse: 154.9877 - val_loss: 184.2568 - val_mse: 184.2568\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 193us/sample - loss: 150.7945 - mse: 150.7945 - val_loss: 197.7514 - val_mse: 197.7514\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 0s 170us/sample - loss: 151.9717 - mse: 151.9717 - val_loss: 183.8347 - val_mse: 183.8347\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 0s 147us/sample - loss: 151.8680 - mse: 151.8681 - val_loss: 183.6783 - val_mse: 183.6783\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 0s 150us/sample - loss: 155.0380 - mse: 155.0380 - val_loss: 185.7599 - val_mse: 185.7599\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 154.8630 - mse: 154.8630 - val_loss: 183.2174 - val_mse: 183.2174\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 152.5333 - mse: 152.5332 - val_loss: 199.3708 - val_mse: 199.3708\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 148.3081 - mse: 148.3080 - val_loss: 185.6723 - val_mse: 185.6723\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 154.0023 - mse: 154.0023 - val_loss: 186.5573 - val_mse: 186.5573\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 153.0845 - mse: 153.0845 - val_loss: 186.2351 - val_mse: 186.2351\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 154.6243 - mse: 154.6243 - val_loss: 209.0063 - val_mse: 209.0063\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 150.1848 - mse: 150.1848 - val_loss: 182.9804 - val_mse: 182.9805\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 151.4068 - mse: 151.4069 - val_loss: 183.5851 - val_mse: 183.5851\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 147.8492 - mse: 147.8492 - val_loss: 190.0732 - val_mse: 190.0732\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 150.0386 - mse: 150.0386 - val_loss: 193.1131 - val_mse: 193.1131\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 153.9062 - mse: 153.9062 - val_loss: 185.2320 - val_mse: 185.2321\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 152.4158 - mse: 152.4157 - val_loss: 183.3600 - val_mse: 183.3601\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 153.2976 - mse: 153.2976 - val_loss: 184.5004 - val_mse: 184.5004\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 156.5332 - mse: 156.5332 - val_loss: 184.6822 - val_mse: 184.6823\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 151.0760 - mse: 151.0760 - val_loss: 184.4313 - val_mse: 184.4313\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 150.4948 - mse: 150.4948 - val_loss: 184.2380 - val_mse: 184.2380\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 156.0356 - mse: 156.0356 - val_loss: 184.0294 - val_mse: 184.0294\n",
      "[CV] ....................................... nl=1, nn=2, total= 1.3min\n",
      "[CV] nl=1, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 588us/sample - loss: 542.0224 - mse: 542.0225 - val_loss: 678.8306 - val_mse: 678.8305\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 511.7204 - mse: 511.7205 - val_loss: 647.3489 - val_mse: 647.3489\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 475.9774 - mse: 475.9774 - val_loss: 602.5566 - val_mse: 602.5566\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 425.7959 - mse: 425.7957 - val_loss: 539.7352 - val_mse: 539.7351\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 374.4789 - mse: 374.4789 - val_loss: 469.8754 - val_mse: 469.8755\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 334.3902 - mse: 334.3901 - val_loss: 411.9826 - val_mse: 411.9826\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 295.2888 - mse: 295.2888 - val_loss: 348.4004 - val_mse: 348.4005\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 267.9380 - mse: 267.9380 - val_loss: 324.5404 - val_mse: 324.5403\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 244.4645 - mse: 244.4645 - val_loss: 285.0056 - val_mse: 285.0056\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 222.8011 - mse: 222.8011 - val_loss: 270.9244 - val_mse: 270.9243\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 216.9392 - mse: 216.9392 - val_loss: 257.5472 - val_mse: 257.5472\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 207.4578 - mse: 207.4578 - val_loss: 246.3803 - val_mse: 246.3803\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 201.5511 - mse: 201.5512 - val_loss: 224.3387 - val_mse: 224.3387\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 195.6213 - mse: 195.6213 - val_loss: 220.0524 - val_mse: 220.0524\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 201.4311 - mse: 201.431 - 1s 210us/sample - loss: 200.5836 - mse: 200.5836 - val_loss: 208.6973 - val_mse: 208.6973\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 191.4907 - mse: 191.4907 - val_loss: 220.6187 - val_mse: 220.6187\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 193.0250 - mse: 193.0250 - val_loss: 215.2193 - val_mse: 215.2193\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 186.6543 - mse: 186.6543 - val_loss: 193.4181 - val_mse: 193.4181\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 192.7128 - mse: 192.7128 - val_loss: 192.1514 - val_mse: 192.1514\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 182.7385 - mse: 182.7385 - val_loss: 208.0729 - val_mse: 208.0730\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 188.4578 - mse: 188.4578 - val_loss: 197.7713 - val_mse: 197.7713\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 185.2558 - mse: 185.2558 - val_loss: 195.8316 - val_mse: 195.8316\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 186.4770 - mse: 186.4770 - val_loss: 193.6069 - val_mse: 193.6069\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 182.7341 - mse: 182.7341 - val_loss: 200.5988 - val_mse: 200.5988\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 187.2292 - mse: 187.2292 - val_loss: 185.6955 - val_mse: 185.6955\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 183.5197 - mse: 183.5197 - val_loss: 189.9194 - val_mse: 189.9194\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 185.0639 - mse: 185.0639 - val_loss: 189.9139 - val_mse: 189.9139\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 181.4650 - mse: 181.4649 - val_loss: 194.8354 - val_mse: 194.8354\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 179.0197 - mse: 179.0197 - val_loss: 197.3718 - val_mse: 197.3718\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 179.5170 - mse: 179.517 - 1s 222us/sample - loss: 175.7850 - mse: 175.7850 - val_loss: 181.7165 - val_mse: 181.7165\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 174.0278 - mse: 174.0278 - val_loss: 189.8012 - val_mse: 189.8012\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 177.6720 - mse: 177.6720 - val_loss: 189.8897 - val_mse: 189.8898\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 176.6986 - mse: 176.6986 - val_loss: 182.0769 - val_mse: 182.0769\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 170.7465 - mse: 170.7465 - val_loss: 184.3515 - val_mse: 184.3515\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 170.2762 - mse: 170.2762 - val_loss: 180.8282 - val_mse: 180.8282\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 212us/sample - loss: 171.4348 - mse: 171.4348 - val_loss: 181.4143 - val_mse: 181.4143\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 177.2445 - mse: 177.2445 - val_loss: 187.4576 - val_mse: 187.4577\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 174.4048 - mse: 174.4048 - val_loss: 184.2126 - val_mse: 184.2126\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 171.7006 - mse: 171.7006 - val_loss: 187.1329 - val_mse: 187.1329\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 165.8453 - mse: 165.8453 - val_loss: 182.5811 - val_mse: 182.5811\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 162.5018 - mse: 162.5017 - val_loss: 176.8817 - val_mse: 176.8817\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 170.2584 - mse: 170.2585 - val_loss: 183.3093 - val_mse: 183.3092\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 165.0973 - mse: 165.0974 - val_loss: 177.5316 - val_mse: 177.5317\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 170.6333 - mse: 170.6333 - val_loss: 191.6044 - val_mse: 191.6045\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 170.6552 - mse: 170.6552 - val_loss: 179.5966 - val_mse: 179.5966\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 163.6957 - mse: 163.6957 - val_loss: 178.1704 - val_mse: 178.1704\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 162.5168 - mse: 162.5169 - val_loss: 180.3634 - val_mse: 180.3634\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 162.3303 - mse: 162.3304 - val_loss: 175.6351 - val_mse: 175.6351\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 166.1313 - mse: 166.1312 - val_loss: 180.9559 - val_mse: 180.9559\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 165.4596 - mse: 165.4595 - val_loss: 188.5144 - val_mse: 188.5144\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 163.2418 - mse: 163.2418 - val_loss: 184.1671 - val_mse: 184.1671\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 164.9171 - mse: 164.9172 - val_loss: 177.3834 - val_mse: 177.3835\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 164.3188 - mse: 164.3189 - val_loss: 180.5619 - val_mse: 180.5620\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 161.6854 - mse: 161.6854 - val_loss: 171.7079 - val_mse: 171.7079\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 158.0286 - mse: 158.0287 - val_loss: 176.5965 - val_mse: 176.5965\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 154.5416 - mse: 154.5416 - val_loss: 170.8116 - val_mse: 170.8116\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 155.7031 - mse: 155.7031 - val_loss: 166.0341 - val_mse: 166.0341\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 156.3450 - mse: 156.3450 - val_loss: 170.7477 - val_mse: 170.7477\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 160.3511 - mse: 160.3510 - val_loss: 164.4342 - val_mse: 164.4342\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 156.6970 - mse: 156.6970 - val_loss: 177.0021 - val_mse: 177.0020\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 151.4147 - mse: 151.4146 - val_loss: 165.8350 - val_mse: 165.8350\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 150.9024 - mse: 150.9024 - val_loss: 201.4674 - val_mse: 201.4674\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 149.8450 - mse: 149.8450 - val_loss: 182.8364 - val_mse: 182.8364\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 155.9289 - mse: 155.9289 - val_loss: 183.4356 - val_mse: 183.4356\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 148.1940 - mse: 148.1940 - val_loss: 174.3906 - val_mse: 174.3906\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 149.2113 - mse: 149.2113 - val_loss: 189.9639 - val_mse: 189.9639\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 148.9763 - mse: 148.9763 - val_loss: 188.3529 - val_mse: 188.3529\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 145.1660 - mse: 145.1660 - val_loss: 173.4701 - val_mse: 173.4700\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 149.7534 - mse: 149.7534 - val_loss: 197.4124 - val_mse: 197.4124\n",
      "[CV] ....................................... nl=1, nn=3, total=  44.7s\n",
      "[CV] nl=1, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 721us/sample - loss: 475.8638 - mse: 475.8639 - val_loss: 668.2647 - val_mse: 668.2648\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 415.4521 - mse: 415.4521 - val_loss: 591.8973 - val_mse: 591.8972\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 369.5671 - mse: 369.5671 - val_loss: 501.8574 - val_mse: 501.8574\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 324.2027 - mse: 324.2027 - val_loss: 464.4201 - val_mse: 464.4201\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 286.9462 - mse: 286.9462 - val_loss: 398.1011 - val_mse: 398.1011\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 252.0136 - mse: 252.0136 - val_loss: 367.1350 - val_mse: 367.1349\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 231.1563 - mse: 231.1562 - val_loss: 308.1579 - val_mse: 308.1578\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 207.8855 - mse: 207.8855 - val_loss: 294.7112 - val_mse: 294.7112\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 193.1415 - mse: 193.1415 - val_loss: 286.2711 - val_mse: 286.2711\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 182.6362 - mse: 182.6362 - val_loss: 269.1685 - val_mse: 269.1685\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 179.0345 - mse: 179.0345 - val_loss: 280.7193 - val_mse: 280.7193\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 174.5783 - mse: 174.5783 - val_loss: 242.3194 - val_mse: 242.3193\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 168.3467 - mse: 168.3467 - val_loss: 241.3862 - val_mse: 241.3861\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 169.2479 - mse: 169.2479 - val_loss: 223.0329 - val_mse: 223.0330\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 173.9481 - mse: 173.9482 - val_loss: 230.8612 - val_mse: 230.8612\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 165.1825 - mse: 165.1825 - val_loss: 220.0049 - val_mse: 220.0049\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 162.1653 - mse: 162.1653 - val_loss: 215.8362 - val_mse: 215.8362\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 158.6463 - mse: 158.6463 - val_loss: 219.2588 - val_mse: 219.2588\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 161.6857 - mse: 161.6857 - val_loss: 215.3099 - val_mse: 215.3098\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 164.3730 - mse: 164.3729 - val_loss: 221.2791 - val_mse: 221.2791\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 149.9952 - mse: 149.9952 - val_loss: 207.8909 - val_mse: 207.8909\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 160.0861 - mse: 160.0860 - val_loss: 201.4992 - val_mse: 201.4992\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 152.3743 - mse: 152.3743 - val_loss: 198.6565 - val_mse: 198.6565\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 157.9872 - mse: 157.9872 - val_loss: 199.5949 - val_mse: 199.5949\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 153.0122 - mse: 153.0122 - val_loss: 200.2711 - val_mse: 200.2711\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 152.8674 - mse: 152.8674 - val_loss: 208.2282 - val_mse: 208.2282\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 149.5310 - mse: 149.5310 - val_loss: 198.8520 - val_mse: 198.8519\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 151.9617 - mse: 151.9617 - val_loss: 196.4877 - val_mse: 196.4877\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 153.0577 - mse: 153.0577 - val_loss: 194.0689 - val_mse: 194.0689\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 156.6827 - mse: 156.6827 - val_loss: 191.8835 - val_mse: 191.8835\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 149.5575 - mse: 149.5575 - val_loss: 196.5973 - val_mse: 196.5973\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 195us/sample - loss: 150.5399 - mse: 150.5398 - val_loss: 193.9502 - val_mse: 193.9502\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 152.9414 - mse: 152.9414 - val_loss: 200.6407 - val_mse: 200.6407\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 198us/sample - loss: 151.6684 - mse: 151.6685 - val_loss: 194.2043 - val_mse: 194.2043\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 149.7962 - mse: 149.7962 - val_loss: 191.2534 - val_mse: 191.2534\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 150.5651 - mse: 150.5651 - val_loss: 183.7051 - val_mse: 183.7052\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 150.1580 - mse: 150.1580 - val_loss: 194.4550 - val_mse: 194.4550\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 157.9023 - mse: 157.9023 - val_loss: 201.4793 - val_mse: 201.4793\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 150.1485 - mse: 150.1486 - val_loss: 189.2540 - val_mse: 189.2540\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 151.0771 - mse: 151.0771 - val_loss: 189.8698 - val_mse: 189.8698\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 146.7037 - mse: 146.7037 - val_loss: 197.2586 - val_mse: 197.2586\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 147.6110 - mse: 147.6110 - val_loss: 201.2893 - val_mse: 201.2893\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 150.7879 - mse: 150.7880 - val_loss: 191.7307 - val_mse: 191.7307\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 145.2714 - mse: 145.2714 - val_loss: 185.6736 - val_mse: 185.6737\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 143.5731 - mse: 143.5731 - val_loss: 190.4411 - val_mse: 190.4411\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 134.8384 - mse: 134.8384 - val_loss: 194.6494 - val_mse: 194.6493\n",
      "[CV] ....................................... nl=1, nn=3, total=  29.8s\n",
      "[CV] nl=1, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 640us/sample - loss: 541.7162 - mse: 541.7162 - val_loss: 668.0144 - val_mse: 668.0142\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 510.7769 - mse: 510.7771 - val_loss: 625.0212 - val_mse: 625.0213\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 475.0030 - mse: 475.0031 - val_loss: 574.4561 - val_mse: 574.4562\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 433.6567 - mse: 433.6567 - val_loss: 517.1689 - val_mse: 517.1688\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 391.4191 - mse: 391.4190 - val_loss: 469.7826 - val_mse: 469.7826\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 257us/sample - loss: 351.4169 - mse: 351.4170 - val_loss: 421.5473 - val_mse: 421.5473\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 316.4549 - mse: 316.4549 - val_loss: 369.8012 - val_mse: 369.8011\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 287.0060 - mse: 287.0060 - val_loss: 345.4930 - val_mse: 345.4930\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 263.3279 - mse: 263.3279 - val_loss: 310.3697 - val_mse: 310.3697\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 252.8686 - mse: 252.8686 - val_loss: 294.4522 - val_mse: 294.4523\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 240.4634 - mse: 240.4634 - val_loss: 277.8308 - val_mse: 277.8309\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 236.8962 - mse: 236.8963 - val_loss: 265.2095 - val_mse: 265.2095\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 220.7577 - mse: 220.7577 - val_loss: 266.2817 - val_mse: 266.2817\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 227.3218 - mse: 227.3217 - val_loss: 250.9534 - val_mse: 250.9533\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 216.6182 - mse: 216.6183 - val_loss: 235.2710 - val_mse: 235.2710\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 218.0476 - mse: 218.0475 - val_loss: 230.2099 - val_mse: 230.2099\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 213.3978 - mse: 213.3978 - val_loss: 240.6139 - val_mse: 240.6139\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 200.9715 - mse: 200.9714 - val_loss: 223.7420 - val_mse: 223.7420\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 213.5963 - mse: 213.5963 - val_loss: 235.6703 - val_mse: 235.6703\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 205.9151 - mse: 205.9151 - val_loss: 218.6432 - val_mse: 218.6432\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 199.9609 - mse: 199.9609 - val_loss: 218.7790 - val_mse: 218.7790\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 194.8905 - mse: 194.8904 - val_loss: 212.2026 - val_mse: 212.2026\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 199.7263 - mse: 199.7263 - val_loss: 211.2441 - val_mse: 211.2441\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 195.0618 - mse: 195.0618 - val_loss: 206.9974 - val_mse: 206.9974\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 189.3494 - mse: 189.3495 - val_loss: 202.3753 - val_mse: 202.3753\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 185.6023 - mse: 185.6023 - val_loss: 201.3002 - val_mse: 201.3002\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 181.7929 - mse: 181.7928 - val_loss: 203.6166 - val_mse: 203.6166\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.9800 - mse: 185.9800 - val_loss: 205.6479 - val_mse: 205.6479\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 214us/sample - loss: 188.2158 - mse: 188.2158 - val_loss: 204.7641 - val_mse: 204.7641\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 183.0978 - mse: 183.0978 - val_loss: 206.3973 - val_mse: 206.3973\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 181.4797 - mse: 181.4797 - val_loss: 203.1585 - val_mse: 203.1585\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 179.8885 - mse: 179.8884 - val_loss: 199.7643 - val_mse: 199.7643\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 178.3398 - mse: 178.3398 - val_loss: 198.3937 - val_mse: 198.3937\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 179.2792 - mse: 179.2792 - val_loss: 201.8177 - val_mse: 201.8177\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 172.9510 - mse: 172.9510 - val_loss: 202.8066 - val_mse: 202.8066\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 178.0370 - mse: 178.0371 - val_loss: 201.6694 - val_mse: 201.6694\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 173.6693 - mse: 173.669 - 1s 216us/sample - loss: 181.2543 - mse: 181.2543 - val_loss: 199.8208 - val_mse: 199.8208\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 183.0738 - mse: 183.0738 - val_loss: 196.8851 - val_mse: 196.8851\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 179.1488 - mse: 179.1488 - val_loss: 198.0566 - val_mse: 198.0565\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 172.6271 - mse: 172.6272 - val_loss: 196.8569 - val_mse: 196.8569\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 162.8442 - mse: 162.8442 - val_loss: 198.5748 - val_mse: 198.5748\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 173.7215 - mse: 173.7216 - val_loss: 200.6093 - val_mse: 200.6093\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 175.0650 - mse: 175.0651 - val_loss: 200.8918 - val_mse: 200.8918\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 173.8249 - mse: 173.8248 - val_loss: 197.6428 - val_mse: 197.6428\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 166.8633 - mse: 166.8633 - val_loss: 198.0540 - val_mse: 198.0540\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 168.4701 - mse: 168.4701 - val_loss: 199.7553 - val_mse: 199.7552\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 175.5811 - mse: 175.5811 - val_loss: 196.2206 - val_mse: 196.2206\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 163.5879 - mse: 163.5879 - val_loss: 198.5397 - val_mse: 198.5397\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 163.9191 - mse: 163.9191 - val_loss: 200.0043 - val_mse: 200.0043\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 171.5657 - mse: 171.5658 - val_loss: 201.2844 - val_mse: 201.2844\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 160.9921 - mse: 160.9921 - val_loss: 195.8801 - val_mse: 195.8801\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 169.0011 - mse: 169.0011 - val_loss: 197.4834 - val_mse: 197.4834\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 161.3795 - mse: 161.3795 - val_loss: 197.3590 - val_mse: 197.3590\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 167.0259 - mse: 167.0259 - val_loss: 196.1365 - val_mse: 196.1364\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 162.7867 - mse: 162.7867 - val_loss: 198.4436 - val_mse: 198.4435\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 162.5918 - mse: 162.5918 - val_loss: 196.3292 - val_mse: 196.3292\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 174.4417 - mse: 174.4417 - val_loss: 195.6302 - val_mse: 195.6302\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 165.8205 - mse: 165.8205 - val_loss: 192.2196 - val_mse: 192.2196\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 161.6267 - mse: 161.6267 - val_loss: 192.4665 - val_mse: 192.4665\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 158.4423 - mse: 158.4422 - val_loss: 190.3426 - val_mse: 190.3426\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 165.8779 - mse: 165.8779 - val_loss: 189.6762 - val_mse: 189.6762\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 166.1203 - mse: 166.1203 - val_loss: 190.1961 - val_mse: 190.1961\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 167.8839 - mse: 167.8839 - val_loss: 188.8893 - val_mse: 188.8893\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 164.4547 - mse: 164.4547 - val_loss: 188.1257 - val_mse: 188.1257\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 163.5448 - mse: 163.5448 - val_loss: 184.8056 - val_mse: 184.8056\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 164.0725 - mse: 164.0725 - val_loss: 183.1402 - val_mse: 183.1402\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 166.1155 - mse: 166.1154 - val_loss: 183.7583 - val_mse: 183.7583\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 163.2317 - mse: 163.2317 - val_loss: 179.9280 - val_mse: 179.9279\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 160.5521 - mse: 160.5520 - val_loss: 183.5663 - val_mse: 183.5663\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 157.8063 - mse: 157.8063 - val_loss: 179.4804 - val_mse: 179.4804\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 167.3592 - mse: 167.3591 - val_loss: 180.8389 - val_mse: 180.8389\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 155.1482 - mse: 155.1482 - val_loss: 189.3141 - val_mse: 189.3141\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 154.1067 - mse: 154.1067 - val_loss: 183.4809 - val_mse: 183.4809\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 155.0930 - mse: 155.0930 - val_loss: 179.1213 - val_mse: 179.1212\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 156.2398 - mse: 156.2398 - val_loss: 174.7593 - val_mse: 174.7593\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 151.3296 - mse: 151.3296 - val_loss: 173.3538 - val_mse: 173.3538\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 153.1083 - mse: 153.1084 - val_loss: 180.2765 - val_mse: 180.2765\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 151.1118 - mse: 151.1118 - val_loss: 177.0258 - val_mse: 177.0258\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 154.2676 - mse: 154.2676 - val_loss: 179.7188 - val_mse: 179.7188\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 146.6348 - mse: 146.6348 - val_loss: 174.4444 - val_mse: 174.4444\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 158.4937 - mse: 158.4937 - val_loss: 174.4636 - val_mse: 174.4635\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 148.8203 - mse: 148.8203 - val_loss: 175.5331 - val_mse: 175.5331\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 155.3548 - mse: 155.3549 - val_loss: 174.0755 - val_mse: 174.0755\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 146.2557 - mse: 146.2557 - val_loss: 169.8642 - val_mse: 169.8643\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 153.7110 - mse: 153.7110 - val_loss: 170.7483 - val_mse: 170.7483\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 149.6169 - mse: 149.6169 - val_loss: 169.3271 - val_mse: 169.3272\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 146.9283 - mse: 146.9283 - val_loss: 168.1888 - val_mse: 168.1888\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 149.8342 - mse: 149.8342 - val_loss: 168.2617 - val_mse: 168.2617\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 151.0252 - mse: 151.0252 - val_loss: 173.4248 - val_mse: 173.4248\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 151.4092 - mse: 151.4092 - val_loss: 169.4399 - val_mse: 169.4400\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 146.9469 - mse: 146.9469 - val_loss: 171.5758 - val_mse: 171.5758\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 155.5126 - mse: 155.5126 - val_loss: 167.1922 - val_mse: 167.1921\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 134.8674 - mse: 134.8674 - val_loss: 172.3198 - val_mse: 172.3198\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 146.7991 - mse: 146.7991 - val_loss: 171.4541 - val_mse: 171.4541\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 140.0602 - mse: 140.0602 - val_loss: 178.3999 - val_mse: 178.3999\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 145.1848 - mse: 145.1847 - val_loss: 160.5218 - val_mse: 160.5218\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.0627 - mse: 141.0627 - val_loss: 176.9729 - val_mse: 176.9730\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 143.6134 - mse: 143.6134 - val_loss: 161.6310 - val_mse: 161.6310\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 147.6610 - mse: 147.6610 - val_loss: 173.0172 - val_mse: 173.0171\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 139.7725 - mse: 139.7725 - val_loss: 163.7138 - val_mse: 163.7138\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 141.3413 - mse: 141.3413 - val_loss: 157.9721 - val_mse: 157.9720\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 140.7067 - mse: 140.7067 - val_loss: 181.8582 - val_mse: 181.8582\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 133.1811 - mse: 133.1810 - val_loss: 159.1446 - val_mse: 159.1447\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 136.1748 - mse: 136.1747 - val_loss: 195.2402 - val_mse: 195.2402\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 141.0046 - mse: 141.0046 - val_loss: 174.1071 - val_mse: 174.1071\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 133.1340 - mse: 133.1340 - val_loss: 158.5886 - val_mse: 158.5885\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 134.4080 - mse: 134.4081 - val_loss: 166.8813 - val_mse: 166.8813\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 135.4460 - mse: 135.4459 - val_loss: 186.0830 - val_mse: 186.0830\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 132.3421 - mse: 132.3421 - val_loss: 163.3006 - val_mse: 163.3006\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 133.5451 - mse: 133.5451 - val_loss: 172.9426 - val_mse: 172.9426\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 131.3018 - mse: 131.3017 - val_loss: 158.3501 - val_mse: 158.3501\n",
      "[CV] ....................................... nl=1, nn=3, total= 1.2min\n",
      "[CV] nl=1, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 651us/sample - loss: 459.3827 - mse: 459.3826 - val_loss: 522.0618 - val_mse: 522.0616\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 407.2520 - mse: 407.2519 - val_loss: 459.0508 - val_mse: 459.0509\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 366.5500 - mse: 366.5500 - val_loss: 372.5165 - val_mse: 372.5165\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 323.5712 - mse: 323.5712 - val_loss: 316.2923 - val_mse: 316.2923\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 284.6828 - mse: 284.6828 - val_loss: 271.3089 - val_mse: 271.3089\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 255.6345 - mse: 255.6344 - val_loss: 235.7370 - val_mse: 235.7370\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 226.6558 - mse: 226.6558 - val_loss: 209.9022 - val_mse: 209.9022\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 208.7306 - mse: 208.7306 - val_loss: 173.9973 - val_mse: 173.9973\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 193.0643 - mse: 193.0643 - val_loss: 167.2382 - val_mse: 167.2382\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 188.2495 - mse: 188.2495 - val_loss: 155.3679 - val_mse: 155.3679\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 174.1953 - mse: 174.1953 - val_loss: 145.7493 - val_mse: 145.7493\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 170.6849 - mse: 170.6850 - val_loss: 142.0235 - val_mse: 142.0235\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 168.1913 - mse: 168.1912 - val_loss: 135.2312 - val_mse: 135.2312\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 172.9161 - mse: 172.9162 - val_loss: 137.8429 - val_mse: 137.8429\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 161.9259 - mse: 161.9259 - val_loss: 135.8386 - val_mse: 135.8385\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 165.0676 - mse: 165.0676 - val_loss: 131.0547 - val_mse: 131.0547\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 163.1764 - mse: 163.1766 - val_loss: 130.8567 - val_mse: 130.8567\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 159.3007 - mse: 159.3008 - val_loss: 131.5206 - val_mse: 131.5206\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 159.7551 - mse: 159.7550 - val_loss: 126.9966 - val_mse: 126.9966\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 158.6664 - mse: 158.6664 - val_loss: 128.8567 - val_mse: 128.8567\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 146.9605 - mse: 146.9604 - val_loss: 124.3065 - val_mse: 124.3065\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 155.6604 - mse: 155.6604 - val_loss: 123.5501 - val_mse: 123.5501\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 147.5090 - mse: 147.5090 - val_loss: 122.5151 - val_mse: 122.5151\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 157.0842 - mse: 157.0842 - val_loss: 125.3660 - val_mse: 125.3660\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 149.0883 - mse: 149.0884 - val_loss: 122.8333 - val_mse: 122.8333\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 152.7936 - mse: 152.7937 - val_loss: 121.4136 - val_mse: 121.4136\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 203us/sample - loss: 147.0499 - mse: 147.0499 - val_loss: 122.1197 - val_mse: 122.1197\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 154.7828 - mse: 154.7828 - val_loss: 120.7966 - val_mse: 120.7966\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 154.6585 - mse: 154.6585 - val_loss: 123.3012 - val_mse: 123.3013\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 149.9429 - mse: 149.9429 - val_loss: 122.8960 - val_mse: 122.8960\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 149.6221 - mse: 149.6221 - val_loss: 122.6420 - val_mse: 122.6420\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 149.9123 - mse: 149.9124 - val_loss: 121.4446 - val_mse: 121.4446\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 154.2035 - mse: 154.2035 - val_loss: 121.0271 - val_mse: 121.0271\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 148.3721 - mse: 148.3721 - val_loss: 120.6409 - val_mse: 120.6409\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 147.8388 - mse: 147.8388 - val_loss: 121.7986 - val_mse: 121.7986\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 142.3892 - mse: 142.3892 - val_loss: 119.0061 - val_mse: 119.0061\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 151.2705 - mse: 151.2705 - val_loss: 116.2219 - val_mse: 116.2219\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 145.4440 - mse: 145.4441 - val_loss: 114.6745 - val_mse: 114.6745\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 146.3758 - mse: 146.3758 - val_loss: 112.3985 - val_mse: 112.3984\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 139.5610 - mse: 139.5611 - val_loss: 115.1124 - val_mse: 115.1124\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 143.2235 - mse: 143.2235 - val_loss: 113.7748 - val_mse: 113.7747\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 143.1585 - mse: 143.1585 - val_loss: 114.7467 - val_mse: 114.7467\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 146.7007 - mse: 146.7007 - val_loss: 117.1465 - val_mse: 117.1465\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 143.9367 - mse: 143.9367 - val_loss: 113.2066 - val_mse: 113.2066\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 147.0997 - mse: 147.0997 - val_loss: 114.3485 - val_mse: 114.3485\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.2506 - mse: 141.2506 - val_loss: 115.2262 - val_mse: 115.2262\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 140.1760 - mse: 140.1760 - val_loss: 113.3500 - val_mse: 113.3500\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 137.8130 - mse: 137.8130 - val_loss: 116.2356 - val_mse: 116.2356\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 141.4891 - mse: 141.4891 - val_loss: 114.6331 - val_mse: 114.6331\n",
      "[CV] ....................................... nl=1, nn=3, total=  32.3s\n",
      "[CV] nl=1, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 704us/sample - loss: 486.7733 - mse: 486.7734 - val_loss: 643.6145 - val_mse: 643.6144\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 453.5729 - mse: 453.5727 - val_loss: 606.3099 - val_mse: 606.3098\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 418.5161 - mse: 418.5162 - val_loss: 551.4424 - val_mse: 551.4425\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 384.2310 - mse: 384.2309 - val_loss: 501.2017 - val_mse: 501.2017\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 349.4645 - mse: 349.4645 - val_loss: 459.0362 - val_mse: 459.0361\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 318.8449 - mse: 318.8449 - val_loss: 427.5732 - val_mse: 427.5732\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 293.4802 - mse: 293.4802 - val_loss: 391.1318 - val_mse: 391.1318\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 272.4284 - mse: 272.4283 - val_loss: 358.8228 - val_mse: 358.8228\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 254.5001 - mse: 254.5001 - val_loss: 333.4856 - val_mse: 333.4855\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 242.3674 - mse: 242.3673 - val_loss: 319.9059 - val_mse: 319.9059\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 233.8093 - mse: 233.8092 - val_loss: 312.5535 - val_mse: 312.5535\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 224.4194 - mse: 224.4194 - val_loss: 284.8754 - val_mse: 284.8754\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 219.3143 - mse: 219.3142 - val_loss: 277.0170 - val_mse: 277.0170\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 210.6355 - mse: 210.6355 - val_loss: 268.2183 - val_mse: 268.2183\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 206.2738 - mse: 206.2738 - val_loss: 260.0277 - val_mse: 260.0277\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 199.7255 - mse: 199.7254 - val_loss: 244.5891 - val_mse: 244.5890\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 193.2904 - mse: 193.2904 - val_loss: 247.2819 - val_mse: 247.2819\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 188.8085 - mse: 188.8084 - val_loss: 243.5040 - val_mse: 243.5041\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 188.3152 - mse: 188.3152 - val_loss: 231.3352 - val_mse: 231.3352\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 183.8239 - mse: 183.8239 - val_loss: 228.4939 - val_mse: 228.4938\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 185.1666 - mse: 185.1666 - val_loss: 218.9121 - val_mse: 218.9121\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 181.9404 - mse: 181.9405 - val_loss: 216.2845 - val_mse: 216.2845\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 172.4019 - mse: 172.4019 - val_loss: 221.6913 - val_mse: 221.6913\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 176.4590 - mse: 176.4589 - val_loss: 212.6351 - val_mse: 212.6351\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 174.8663 - mse: 174.8662 - val_loss: 216.3095 - val_mse: 216.3095\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 168.3460 - mse: 168.3460 - val_loss: 217.3075 - val_mse: 217.3075\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 179.4803 - mse: 179.4803 - val_loss: 208.9754 - val_mse: 208.9754\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 171.8483 - mse: 171.8483 - val_loss: 204.5371 - val_mse: 204.5371\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 173.0504 - mse: 173.0504 - val_loss: 199.9221 - val_mse: 199.9220\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 165.0718 - mse: 165.0718 - val_loss: 201.4860 - val_mse: 201.4860\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 163.9970 - mse: 163.9971 - val_loss: 198.0786 - val_mse: 198.0786\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 170.5939 - mse: 170.5938 - val_loss: 198.0411 - val_mse: 198.0412\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 173.2815 - mse: 173.2815 - val_loss: 201.1079 - val_mse: 201.1079\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 174.8107 - mse: 174.8107 - val_loss: 197.3811 - val_mse: 197.3811\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 167.3383 - mse: 167.3383 - val_loss: 199.9317 - val_mse: 199.9317\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 169.0780 - mse: 169.0780 - val_loss: 199.2662 - val_mse: 199.2662\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 165.7334 - mse: 165.7335 - val_loss: 197.6683 - val_mse: 197.6683\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 165.2082 - mse: 165.2082 - val_loss: 193.1095 - val_mse: 193.1095\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 167.3395 - mse: 167.3395 - val_loss: 193.8445 - val_mse: 193.8445\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 170.0472 - mse: 170.0472 - val_loss: 191.0664 - val_mse: 191.0664\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 160.2123 - mse: 160.2123 - val_loss: 198.5149 - val_mse: 198.5150\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 196us/sample - loss: 164.5425 - mse: 164.5425 - val_loss: 192.5176 - val_mse: 192.5176\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 167.8656 - mse: 167.8656 - val_loss: 190.6975 - val_mse: 190.6975\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 166.4455 - mse: 166.4455 - val_loss: 193.7021 - val_mse: 193.7021\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 166.7541 - mse: 166.7541 - val_loss: 195.8756 - val_mse: 195.8756\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 161.8116 - mse: 161.8115 - val_loss: 197.1475 - val_mse: 197.1474\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 156.9275 - mse: 156.9275 - val_loss: 191.2636 - val_mse: 191.2637\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 162.1788 - mse: 162.1788 - val_loss: 193.5311 - val_mse: 193.5312\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 161.2833 - mse: 161.2833 - val_loss: 190.0174 - val_mse: 190.0174\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 158.3126 - mse: 158.3126 - val_loss: 189.0941 - val_mse: 189.0941\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 160.1669 - mse: 160.1669 - val_loss: 189.2991 - val_mse: 189.2991\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 158.5223 - mse: 158.5223 - val_loss: 193.0690 - val_mse: 193.0690\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 155.3805 - mse: 155.3805 - val_loss: 189.1641 - val_mse: 189.1642\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 162.5902 - mse: 162.5901 - val_loss: 187.1574 - val_mse: 187.1575\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 151.5547 - mse: 151.5547 - val_loss: 187.8467 - val_mse: 187.8467\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 159.0345 - mse: 159.0345 - val_loss: 193.2612 - val_mse: 193.2612\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 150.5981 - mse: 150.5981 - val_loss: 187.4422 - val_mse: 187.4422\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 155.6755 - mse: 155.6756 - val_loss: 190.4330 - val_mse: 190.4330\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 149.4335 - mse: 149.4335 - val_loss: 190.4543 - val_mse: 190.4543\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 155.5190 - mse: 155.5190 - val_loss: 185.8382 - val_mse: 185.8382\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 162.1308 - mse: 162.1308 - val_loss: 187.7835 - val_mse: 187.7835\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 154.7742 - mse: 154.7742 - val_loss: 189.8185 - val_mse: 189.8185\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 149.1587 - mse: 149.1587 - val_loss: 185.3533 - val_mse: 185.3533\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 153.0893 - mse: 153.0893 - val_loss: 185.8141 - val_mse: 185.8141\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 148.6457 - mse: 148.6457 - val_loss: 185.0062 - val_mse: 185.0062\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 154.9620 - mse: 154.9620 - val_loss: 185.8017 - val_mse: 185.8017\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 152.0275 - mse: 152.0275 - val_loss: 186.7192 - val_mse: 186.7192\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 159.0017 - mse: 159.0017 - val_loss: 184.6490 - val_mse: 184.6490\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 151.8832 - mse: 151.8832 - val_loss: 184.0597 - val_mse: 184.0597\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 149.8093 - mse: 149.8092 - val_loss: 188.3723 - val_mse: 188.3723\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 157.7042 - mse: 157.7043 - val_loss: 183.3627 - val_mse: 183.3627\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 151.6367 - mse: 151.6367 - val_loss: 185.4122 - val_mse: 185.4122\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 149.2992 - mse: 149.2993 - val_loss: 182.3480 - val_mse: 182.3480\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 151.0765 - mse: 151.0764 - val_loss: 181.7896 - val_mse: 181.7896\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 147.7737 - mse: 147.7737 - val_loss: 184.3016 - val_mse: 184.3016\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 151.2122 - mse: 151.2122 - val_loss: 180.6913 - val_mse: 180.6913\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 148.3755 - mse: 148.3755 - val_loss: 180.5755 - val_mse: 180.5755\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 146.3371 - mse: 146.3371 - val_loss: 180.4207 - val_mse: 180.4207\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 155.0859 - mse: 155.0859 - val_loss: 184.5099 - val_mse: 184.5099\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 149.7596 - mse: 149.7596 - val_loss: 181.4468 - val_mse: 181.4468\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 153.9507 - mse: 153.9507 - val_loss: 186.7606 - val_mse: 186.7606\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 146.8113 - mse: 146.8113 - val_loss: 181.7166 - val_mse: 181.7166\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 150.8566 - mse: 150.8566 - val_loss: 185.1003 - val_mse: 185.1003\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 147.2764 - mse: 147.2764 - val_loss: 180.4940 - val_mse: 180.4940\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 147.8591 - mse: 147.8591 - val_loss: 178.9424 - val_mse: 178.9424\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 152.2151 - mse: 152.2150 - val_loss: 183.3146 - val_mse: 183.3146\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 214us/sample - loss: 150.6449 - mse: 150.6449 - val_loss: 182.4537 - val_mse: 182.4537\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 149.0293 - mse: 149.0293 - val_loss: 179.7176 - val_mse: 179.7176\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 1s 201us/sample - loss: 150.0077 - mse: 150.0077 - val_loss: 181.4790 - val_mse: 181.4790\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 153.6362 - mse: 153.6362 - val_loss: 180.4021 - val_mse: 180.4021\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 147.4702 - mse: 147.4702 - val_loss: 183.7033 - val_mse: 183.7032\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.2060 - mse: 141.2060 - val_loss: 180.4082 - val_mse: 180.4082\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 148.8769 - mse: 148.8769 - val_loss: 187.9037 - val_mse: 187.9037\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 149.9216 - mse: 149.9216 - val_loss: 178.8341 - val_mse: 178.8341\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.6501 - mse: 141.6501 - val_loss: 179.0141 - val_mse: 179.0141\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 148.0586 - mse: 148.0585 - val_loss: 178.1824 - val_mse: 178.1824\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 147.7388 - mse: 147.7388 - val_loss: 183.6350 - val_mse: 183.6350\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 145.4509 - mse: 145.4509 - val_loss: 178.6530 - val_mse: 178.6530\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 149.0079 - mse: 149.0079 - val_loss: 178.9460 - val_mse: 178.9459\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 148.1178 - mse: 148.1178 - val_loss: 178.9392 - val_mse: 178.9392\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 146.3636 - mse: 146.3636 - val_loss: 182.4185 - val_mse: 182.4186\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 142.5456 - mse: 142.5456 - val_loss: 179.4636 - val_mse: 179.4636\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 146.9156 - mse: 146.9156 - val_loss: 178.8066 - val_mse: 178.8065\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 148.0052 - mse: 148.0052 - val_loss: 183.7227 - val_mse: 183.7227\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 144.2544 - mse: 144.2544 - val_loss: 188.3169 - val_mse: 188.3169\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 146.5152 - mse: 146.5152 - val_loss: 177.5944 - val_mse: 177.5944\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 142.6527 - mse: 142.6527 - val_loss: 179.1850 - val_mse: 179.1850\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 1s 197us/sample - loss: 150.3445 - mse: 150.3445 - val_loss: 187.8664 - val_mse: 187.8663\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 145.6936 - mse: 145.6936 - val_loss: 183.1946 - val_mse: 183.1946\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 144.6820 - mse: 144.6820 - val_loss: 181.4404 - val_mse: 181.4404\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 150.1957 - mse: 150.1956 - val_loss: 177.5438 - val_mse: 177.5438\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 137.5939 - mse: 137.5939 - val_loss: 177.1631 - val_mse: 177.1631\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 146.7996 - mse: 146.7997 - val_loss: 178.4711 - val_mse: 178.4712\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 143.0296 - mse: 143.0296 - val_loss: 175.2455 - val_mse: 175.2455\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 144.0076 - mse: 144.0077 - val_loss: 174.1909 - val_mse: 174.1909\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 140.1441 - mse: 140.1442 - val_loss: 175.9367 - val_mse: 175.9368\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 145.7890 - mse: 145.7890 - val_loss: 174.4843 - val_mse: 174.4843\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 141.1467 - mse: 141.1467 - val_loss: 177.3671 - val_mse: 177.3671\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 143.4991 - mse: 143.4991 - val_loss: 171.2798 - val_mse: 171.2798\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 144.0111 - mse: 144.0110 - val_loss: 169.2762 - val_mse: 169.2763\n",
      "Epoch 121/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 141.0559 - mse: 141.0559 - val_loss: 176.3896 - val_mse: 176.3896\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 139.6004 - mse: 139.6004 - val_loss: 168.6443 - val_mse: 168.6443\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 138.7624 - mse: 138.7623 - val_loss: 180.4429 - val_mse: 180.4429\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 143.5716 - mse: 143.5715 - val_loss: 183.6262 - val_mse: 183.6263\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 147.1508 - mse: 147.1509 - val_loss: 170.0039 - val_mse: 170.0038\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 147.2177 - mse: 147.2177 - val_loss: 168.4455 - val_mse: 168.4455\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 143.1078 - mse: 143.1078 - val_loss: 171.0997 - val_mse: 171.0997\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 135.1177 - mse: 135.1176 - val_loss: 168.7066 - val_mse: 168.7066\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 138.3709 - mse: 138.3710 - val_loss: 168.8491 - val_mse: 168.8491\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 140.5458 - mse: 140.5458 - val_loss: 182.3286 - val_mse: 182.3286\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 132.6931 - mse: 132.6931 - val_loss: 186.1129 - val_mse: 186.1130\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 142.0934 - mse: 142.0934 - val_loss: 166.3571 - val_mse: 166.3571\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 137.4637 - mse: 137.4636 - val_loss: 195.9699 - val_mse: 195.9699\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 136.0157 - mse: 136.0157 - val_loss: 331.9430 - val_mse: 331.9431\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 138.9883 - mse: 138.9883 - val_loss: 208.3637 - val_mse: 208.3637\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 137.4854 - mse: 137.4854 - val_loss: 169.7508 - val_mse: 169.7508\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 140.8185 - mse: 140.8185 - val_loss: 189.0648 - val_mse: 189.0648\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 133.4158 - mse: 133.4158 - val_loss: 165.9600 - val_mse: 165.9600\n",
      "Epoch 139/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 137.9858 - mse: 137.9858 - val_loss: 198.6794 - val_mse: 198.6793\n",
      "Epoch 140/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 144.8571 - mse: 144.8571 - val_loss: 284.9413 - val_mse: 284.9413\n",
      "Epoch 141/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 142.0133 - mse: 142.0133 - val_loss: 185.9999 - val_mse: 185.9999\n",
      "Epoch 142/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 137.1028 - mse: 137.1028 - val_loss: 343.6713 - val_mse: 343.6714\n",
      "Epoch 143/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 139.5974 - mse: 139.5974 - val_loss: 173.4913 - val_mse: 173.4913\n",
      "Epoch 144/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 131.8174 - mse: 131.8174 - val_loss: 216.9826 - val_mse: 216.9826\n",
      "Epoch 145/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 140.2337 - mse: 140.2338 - val_loss: 168.3996 - val_mse: 168.3996\n",
      "Epoch 146/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 132.3458 - mse: 132.3457 - val_loss: 222.5829 - val_mse: 222.5830\n",
      "Epoch 147/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 136.1304 - mse: 136.1303 - val_loss: 173.8836 - val_mse: 173.8836\n",
      "Epoch 148/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 135.3166 - mse: 135.3166 - val_loss: 205.8162 - val_mse: 205.8162\n",
      "[CV] ....................................... nl=1, nn=3, total= 1.5min\n",
      "[CV] nl=1, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 632us/sample - loss: 516.1401 - mse: 516.1398 - val_loss: 663.5868 - val_mse: 663.5870\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 457.9421 - mse: 457.9421 - val_loss: 600.3262 - val_mse: 600.3262\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 390.9914 - mse: 390.9914 - val_loss: 487.7061 - val_mse: 487.7061\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 331.4023 - mse: 331.4023 - val_loss: 371.6324 - val_mse: 371.6324\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 281.8098 - mse: 281.8098 - val_loss: 322.8259 - val_mse: 322.8259\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 237.0046 - mse: 237.0046 - val_loss: 278.1857 - val_mse: 278.1858\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 213.8355 - mse: 213.8355 - val_loss: 257.2462 - val_mse: 257.2462\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 211.2199 - mse: 211.2199 - val_loss: 241.5952 - val_mse: 241.5952\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 196.0631 - mse: 196.0631 - val_loss: 229.6119 - val_mse: 229.6119\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 195.3283 - mse: 195.3284 - val_loss: 219.7690 - val_mse: 219.7690\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 193.8410 - mse: 193.8410 - val_loss: 217.5832 - val_mse: 217.5832\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 188.4347 - mse: 188.4347 - val_loss: 210.4352 - val_mse: 210.4353\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 182.9404 - mse: 182.9404 - val_loss: 208.0683 - val_mse: 208.0683\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 183.9428 - mse: 183.9428 - val_loss: 203.6429 - val_mse: 203.6430\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 174.8702 - mse: 174.8702 - val_loss: 205.1879 - val_mse: 205.1879\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 173.4955 - mse: 173.4955 - val_loss: 198.1765 - val_mse: 198.1766\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 169.6418 - mse: 169.6418 - val_loss: 203.2816 - val_mse: 203.2816\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 171.0685 - mse: 171.0685 - val_loss: 197.1102 - val_mse: 197.1102\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 177.4275 - mse: 177.4275 - val_loss: 199.1564 - val_mse: 199.1564\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 169.3126 - mse: 169.3126 - val_loss: 196.1261 - val_mse: 196.1261\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 157.9237 - mse: 157.9237 - val_loss: 197.3502 - val_mse: 197.3502\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 163.8615 - mse: 163.8614 - val_loss: 202.1682 - val_mse: 202.1682\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 166.1506 - mse: 166.1506 - val_loss: 205.4454 - val_mse: 205.4454\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 160.8157 - mse: 160.8157 - val_loss: 195.7971 - val_mse: 195.7971\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 155.9576 - mse: 155.9577 - val_loss: 202.1538 - val_mse: 202.1539\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 158.6036 - mse: 158.6036 - val_loss: 202.3189 - val_mse: 202.3189\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 152.9556 - mse: 152.9556 - val_loss: 196.8165 - val_mse: 196.8166\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 150.4338 - mse: 150.4338 - val_loss: 201.8960 - val_mse: 201.8960\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 147.6785 - mse: 147.6785 - val_loss: 199.6738 - val_mse: 199.6738\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 151.7369 - mse: 151.7369 - val_loss: 204.2560 - val_mse: 204.2560\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 149.6759 - mse: 149.6759 - val_loss: 196.9059 - val_mse: 196.9059\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 150.7266 - mse: 150.7266 - val_loss: 193.7102 - val_mse: 193.7102\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 146.1237 - mse: 146.1237 - val_loss: 198.6995 - val_mse: 198.6995\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 146.8854 - mse: 146.8854 - val_loss: 195.2382 - val_mse: 195.2382\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 212us/sample - loss: 145.8432 - mse: 145.8432 - val_loss: 195.3862 - val_mse: 195.3862\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 160.8120 - mse: 160.8120 - val_loss: 188.7488 - val_mse: 188.7488\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 143.0520 - mse: 143.0521 - val_loss: 184.3743 - val_mse: 184.3743\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 152.2720 - mse: 152.2720 - val_loss: 182.7744 - val_mse: 182.7744\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 145.7073 - mse: 145.7073 - val_loss: 178.9154 - val_mse: 178.9154\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 143.9848 - mse: 143.9848 - val_loss: 189.3876 - val_mse: 189.3875\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 145.8838 - mse: 145.8839 - val_loss: 180.4090 - val_mse: 180.4090\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 143.9841 - mse: 143.9840 - val_loss: 191.8391 - val_mse: 191.8391\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 139.5937 - mse: 139.5937 - val_loss: 181.6484 - val_mse: 181.6484\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 134.1449 - mse: 134.1449 - val_loss: 191.0686 - val_mse: 191.0686\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 135.6024 - mse: 135.6023 - val_loss: 187.2130 - val_mse: 187.2130\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 137.9195 - mse: 137.9196 - val_loss: 179.6631 - val_mse: 179.6632\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 132.3820 - mse: 132.3820 - val_loss: 172.4536 - val_mse: 172.4536\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 230us/sample - loss: 136.3522 - mse: 136.3521 - val_loss: 168.5712 - val_mse: 168.5712\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 129.7957 - mse: 129.7957 - val_loss: 164.4416 - val_mse: 164.4417\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 130.3473 - mse: 130.3473 - val_loss: 157.7497 - val_mse: 157.7497\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 135.4541 - mse: 135.4541 - val_loss: 168.1242 - val_mse: 168.1242\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 125.3859 - mse: 125.3859 - val_loss: 169.0552 - val_mse: 169.0552\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 127.9403 - mse: 127.9403 - val_loss: 172.2029 - val_mse: 172.2029\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 118.1815 - mse: 118.1815 - val_loss: 159.9339 - val_mse: 159.9339\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 120.7547 - mse: 120.7547 - val_loss: 155.4768 - val_mse: 155.4769\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 128.3964 - mse: 128.3964 - val_loss: 169.4786 - val_mse: 169.4785\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 118.5952 - mse: 118.5952 - val_loss: 157.1285 - val_mse: 157.1285\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 113.7082 - mse: 113.7082 - val_loss: 161.0206 - val_mse: 161.0206\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 202us/sample - loss: 116.2286 - mse: 116.2286 - val_loss: 159.3940 - val_mse: 159.3941\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 112.5079 - mse: 112.5079 - val_loss: 175.1578 - val_mse: 175.1578\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 199us/sample - loss: 119.3397 - mse: 119.3397 - val_loss: 151.0779 - val_mse: 151.0779\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 112.4832 - mse: 112.4832 - val_loss: 167.0346 - val_mse: 167.0346\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 116.0326 - mse: 116.0326 - val_loss: 167.4063 - val_mse: 167.4062\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 109.9941 - mse: 109.9941 - val_loss: 166.3346 - val_mse: 166.3346\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 112.9195 - mse: 112.9194 - val_loss: 171.4597 - val_mse: 171.4597\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 107.2361 - mse: 107.2361 - val_loss: 164.7837 - val_mse: 164.7838\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 104.0087 - mse: 104.0086 - val_loss: 161.7078 - val_mse: 161.7078\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 102.4072 - mse: 102.4073 - val_loss: 174.4540 - val_mse: 174.4539\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 115.7992 - mse: 115.7992 - val_loss: 165.5758 - val_mse: 165.5758\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 106.2098 - mse: 106.2098 - val_loss: 162.6433 - val_mse: 162.6433\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 111.7702 - mse: 111.7702 - val_loss: 167.8537 - val_mse: 167.8537\n",
      "[CV] ....................................... nl=1, nn=6, total=  45.1s\n",
      "[CV] nl=1, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 790us/sample - loss: 463.7427 - mse: 463.7428 - val_loss: 664.1133 - val_mse: 664.1134\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 411.8737 - mse: 411.8737 - val_loss: 589.6087 - val_mse: 589.6088\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 351.9566 - mse: 351.9568 - val_loss: 486.0620 - val_mse: 486.0620\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 296.0059 - mse: 296.0060 - val_loss: 381.8740 - val_mse: 381.8740\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 242.5632 - mse: 242.5632 - val_loss: 359.0899 - val_mse: 359.0899\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 207.4438 - mse: 207.4438 - val_loss: 295.6380 - val_mse: 295.6380\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 189.8790 - mse: 189.8790 - val_loss: 261.7535 - val_mse: 261.7535\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 181.5782 - mse: 181.5782 - val_loss: 250.0872 - val_mse: 250.0871\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 180.0865 - mse: 180.0865 - val_loss: 257.1352 - val_mse: 257.1352\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 171.1357 - mse: 171.1357 - val_loss: 231.8945 - val_mse: 231.8946\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 165.4390 - mse: 165.4390 - val_loss: 234.4845 - val_mse: 234.4845\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 170.2787 - mse: 170.2788 - val_loss: 229.9800 - val_mse: 229.9800\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 156.3761 - mse: 156.376 - 1s 224us/sample - loss: 159.7858 - mse: 159.7858 - val_loss: 231.8424 - val_mse: 231.8424\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 158.4118 - mse: 158.4119 - val_loss: 213.9381 - val_mse: 213.9381\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 162.3959 - mse: 162.3960 - val_loss: 210.6407 - val_mse: 210.6407\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 164.9229 - mse: 164.922 - 1s 213us/sample - loss: 160.9085 - mse: 160.9085 - val_loss: 209.6571 - val_mse: 209.6571\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 154.5339 - mse: 154.5339 - val_loss: 212.7140 - val_mse: 212.7140\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 153.6195 - mse: 153.6195 - val_loss: 210.0861 - val_mse: 210.0861\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 158.9731 - mse: 158.9731 - val_loss: 202.5155 - val_mse: 202.5154\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 152.3401 - mse: 152.3401 - val_loss: 213.4108 - val_mse: 213.4109\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 154.9053 - mse: 154.9053 - val_loss: 197.2162 - val_mse: 197.2162\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 151.9817 - mse: 151.9817 - val_loss: 193.3193 - val_mse: 193.3192\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 151.0311 - mse: 151.0311 - val_loss: 201.6167 - val_mse: 201.6167\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 146.9596 - mse: 146.9596 - val_loss: 189.1537 - val_mse: 189.1537\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 144.5737 - mse: 144.5737 - val_loss: 191.8062 - val_mse: 191.8062\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 156.9467 - mse: 156.9467 - val_loss: 194.4016 - val_mse: 194.4016\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 144.2433 - mse: 144.2433 - val_loss: 197.4466 - val_mse: 197.4466\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 150.8588 - mse: 150.8588 - val_loss: 195.6312 - val_mse: 195.6312\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 153.4540 - mse: 153.4540 - val_loss: 193.5528 - val_mse: 193.5528\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 143.8503 - mse: 143.8503 - val_loss: 187.8753 - val_mse: 187.8754\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 149.5976 - mse: 149.5976 - val_loss: 193.5379 - val_mse: 193.5379\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 144.6133 - mse: 144.6134 - val_loss: 193.4778 - val_mse: 193.4778\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 141.5590 - mse: 141.5590 - val_loss: 191.1419 - val_mse: 191.1420\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 140.0313 - mse: 140.0314 - val_loss: 189.7779 - val_mse: 189.7779\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 144.8556 - mse: 144.8556 - val_loss: 191.8592 - val_mse: 191.8592\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 147.5713 - mse: 147.5713 - val_loss: 186.1466 - val_mse: 186.1466\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 150.5956 - mse: 150.5956 - val_loss: 189.5873 - val_mse: 189.5873\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 140.2333 - mse: 140.2333 - val_loss: 186.3436 - val_mse: 186.3436\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 140.4134 - mse: 140.4134 - val_loss: 191.6602 - val_mse: 191.6602\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 148.0290 - mse: 148.0291 - val_loss: 188.7447 - val_mse: 188.7446\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 142.6334 - mse: 142.6333 - val_loss: 187.4623 - val_mse: 187.4623\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 139.5915 - mse: 139.5915 - val_loss: 193.0100 - val_mse: 193.0100\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 267us/sample - loss: 144.6423 - mse: 144.6424 - val_loss: 198.6522 - val_mse: 198.6523\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 143.9918 - mse: 143.9917 - val_loss: 190.1019 - val_mse: 190.1019\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 145.0577 - mse: 145.0577 - val_loss: 191.7518 - val_mse: 191.7519\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 139.9146 - mse: 139.9146 - val_loss: 193.7776 - val_mse: 193.7776\n",
      "[CV] ....................................... nl=1, nn=6, total=  33.6s\n",
      "[CV] nl=1, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 755us/sample - loss: 529.6679 - mse: 529.6678 - val_loss: 664.6567 - val_mse: 664.6567\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 488.0207 - mse: 488.0206 - val_loss: 598.1152 - val_mse: 598.1152\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 430.6033 - mse: 430.6033 - val_loss: 522.6429 - val_mse: 522.6428\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 374.6204 - mse: 374.6205 - val_loss: 440.4407 - val_mse: 440.4406\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 327.6202 - mse: 327.6203 - val_loss: 379.8037 - val_mse: 379.8038\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 288.0323 - mse: 288.0323 - val_loss: 337.2053 - val_mse: 337.2054\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 267.3347 - mse: 267.3347 - val_loss: 315.2160 - val_mse: 315.2160\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 251.7811 - mse: 251.7810 - val_loss: 310.1324 - val_mse: 310.1324\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 240.9248 - mse: 240.9248 - val_loss: 282.1012 - val_mse: 282.1012\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 232.2138 - mse: 232.2138 - val_loss: 270.3334 - val_mse: 270.3334\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 223.7487 - mse: 223.7487 - val_loss: 271.5107 - val_mse: 271.5107\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 223.1331 - mse: 223.1331 - val_loss: 262.3852 - val_mse: 262.3853\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 224.4573 - mse: 224.4572 - val_loss: 262.9856 - val_mse: 262.9856\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 207.6988 - mse: 207.6989 - val_loss: 247.7252 - val_mse: 247.7252\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 209.9613 - mse: 209.9613 - val_loss: 238.6326 - val_mse: 238.6326\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 205.9873 - mse: 205.9873 - val_loss: 240.6935 - val_mse: 240.6936\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 205.9237 - mse: 205.9237 - val_loss: 240.0207 - val_mse: 240.0207\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 199.6290 - mse: 199.6291 - val_loss: 226.7722 - val_mse: 226.7722\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 243us/sample - loss: 197.8954 - mse: 197.8954 - val_loss: 219.9453 - val_mse: 219.9453\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 189.2082 - mse: 189.2082 - val_loss: 217.5235 - val_mse: 217.5235\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 190.9470 - mse: 190.9470 - val_loss: 214.4793 - val_mse: 214.4793\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 194.6599 - mse: 194.6599 - val_loss: 215.6934 - val_mse: 215.6934\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 195.5520 - mse: 195.5520 - val_loss: 223.6729 - val_mse: 223.6729\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 184.5943 - mse: 184.5943 - val_loss: 207.6385 - val_mse: 207.6386\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 185.5931 - mse: 185.5931 - val_loss: 206.4388 - val_mse: 206.4388\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 184.2027 - mse: 184.2027 - val_loss: 204.2924 - val_mse: 204.2924\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 173.9818 - mse: 173.981 - 1s 218us/sample - loss: 177.2575 - mse: 177.2574 - val_loss: 228.0633 - val_mse: 228.0633\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 173.5431 - mse: 173.5431 - val_loss: 211.8697 - val_mse: 211.8697\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 184.7318 - mse: 184.7318 - val_loss: 200.9436 - val_mse: 200.9437\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 177.9764 - mse: 177.9764 - val_loss: 206.9953 - val_mse: 206.9953\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 180.9972 - mse: 180.9972 - val_loss: 200.7259 - val_mse: 200.7260\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 179.5770 - mse: 179.5770 - val_loss: 228.5681 - val_mse: 228.5681\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 176.6403 - mse: 176.6403 - val_loss: 197.6847 - val_mse: 197.6847\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 265us/sample - loss: 173.4360 - mse: 173.4360 - val_loss: 211.3909 - val_mse: 211.3909\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 170.3209 - mse: 170.3209 - val_loss: 202.0006 - val_mse: 202.0006\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 166.8661 - mse: 166.8661 - val_loss: 198.2253 - val_mse: 198.2253\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 182.1999 - mse: 182.1999 - val_loss: 198.9028 - val_mse: 198.9028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 169.1769 - mse: 169.1769 - val_loss: 189.4527 - val_mse: 189.4527\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 170.6960 - mse: 170.6960 - val_loss: 194.4383 - val_mse: 194.4383\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 204us/sample - loss: 163.3790 - mse: 163.3790 - val_loss: 196.0067 - val_mse: 196.0067\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 174.6772 - mse: 174.6772 - val_loss: 194.2180 - val_mse: 194.2180\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 170.5188 - mse: 170.5187 - val_loss: 192.9603 - val_mse: 192.9604\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 173.2863 - mse: 173.2863 - val_loss: 187.9682 - val_mse: 187.9682\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 171.5290 - mse: 171.5290 - val_loss: 190.9426 - val_mse: 190.9426\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 171.5551 - mse: 171.5551 - val_loss: 196.6754 - val_mse: 196.6754\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 174.4544 - mse: 174.4544 - val_loss: 193.5278 - val_mse: 193.5278\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 168.0652 - mse: 168.0653 - val_loss: 186.8906 - val_mse: 186.8906\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 159.8475 - mse: 159.8475 - val_loss: 196.0895 - val_mse: 196.0895\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 171.4719 - mse: 171.4719 - val_loss: 244.0732 - val_mse: 244.0731\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 170.8264 - mse: 170.8264 - val_loss: 204.7399 - val_mse: 204.7399\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 162.2512 - mse: 162.2512 - val_loss: 205.5155 - val_mse: 205.5155\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 164.2924 - mse: 164.2924 - val_loss: 200.8828 - val_mse: 200.8828\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 268us/sample - loss: 161.0284 - mse: 161.0284 - val_loss: 189.7684 - val_mse: 189.7684\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 164.6732 - mse: 164.6732 - val_loss: 191.8884 - val_mse: 191.8884\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 175.7569 - mse: 175.7569 - val_loss: 192.2995 - val_mse: 192.2995\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 164.4474 - mse: 164.4474 - val_loss: 193.7510 - val_mse: 193.7510\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 168.5303 - mse: 168.5303 - val_loss: 189.2830 - val_mse: 189.2831\n",
      "[CV] ....................................... nl=1, nn=6, total=  40.6s\n",
      "[CV] nl=1, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 710us/sample - loss: 461.1118 - mse: 461.1119 - val_loss: 526.8892 - val_mse: 526.8890\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 411.0220 - mse: 411.0221 - val_loss: 469.0009 - val_mse: 469.0009\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 295us/sample - loss: 356.3901 - mse: 356.3900 - val_loss: 370.4335 - val_mse: 370.4335\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 283us/sample - loss: 302.3276 - mse: 302.3276 - val_loss: 290.7746 - val_mse: 290.7746\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 299us/sample - loss: 258.1607 - mse: 258.1606 - val_loss: 240.2329 - val_mse: 240.2329\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 219.9877 - mse: 219.9876 - val_loss: 200.4619 - val_mse: 200.4619\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 201.6801 - mse: 201.6801 - val_loss: 177.4563 - val_mse: 177.4564\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 186.4610 - mse: 186.4610 - val_loss: 171.0535 - val_mse: 171.0535\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 178.8246 - mse: 178.8246 - val_loss: 154.5908 - val_mse: 154.5908\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 171.9656 - mse: 171.9656 - val_loss: 147.3178 - val_mse: 147.3178\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 173.8174 - mse: 173.8174 - val_loss: 145.5000 - val_mse: 145.5000\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 169.5428 - mse: 169.5428 - val_loss: 147.9928 - val_mse: 147.9928\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 167.7386 - mse: 167.7386 - val_loss: 139.1259 - val_mse: 139.1259\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 163.5537 - mse: 163.5538 - val_loss: 139.3442 - val_mse: 139.3442\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 275us/sample - loss: 170.4925 - mse: 170.4925 - val_loss: 142.7203 - val_mse: 142.7203\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 160.0844 - mse: 160.0843 - val_loss: 139.2867 - val_mse: 139.2867\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 155.7847 - mse: 155.7847 - val_loss: 134.6777 - val_mse: 134.6777\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 157.1292 - mse: 157.1292 - val_loss: 139.9467 - val_mse: 139.9467\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 153.0819 - mse: 153.0819 - val_loss: 137.9549 - val_mse: 137.9548\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 157.2412 - mse: 157.2412 - val_loss: 132.4668 - val_mse: 132.4668\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 157.7701 - mse: 157.7701 - val_loss: 134.9487 - val_mse: 134.9486\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 148.8570 - mse: 148.8571 - val_loss: 140.0139 - val_mse: 140.0139\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 154.9224 - mse: 154.9224 - val_loss: 134.7272 - val_mse: 134.7273\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 154.9423 - mse: 154.9423 - val_loss: 138.0085 - val_mse: 138.0085\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 297us/sample - loss: 152.0253 - mse: 152.0253 - val_loss: 135.1921 - val_mse: 135.1921\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 325us/sample - loss: 149.1200 - mse: 149.1200 - val_loss: 136.4729 - val_mse: 136.4729\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 304us/sample - loss: 152.9228 - mse: 152.9228 - val_loss: 134.5327 - val_mse: 134.5327\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 145.6325 - mse: 145.6325 - val_loss: 137.8190 - val_mse: 137.8190\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 153.0021 - mse: 153.0022 - val_loss: 138.6614 - val_mse: 138.6615\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 145.3433 - mse: 145.3433 - val_loss: 135.5770 - val_mse: 135.5770\n",
      "[CV] ....................................... nl=1, nn=6, total=  24.1s\n",
      "[CV] nl=1, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 3s 877us/sample - loss: 452.2135 - mse: 452.2134 - val_loss: 619.8951 - val_mse: 619.8951\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 397.5832 - mse: 397.5833 - val_loss: 548.2699 - val_mse: 548.2698\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 233us/sample - loss: 343.8759 - mse: 343.8757 - val_loss: 452.6602 - val_mse: 452.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 299.1004 - mse: 299.1004 - val_loss: 370.7527 - val_mse: 370.7527\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 264.3573 - mse: 264.3573 - val_loss: 324.4354 - val_mse: 324.4354\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 230.1920 - mse: 230.1920 - val_loss: 308.4808 - val_mse: 308.4807\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 262us/sample - loss: 207.5205 - mse: 207.5204 - val_loss: 280.8323 - val_mse: 280.8323\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 203.3544 - mse: 203.3544 - val_loss: 266.8496 - val_mse: 266.8495\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 194.8406 - mse: 194.8406 - val_loss: 260.8753 - val_mse: 260.8753\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 191.9242 - mse: 191.9242 - val_loss: 259.1809 - val_mse: 259.1809\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 186.8062 - mse: 186.8062 - val_loss: 249.7080 - val_mse: 249.7080\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 188.9700 - mse: 188.9700 - val_loss: 253.3401 - val_mse: 253.3401\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 184.9212 - mse: 184.9211 - val_loss: 241.4671 - val_mse: 241.4671\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 178.4900 - mse: 178.4900 - val_loss: 238.2451 - val_mse: 238.2452\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 173.5590 - mse: 173.5590 - val_loss: 242.5798 - val_mse: 242.5798\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 176.8517 - mse: 176.8517 - val_loss: 233.9614 - val_mse: 233.9614\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 168.9041 - mse: 168.9041 - val_loss: 223.8723 - val_mse: 223.8723\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 172.7731 - mse: 172.7731 - val_loss: 222.0297 - val_mse: 222.0297\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 170.7759 - mse: 170.7759 - val_loss: 224.0236 - val_mse: 224.0236\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 167.8061 - mse: 167.8062 - val_loss: 218.4453 - val_mse: 218.4453\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 301us/sample - loss: 166.5743 - mse: 166.5742 - val_loss: 224.3559 - val_mse: 224.3560\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 170.6534 - mse: 170.6533 - val_loss: 216.4425 - val_mse: 216.4424\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 167.2954 - mse: 167.2954 - val_loss: 215.7020 - val_mse: 215.7020\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 167.3637 - mse: 167.3637 - val_loss: 235.4466 - val_mse: 235.4466\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 167.1506 - mse: 167.1506 - val_loss: 218.0356 - val_mse: 218.0356\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 163.0834 - mse: 163.0834 - val_loss: 208.6781 - val_mse: 208.6781\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 165.8570 - mse: 165.8570 - val_loss: 204.4138 - val_mse: 204.4138\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 159.1678 - mse: 159.1678 - val_loss: 213.4050 - val_mse: 213.4050\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 165.0832 - mse: 165.0832 - val_loss: 202.5493 - val_mse: 202.5493\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 160.0803 - mse: 160.0803 - val_loss: 203.2268 - val_mse: 203.2267\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 156.5613 - mse: 156.5613 - val_loss: 202.5987 - val_mse: 202.5988\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 152.9943 - mse: 152.9943 - val_loss: 205.1523 - val_mse: 205.1523\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 160.4523 - mse: 160.4523 - val_loss: 199.8651 - val_mse: 199.8651\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 160.1546 - mse: 160.1546 - val_loss: 209.9474 - val_mse: 209.9474\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 165.6013 - mse: 165.6014 - val_loss: 197.5277 - val_mse: 197.5277\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 156.6913 - mse: 156.6913 - val_loss: 210.9104 - val_mse: 210.9105\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 154.4541 - mse: 154.4541 - val_loss: 205.1322 - val_mse: 205.1322\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 157.1955 - mse: 157.1955 - val_loss: 197.6598 - val_mse: 197.6598\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 160.4074 - mse: 160.4074 - val_loss: 195.7173 - val_mse: 195.7172\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 155.4505 - mse: 155.4505 - val_loss: 195.7924 - val_mse: 195.7923\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 156.6515 - mse: 156.6515 - val_loss: 199.5355 - val_mse: 199.5355\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 152.8548 - mse: 152.8548 - val_loss: 195.1796 - val_mse: 195.1796\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 153.6049 - mse: 153.6049 - val_loss: 203.1808 - val_mse: 203.1808\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 166.0246 - mse: 166.0246 - val_loss: 201.1878 - val_mse: 201.1879\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 272us/sample - loss: 155.8412 - mse: 155.8412 - val_loss: 204.1804 - val_mse: 204.1804\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 287us/sample - loss: 154.3989 - mse: 154.3989 - val_loss: 199.6367 - val_mse: 199.6367\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 155.1376 - mse: 155.1376 - val_loss: 196.1479 - val_mse: 196.1480\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 159.4953 - mse: 159.4952 - val_loss: 195.6402 - val_mse: 195.6401\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 153.9747 - mse: 153.9746 - val_loss: 196.7179 - val_mse: 196.7180\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 157.0343 - mse: 157.0343 - val_loss: 203.2382 - val_mse: 203.2383\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 159.4281 - mse: 159.4281 - val_loss: 197.5050 - val_mse: 197.5050\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 297us/sample - loss: 150.1784 - mse: 150.1784 - val_loss: 203.5908 - val_mse: 203.5909\n",
      "[CV] ....................................... nl=1, nn=6, total=  39.4s\n",
      "[CV] nl=1, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 612us/sample - loss: 520.5035 - mse: 520.5035 - val_loss: 649.0811 - val_mse: 649.0812\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 425.4233 - mse: 425.4234 - val_loss: 560.8247 - val_mse: 560.8247\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 343.7951 - mse: 343.7950 - val_loss: 407.3086 - val_mse: 407.3086\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 274.0469 - mse: 274.0468 - val_loss: 317.1816 - val_mse: 317.1817\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 231.8071 - mse: 231.8072 - val_loss: 255.6361 - val_mse: 255.6361\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 207.7293 - mse: 207.7292 - val_loss: 242.6069 - val_mse: 242.6069\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 193.9002 - mse: 193.9001 - val_loss: 219.3624 - val_mse: 219.3624\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 206us/sample - loss: 185.2465 - mse: 185.2465 - val_loss: 203.3325 - val_mse: 203.3325\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 190.2830 - mse: 190.2829 - val_loss: 205.0639 - val_mse: 205.0639\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 190.7934 - mse: 190.7934 - val_loss: 202.8510 - val_mse: 202.8510\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 205us/sample - loss: 185.0003 - mse: 185.0004 - val_loss: 206.4795 - val_mse: 206.4795\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 172.7933 - mse: 172.7934 - val_loss: 188.9861 - val_mse: 188.9861\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 171.1625 - mse: 171.1625 - val_loss: 181.7503 - val_mse: 181.7503\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 170.4602 - mse: 170.4601 - val_loss: 186.2714 - val_mse: 186.2714\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 209us/sample - loss: 169.9772 - mse: 169.9772 - val_loss: 180.1283 - val_mse: 180.1283\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 166.8605 - mse: 166.8605 - val_loss: 175.1032 - val_mse: 175.1031\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 207us/sample - loss: 160.8758 - mse: 160.8758 - val_loss: 186.6341 - val_mse: 186.6340\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 159.8506 - mse: 159.8506 - val_loss: 173.2545 - val_mse: 173.2545\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 157.8935 - mse: 157.8935 - val_loss: 182.0269 - val_mse: 182.0269\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 157.6837 - mse: 157.6838 - val_loss: 174.3412 - val_mse: 174.3413\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 155.5047 - mse: 155.5047 - val_loss: 168.7955 - val_mse: 168.7954\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 159.5267 - mse: 159.5267 - val_loss: 176.0914 - val_mse: 176.0914\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 229us/sample - loss: 153.5756 - mse: 153.5756 - val_loss: 167.7162 - val_mse: 167.7162\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 145.8366 - mse: 145.8365 - val_loss: 171.2277 - val_mse: 171.2277\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 150.1770 - mse: 150.1770 - val_loss: 178.5860 - val_mse: 178.5860\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 143.0850 - mse: 143.0851 - val_loss: 169.7045 - val_mse: 169.7045\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 147.1545 - mse: 147.1545 - val_loss: 179.9663 - val_mse: 179.9662\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 284us/sample - loss: 146.5480 - mse: 146.5479 - val_loss: 178.3192 - val_mse: 178.3192\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 146.7180 - mse: 146.7180 - val_loss: 179.4686 - val_mse: 179.4686\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 144.4181 - mse: 144.4180 - val_loss: 182.8218 - val_mse: 182.8219\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 141.7839 - mse: 141.7839 - val_loss: 181.0297 - val_mse: 181.0297\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 138.2398 - mse: 138.2398 - val_loss: 178.0613 - val_mse: 178.0613\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 138.1615 - mse: 138.1616 - val_loss: 184.5304 - val_mse: 184.5304\n",
      "[CV] ...................................... nl=1, nn=12, total=  23.8s\n",
      "[CV] nl=1, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 748us/sample - loss: 457.9043 - mse: 457.9043 - val_loss: 655.7639 - val_mse: 655.7637\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 382.7934 - mse: 382.7934 - val_loss: 576.0138 - val_mse: 576.0138\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 305.2401 - mse: 305.2401 - val_loss: 433.3079 - val_mse: 433.3079\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 235.2657 - mse: 235.2657 - val_loss: 318.1647 - val_mse: 318.1647\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 186.7421 - mse: 186.7421 - val_loss: 243.6923 - val_mse: 243.6923\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 245us/sample - loss: 166.5585 - mse: 166.5585 - val_loss: 219.4368 - val_mse: 219.4368\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 253us/sample - loss: 153.3926 - mse: 153.3926 - val_loss: 205.0365 - val_mse: 205.0365\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 150.5469 - mse: 150.5469 - val_loss: 203.0249 - val_mse: 203.0249\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 151.3126 - mse: 151.3126 - val_loss: 193.2192 - val_mse: 193.2192\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 223us/sample - loss: 148.1572 - mse: 148.1572 - val_loss: 198.5107 - val_mse: 198.5107\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 239us/sample - loss: 149.1854 - mse: 149.1854 - val_loss: 189.9919 - val_mse: 189.9920\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 258us/sample - loss: 141.5821 - mse: 141.5821 - val_loss: 188.0968 - val_mse: 188.0968\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 138.6013 - mse: 138.6013 - val_loss: 187.2495 - val_mse: 187.2495\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 140.6494 - mse: 140.6494 - val_loss: 187.6327 - val_mse: 187.6327\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 139.5940 - mse: 139.5940 - val_loss: 185.8667 - val_mse: 185.8667\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 137.3618 - mse: 137.3619 - val_loss: 183.5501 - val_mse: 183.5501\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 288us/sample - loss: 138.4873 - mse: 138.4874 - val_loss: 191.8937 - val_mse: 191.8937\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 141.1415 - mse: 141.1416 - val_loss: 187.5675 - val_mse: 187.5675\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 134.7822 - mse: 134.7822 - val_loss: 194.2970 - val_mse: 194.2970\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 264us/sample - loss: 139.4324 - mse: 139.4324 - val_loss: 185.7231 - val_mse: 185.7231\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 144.2540 - mse: 144.2540 - val_loss: 188.2407 - val_mse: 188.2408\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 131.1062 - mse: 131.1062 - val_loss: 193.0700 - val_mse: 193.0699\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 282us/sample - loss: 138.8995 - mse: 138.8995 - val_loss: 191.4865 - val_mse: 191.4866\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 273us/sample - loss: 133.3756 - mse: 133.3756 - val_loss: 187.0262 - val_mse: 187.0262\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 134.8529 - mse: 134.8529 - val_loss: 192.6624 - val_mse: 192.6624\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 133.0809 - mse: 133.0808 - val_loss: 188.2209 - val_mse: 188.2209\n",
      "[CV] ...................................... nl=1, nn=12, total=  21.4s\n",
      "[CV] nl=1, nn=12 .....................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 745us/sample - loss: 509.4923 - mse: 509.4923 - val_loss: 648.0629 - val_mse: 648.0627\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 254us/sample - loss: 430.6897 - mse: 430.6897 - val_loss: 562.5342 - val_mse: 562.5342\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 350.2771 - mse: 350.2772 - val_loss: 443.8980 - val_mse: 443.8979\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 286us/sample - loss: 295.3781 - mse: 295.3781 - val_loss: 341.1871 - val_mse: 341.1870\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 293us/sample - loss: 256.7709 - mse: 256.7709 - val_loss: 304.1479 - val_mse: 304.1479\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 237.1484 - mse: 237.1483 - val_loss: 283.1019 - val_mse: 283.1018\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 298us/sample - loss: 236.8345 - mse: 236.8345 - val_loss: 269.0555 - val_mse: 269.0555\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 223.7539 - mse: 223.7538 - val_loss: 264.2031 - val_mse: 264.2032\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 213.7499 - mse: 213.7498 - val_loss: 243.9028 - val_mse: 243.9028\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 218us/sample - loss: 210.4566 - mse: 210.4565 - val_loss: 241.8828 - val_mse: 241.8829\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 206.0612 - mse: 206.0612 - val_loss: 231.6747 - val_mse: 231.6747\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 263us/sample - loss: 202.7987 - mse: 202.7987 - val_loss: 224.8617 - val_mse: 224.8617\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 199.7519 - mse: 199.7519 - val_loss: 221.3867 - val_mse: 221.3867\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 194.5586 - mse: 194.5586 - val_loss: 215.7336 - val_mse: 215.7336\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 240us/sample - loss: 189.1847 - mse: 189.1847 - val_loss: 213.3083 - val_mse: 213.3084\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 190.2600 - mse: 190.2600 - val_loss: 228.1661 - val_mse: 228.1661\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 185.5934 - mse: 185.5933 - val_loss: 205.2380 - val_mse: 205.2380\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 242us/sample - loss: 182.5219 - mse: 182.5219 - val_loss: 201.2419 - val_mse: 201.2419\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 179.6809 - mse: 179.6809 - val_loss: 203.5780 - val_mse: 203.5781\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 177.5074 - mse: 177.5074 - val_loss: 199.1752 - val_mse: 199.1752\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 177.7468 - mse: 177.7469 - val_loss: 193.6143 - val_mse: 193.6143\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 230us/sample - loss: 167.2944 - mse: 167.2945 - val_loss: 194.4260 - val_mse: 194.4260\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 176.5473 - mse: 176.5473 - val_loss: 191.3724 - val_mse: 191.3724\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 173.0970 - mse: 173.0969 - val_loss: 192.1126 - val_mse: 192.1125\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 163.4249 - mse: 163.4249 - val_loss: 183.2135 - val_mse: 183.2135\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 251us/sample - loss: 169.9583 - mse: 169.9583 - val_loss: 189.2330 - val_mse: 189.2330\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 171.4081 - mse: 171.4081 - val_loss: 187.2212 - val_mse: 187.2212\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 1s 324us/sample - loss: 160.3591 - mse: 160.3592 - val_loss: 188.4597 - val_mse: 188.4596\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 318us/sample - loss: 161.5496 - mse: 161.5496 - val_loss: 181.3281 - val_mse: 181.3280\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 162.6656 - mse: 162.6657 - val_loss: 179.9943 - val_mse: 179.9942\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 276us/sample - loss: 162.4418 - mse: 162.4418 - val_loss: 185.9606 - val_mse: 185.9606\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 166.4392 - mse: 166.4392 - val_loss: 175.7289 - val_mse: 175.7289\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 162.0066 - mse: 162.0066 - val_loss: 176.7993 - val_mse: 176.7993\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 155.7723 - mse: 155.7723 - val_loss: 177.2134 - val_mse: 177.2133\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 160.9406 - mse: 160.9406 - val_loss: 176.7970 - val_mse: 176.7971\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 219us/sample - loss: 155.4869 - mse: 155.4869 - val_loss: 173.6692 - val_mse: 173.6692\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 160.6399 - mse: 160.6398 - val_loss: 179.8581 - val_mse: 179.8582\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 279us/sample - loss: 162.7393 - mse: 162.7393 - val_loss: 180.2870 - val_mse: 180.2870\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 158.5734 - mse: 158.5734 - val_loss: 182.5918 - val_mse: 182.5918\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 160.0736 - mse: 160.0736 - val_loss: 175.0141 - val_mse: 175.0141\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 1s 271us/sample - loss: 155.0820 - mse: 155.0820 - val_loss: 171.5173 - val_mse: 171.5173\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 1s 260us/sample - loss: 169.0482 - mse: 169.0482 - val_loss: 172.9959 - val_mse: 172.9959\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 1s 237us/sample - loss: 153.6260 - mse: 153.6260 - val_loss: 165.8864 - val_mse: 165.8865\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 148.9134 - mse: 148.9134 - val_loss: 173.7522 - val_mse: 173.7522\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 152.4314 - mse: 152.4313 - val_loss: 173.3160 - val_mse: 173.3160\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 154.3753 - mse: 154.3753 - val_loss: 196.5824 - val_mse: 196.5824\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 1s 236us/sample - loss: 151.8114 - mse: 151.8114 - val_loss: 170.3923 - val_mse: 170.3923\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 148.7625 - mse: 148.7625 - val_loss: 170.9082 - val_mse: 170.9081\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 151.7408 - mse: 151.7408 - val_loss: 166.3658 - val_mse: 166.3658\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 1s 241us/sample - loss: 147.5568 - mse: 147.5568 - val_loss: 169.4889 - val_mse: 169.4889\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 1s 225us/sample - loss: 142.7350 - mse: 142.7350 - val_loss: 171.9734 - val_mse: 171.9735\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 154.1386 - mse: 154.1386 - val_loss: 164.1217 - val_mse: 164.1217\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 1s 215us/sample - loss: 147.1607 - mse: 147.1606 - val_loss: 158.8227 - val_mse: 158.8227\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 1s 250us/sample - loss: 145.8419 - mse: 145.8418 - val_loss: 164.7212 - val_mse: 164.7212\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 1s 244us/sample - loss: 150.7002 - mse: 150.7002 - val_loss: 159.7046 - val_mse: 159.7046\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 1s 294us/sample - loss: 147.5986 - mse: 147.5986 - val_loss: 170.3125 - val_mse: 170.3125\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 1s 259us/sample - loss: 145.7485 - mse: 145.7485 - val_loss: 164.6468 - val_mse: 164.6468\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 1s 285us/sample - loss: 150.1893 - mse: 150.1893 - val_loss: 159.4082 - val_mse: 159.4081\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 146.1788 - mse: 146.1788 - val_loss: 172.7586 - val_mse: 172.7586\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 148.2744 - mse: 148.2744 - val_loss: 158.6386 - val_mse: 158.6387\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 1s 248us/sample - loss: 138.0572 - mse: 138.0572 - val_loss: 161.6352 - val_mse: 161.6353\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 1s 221us/sample - loss: 150.5432 - mse: 150.5433 - val_loss: 157.2410 - val_mse: 157.2410\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 1s 228us/sample - loss: 146.6629 - mse: 146.6628 - val_loss: 151.8093 - val_mse: 151.8093\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 138.1000 - mse: 138.1000 - val_loss: 184.4911 - val_mse: 184.4911\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 1s 235us/sample - loss: 142.7628 - mse: 142.7628 - val_loss: 152.3627 - val_mse: 152.3627\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 1s 222us/sample - loss: 139.5488 - mse: 139.5488 - val_loss: 153.8817 - val_mse: 153.8818\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 1s 247us/sample - loss: 145.8135 - mse: 145.8135 - val_loss: 161.0492 - val_mse: 161.0492\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 139.8717 - mse: 139.8717 - val_loss: 151.0128 - val_mse: 151.0128\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 139.1079 - mse: 139.1079 - val_loss: 152.9507 - val_mse: 152.9507\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 148.4154 - mse: 148.4154 - val_loss: 156.1007 - val_mse: 156.1007\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 136.6214 - mse: 136.6214 - val_loss: 149.5620 - val_mse: 149.5620\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 135.5549 - mse: 135.5549 - val_loss: 169.5177 - val_mse: 169.5177\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 1s 231us/sample - loss: 129.6405 - mse: 129.6406 - val_loss: 151.0481 - val_mse: 151.0480\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 1s 249us/sample - loss: 140.1134 - mse: 140.1134 - val_loss: 156.0465 - val_mse: 156.0466\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 1s 216us/sample - loss: 137.0375 - mse: 137.0376 - val_loss: 162.2982 - val_mse: 162.2982\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 1s 200us/sample - loss: 132.0057 - mse: 132.0057 - val_loss: 167.3562 - val_mse: 167.3562\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 138.8433 - mse: 138.8433 - val_loss: 154.0080 - val_mse: 154.0080\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 1s 224us/sample - loss: 133.7291 - mse: 133.7291 - val_loss: 153.5337 - val_mse: 153.5337\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 141.9526 - mse: 141.9526 - val_loss: 152.4444 - val_mse: 152.4444\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 135.6967 - mse: 135.6966 - val_loss: 155.1439 - val_mse: 155.1439\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 137.6991 - mse: 137.6991 - val_loss: 151.4502 - val_mse: 151.4502\n",
      "[CV] ...................................... nl=1, nn=12, total=  59.0s\n",
      "[CV] nl=1, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 2s 619us/sample - loss: 462.4874 - mse: 462.4873 - val_loss: 516.7296 - val_mse: 516.7294\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 1s 211us/sample - loss: 385.2138 - mse: 385.2137 - val_loss: 446.4550 - val_mse: 446.4551\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 306.6712 - mse: 306.671 - 1s 221us/sample - loss: 311.7462 - mse: 311.7462 - val_loss: 341.1950 - val_mse: 341.1950\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 1s 227us/sample - loss: 250.5054 - mse: 250.5055 - val_loss: 239.8530 - val_mse: 239.8530\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 1s 208us/sample - loss: 209.9051 - mse: 209.9051 - val_loss: 205.3799 - val_mse: 205.3799\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 193.4126 - mse: 193.4126 - val_loss: 183.3779 - val_mse: 183.3779\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 182.2259 - mse: 182.2260 - val_loss: 174.0184 - val_mse: 174.0184\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 1s 210us/sample - loss: 184.8219 - mse: 184.8219 - val_loss: 169.6751 - val_mse: 169.6751\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 183.0023 - mse: 183.0023 - val_loss: 165.8108 - val_mse: 165.8108\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 183.9207 - mse: 183.9208 - val_loss: 164.4389 - val_mse: 164.4389\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 1s 255us/sample - loss: 164.3808 - mse: 164.3807 - val_loss: 157.5903 - val_mse: 157.5903\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 177.6140 - mse: 177.6140 - val_loss: 154.4683 - val_mse: 154.4683\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 1s 290us/sample - loss: 175.4146 - mse: 175.4145 - val_loss: 155.0086 - val_mse: 155.0086\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 165.8182 - mse: 165.8181 - val_loss: 150.9663 - val_mse: 150.9663\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 171.0309 - mse: 171.0309 - val_loss: 146.8320 - val_mse: 146.8320\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 166.3608 - mse: 166.3608 - val_loss: 145.9952 - val_mse: 145.9952\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 1s 266us/sample - loss: 168.4182 - mse: 168.4182 - val_loss: 144.8158 - val_mse: 144.8158\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 1s 270us/sample - loss: 161.4866 - mse: 161.4866 - val_loss: 141.8150 - val_mse: 141.8150\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 1s 274us/sample - loss: 164.0022 - mse: 164.0022 - val_loss: 143.1379 - val_mse: 143.1379\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 1s 280us/sample - loss: 163.3380 - mse: 163.3380 - val_loss: 143.9887 - val_mse: 143.9887\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 1s 277us/sample - loss: 158.0884 - mse: 158.0885 - val_loss: 137.7063 - val_mse: 137.7063\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 1s 281us/sample - loss: 155.8877 - mse: 155.8877 - val_loss: 133.9810 - val_mse: 133.9810\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 1s 214us/sample - loss: 158.6213 - mse: 158.6213 - val_loss: 135.9586 - val_mse: 135.9586\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 152.4929 - mse: 152.4930 - val_loss: 132.7100 - val_mse: 132.7100\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 1s 226us/sample - loss: 156.7782 - mse: 156.7783 - val_loss: 130.7189 - val_mse: 130.7190\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 1s 220us/sample - loss: 146.8908 - mse: 146.8908 - val_loss: 129.0797 - val_mse: 129.0797\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 1s 203us/sample - loss: 157.5772 - mse: 157.5772 - val_loss: 129.5161 - val_mse: 129.5161\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 153.7336 - mse: 153.733 - 1s 220us/sample - loss: 154.6469 - mse: 154.6469 - val_loss: 130.6764 - val_mse: 130.6764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 1s 213us/sample - loss: 145.0642 - mse: 145.0641 - val_loss: 131.7853 - val_mse: 131.7853\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 155.4492 - mse: 155.4492 - val_loss: 128.9204 - val_mse: 128.9204\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 1s 232us/sample - loss: 149.6243 - mse: 149.6244 - val_loss: 131.4223 - val_mse: 131.4223\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 1s 246us/sample - loss: 152.2454 - mse: 152.2454 - val_loss: 126.8235 - val_mse: 126.8235\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 1s 238us/sample - loss: 147.3892 - mse: 147.3892 - val_loss: 123.2000 - val_mse: 123.2000\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 1s 261us/sample - loss: 141.1692 - mse: 141.1691 - val_loss: 126.1033 - val_mse: 126.1033\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 1s 234us/sample - loss: 146.0905 - mse: 146.0906 - val_loss: 127.5130 - val_mse: 127.5130\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 1s 252us/sample - loss: 137.4344 - mse: 137.4344 - val_loss: 122.1573 - val_mse: 122.1573\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 1s 217us/sample - loss: 144.6308 - mse: 144.6308 - val_loss: 122.6285 - val_mse: 122.6285\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 1s 269us/sample - loss: 144.0036 - mse: 144.0036 - val_loss: 125.4412 - val_mse: 125.4412\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 1s 181us/sample - loss: 136.0676 - mse: 136.0676 - val_loss: 125.4840 - val_mse: 125.4840\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 137.2044 - mse: 137.2044 - val_loss: 123.7890 - val_mse: 123.7890\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 141.4314 - mse: 141.4313 - val_loss: 121.8297 - val_mse: 121.8297\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 149.6272 - mse: 149.6272 - val_loss: 122.6086 - val_mse: 122.6086\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 138.6264 - mse: 138.6264 - val_loss: 121.0484 - val_mse: 121.0484\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 138.6251 - mse: 138.6250 - val_loss: 121.6569 - val_mse: 121.6569\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 135.9453 - mse: 135.9452 - val_loss: 123.3832 - val_mse: 123.3831\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 136.8491 - mse: 136.8492 - val_loss: 119.9718 - val_mse: 119.9718\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 138.8829 - mse: 138.8828 - val_loss: 120.7068 - val_mse: 120.7068\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 143us/sample - loss: 135.3381 - mse: 135.3381 - val_loss: 120.1899 - val_mse: 120.1899\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 135.9170 - mse: 135.9170 - val_loss: 120.6761 - val_mse: 120.6761\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 135.7817 - mse: 135.7817 - val_loss: 124.7925 - val_mse: 124.7924\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 133.3848 - mse: 133.3849 - val_loss: 126.4433 - val_mse: 126.4433\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 135.0477 - mse: 135.0477 - val_loss: 119.9051 - val_mse: 119.9050\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 135.5925 - mse: 135.5925 - val_loss: 124.0652 - val_mse: 124.0652\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 141.4963 - mse: 141.4963 - val_loss: 117.9788 - val_mse: 117.9788\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 137.2295 - mse: 137.2294 - val_loss: 123.3093 - val_mse: 123.3092\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 134.4354 - mse: 134.4354 - val_loss: 118.5509 - val_mse: 118.5509\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 136.9784 - mse: 136.9783 - val_loss: 117.5081 - val_mse: 117.5081\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 133.1590 - mse: 133.1589 - val_loss: 120.6691 - val_mse: 120.6691\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 132.6311 - mse: 132.6311 - val_loss: 117.9116 - val_mse: 117.9116\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 134.5704 - mse: 134.5705 - val_loss: 122.6887 - val_mse: 122.6886\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 128.2096 - mse: 128.2096 - val_loss: 118.7534 - val_mse: 118.7534\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 140.1515 - mse: 140.1516 - val_loss: 117.1128 - val_mse: 117.1128\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 132.7600 - mse: 132.7600 - val_loss: 118.0161 - val_mse: 118.0161\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 134.2430 - mse: 134.2430 - val_loss: 116.2371 - val_mse: 116.2371\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 129.6073 - mse: 129.6073 - val_loss: 115.3085 - val_mse: 115.3086\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 137.8535 - mse: 137.8535 - val_loss: 113.9694 - val_mse: 113.9694\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 129.8894 - mse: 129.8895 - val_loss: 118.1742 - val_mse: 118.1742\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 134.4032 - mse: 134.4032 - val_loss: 117.8052 - val_mse: 117.8052\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 131.1983 - mse: 131.1983 - val_loss: 119.5346 - val_mse: 119.5346\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 130.8424 - mse: 130.8424 - val_loss: 117.7601 - val_mse: 117.7600\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 135.5698 - mse: 135.5698 - val_loss: 116.2915 - val_mse: 116.2915\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 132.4705 - mse: 132.4705 - val_loss: 115.2962 - val_mse: 115.2962\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 129.6568 - mse: 129.6568 - val_loss: 113.6047 - val_mse: 113.6048\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 128.2041 - mse: 128.2041 - val_loss: 116.0651 - val_mse: 116.0651\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 124.8430 - mse: 124.8431 - val_loss: 115.3811 - val_mse: 115.3811\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 129.4177 - mse: 129.4177 - val_loss: 118.3955 - val_mse: 118.3955\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 131.1286 - mse: 131.1286 - val_loss: 115.0751 - val_mse: 115.0751\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 131.0725 - mse: 131.0725 - val_loss: 118.3232 - val_mse: 118.3232\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 129.6656 - mse: 129.6656 - val_loss: 115.8445 - val_mse: 115.8445\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 130.7219 - mse: 130.7219 - val_loss: 115.2506 - val_mse: 115.2506\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 1s 178us/sample - loss: 132.1540 - mse: 132.1540 - val_loss: 114.3686 - val_mse: 114.3686\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 158us/sample - loss: 134.5889 - mse: 134.5890 - val_loss: 114.6935 - val_mse: 114.6935\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 131.6576 - mse: 131.6576 - val_loss: 119.2020 - val_mse: 119.2020\n",
      "[CV] ...................................... nl=1, nn=12, total=  42.6s\n",
      "[CV] nl=1, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 300us/sample - loss: 462.1072 - mse: 462.1071 - val_loss: 627.4501 - val_mse: 627.4501\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 387.8586 - mse: 387.8588 - val_loss: 529.9178 - val_mse: 529.9177\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 314.3951 - mse: 314.3951 - val_loss: 424.0097 - val_mse: 424.0097\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 261.7734 - mse: 261.7734 - val_loss: 360.7193 - val_mse: 360.7193\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 231.1611 - mse: 231.1612 - val_loss: 324.3122 - val_mse: 324.3123\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 212.3415 - mse: 212.3416 - val_loss: 284.7823 - val_mse: 284.7823\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 199.1230 - mse: 199.1229 - val_loss: 268.0903 - val_mse: 268.0902\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 182.1990 - mse: 182.1989 - val_loss: 248.8280 - val_mse: 248.8280\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 177.3450 - mse: 177.3450 - val_loss: 240.9352 - val_mse: 240.9353\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 178.0062 - mse: 178.0062 - val_loss: 238.3438 - val_mse: 238.3438\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 181.5806 - mse: 181.5805 - val_loss: 237.2368 - val_mse: 237.2368\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 173.4574 - mse: 173.4574 - val_loss: 233.1818 - val_mse: 233.1818\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 171.7434 - mse: 171.7434 - val_loss: 232.5030 - val_mse: 232.5030\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 176.2679 - mse: 176.2679 - val_loss: 226.0676 - val_mse: 226.0676\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 173.8093 - mse: 173.8092 - val_loss: 225.2080 - val_mse: 225.2080\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 169.7084 - mse: 169.7085 - val_loss: 221.3543 - val_mse: 221.3543\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 170.3652 - mse: 170.3651 - val_loss: 220.4643 - val_mse: 220.4643\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 167.9459 - mse: 167.9459 - val_loss: 219.2328 - val_mse: 219.2328\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 164.9121 - mse: 164.9121 - val_loss: 220.3904 - val_mse: 220.3903\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 165.5063 - mse: 165.5063 - val_loss: 214.0187 - val_mse: 214.0187\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 169.8625 - mse: 169.8625 - val_loss: 215.5405 - val_mse: 215.5405\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 164.4130 - mse: 164.4129 - val_loss: 209.3428 - val_mse: 209.3428\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 164.5964 - mse: 164.5964 - val_loss: 208.4237 - val_mse: 208.4237\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 153.8301 - mse: 153.8301 - val_loss: 202.0040 - val_mse: 202.0040\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 151.7435 - mse: 151.7435 - val_loss: 202.1938 - val_mse: 202.1938\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 150.6788 - mse: 150.6788 - val_loss: 200.1689 - val_mse: 200.1690\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 154.0138 - mse: 154.0138 - val_loss: 197.1374 - val_mse: 197.1374\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 155.9597 - mse: 155.9598 - val_loss: 195.8723 - val_mse: 195.8723\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 157.4054 - mse: 157.4054 - val_loss: 192.3572 - val_mse: 192.3572\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 155.0876 - mse: 155.0876 - val_loss: 193.8807 - val_mse: 193.8807\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 149.2364 - mse: 149.2364 - val_loss: 193.2922 - val_mse: 193.2922\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 148.7349 - mse: 148.7349 - val_loss: 189.5910 - val_mse: 189.5910\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 145.5348 - mse: 145.5348 - val_loss: 188.6242 - val_mse: 188.6242\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 155.3817 - mse: 155.3817 - val_loss: 184.9866 - val_mse: 184.9865\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 154.8861 - mse: 154.8860 - val_loss: 185.3248 - val_mse: 185.3247\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 150.1280 - mse: 150.1280 - val_loss: 184.2200 - val_mse: 184.2200\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 149.6130 - mse: 149.6130 - val_loss: 186.0691 - val_mse: 186.0691\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 145.9463 - mse: 145.9463 - val_loss: 182.7772 - val_mse: 182.7772\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 148.8979 - mse: 148.8979 - val_loss: 195.1796 - val_mse: 195.1796\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 148.4330 - mse: 148.4330 - val_loss: 180.3364 - val_mse: 180.3365\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 145.7976 - mse: 145.7976 - val_loss: 181.6975 - val_mse: 181.6975\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 141.4643 - mse: 141.4642 - val_loss: 178.4119 - val_mse: 178.4119\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 155.0879 - mse: 155.0879 - val_loss: 178.2828 - val_mse: 178.2828\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 149.1450 - mse: 149.1450 - val_loss: 178.6380 - val_mse: 178.6380\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 142.5880 - mse: 142.5881 - val_loss: 177.1834 - val_mse: 177.1834\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.2407 - mse: 144.2408 - val_loss: 184.1468 - val_mse: 184.1468\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 147.2106 - mse: 147.2106 - val_loss: 178.4436 - val_mse: 178.4436\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 144.8328 - mse: 144.8328 - val_loss: 174.6040 - val_mse: 174.6040\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 142.1889 - mse: 142.1889 - val_loss: 173.3197 - val_mse: 173.3196\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 144.7295 - mse: 144.7294 - val_loss: 177.2154 - val_mse: 177.2154\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 142.3837 - mse: 142.3837 - val_loss: 175.8442 - val_mse: 175.8442\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 146.4594 - mse: 146.4594 - val_loss: 172.1487 - val_mse: 172.1487\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 140.1717 - mse: 140.1717 - val_loss: 173.2864 - val_mse: 173.2864\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 138.3479 - mse: 138.3479 - val_loss: 172.4387 - val_mse: 172.4387\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 102us/sample - loss: 143.9910 - mse: 143.9910 - val_loss: 173.7228 - val_mse: 173.7228\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 141.6900 - mse: 141.6900 - val_loss: 170.0056 - val_mse: 170.0056\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 140.5736 - mse: 140.5736 - val_loss: 170.1830 - val_mse: 170.1830\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 140.5881 - mse: 140.5881 - val_loss: 170.0676 - val_mse: 170.0676\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.5730 - mse: 141.5729 - val_loss: 172.1633 - val_mse: 172.1633\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 145.6899 - mse: 145.6899 - val_loss: 171.7497 - val_mse: 171.7496\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 135.1933 - mse: 135.1933 - val_loss: 174.4761 - val_mse: 174.4761\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 136.6333 - mse: 136.6332 - val_loss: 175.0907 - val_mse: 175.0907\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 135.4135 - mse: 135.4135 - val_loss: 173.2036 - val_mse: 173.2036\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 139.4464 - mse: 139.4464 - val_loss: 170.9833 - val_mse: 170.9833\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 137.0898 - mse: 137.0898 - val_loss: 184.2717 - val_mse: 184.2716\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 135.4191 - mse: 135.4191 - val_loss: 181.8118 - val_mse: 181.8118\n",
      "[CV] ...................................... nl=1, nn=12, total=  21.0s\n",
      "[CV] nl=1, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 373us/sample - loss: 493.0228 - mse: 493.0227 - val_loss: 631.0035 - val_mse: 631.0035\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 369.9205 - mse: 369.9205 - val_loss: 499.4078 - val_mse: 499.4078\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 276.1084 - mse: 276.1084 - val_loss: 346.6849 - val_mse: 346.6848\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 225.6096 - mse: 225.6096 - val_loss: 277.6779 - val_mse: 277.6779\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 207.8505 - mse: 207.8505 - val_loss: 249.9947 - val_mse: 249.9947\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 198.1190 - mse: 198.1189 - val_loss: 231.9362 - val_mse: 231.9362\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 202.4248 - mse: 202.4248 - val_loss: 233.4485 - val_mse: 233.4485\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 197.8662 - mse: 197.8662 - val_loss: 223.7496 - val_mse: 223.7497\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 194.2824 - mse: 194.2824 - val_loss: 220.2130 - val_mse: 220.2130\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 185.4889 - mse: 185.4890 - val_loss: 216.5723 - val_mse: 216.5723\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 186.5641 - mse: 186.5641 - val_loss: 215.9189 - val_mse: 215.9189\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 182.5324 - mse: 182.5324 - val_loss: 200.9396 - val_mse: 200.9397\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 169.0826 - mse: 169.0826 - val_loss: 204.2178 - val_mse: 204.2178\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 173.8744 - mse: 173.8744 - val_loss: 199.2155 - val_mse: 199.2155\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 166.2815 - mse: 166.2815 - val_loss: 193.0253 - val_mse: 193.0253\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 162.0006 - mse: 162.0006 - val_loss: 183.8693 - val_mse: 183.8693\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 165.3692 - mse: 165.3692 - val_loss: 175.7937 - val_mse: 175.7937\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.5726 - mse: 151.5726 - val_loss: 177.8469 - val_mse: 177.8469\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 151.7603 - mse: 151.7603 - val_loss: 181.3565 - val_mse: 181.3564\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 157.7882 - mse: 157.7882 - val_loss: 176.4517 - val_mse: 176.4518\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 146.5948 - mse: 146.5947 - val_loss: 180.9627 - val_mse: 180.9627\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 146.4824 - mse: 146.4824 - val_loss: 178.8581 - val_mse: 178.8582\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 141.1514 - mse: 141.1514 - val_loss: 179.1803 - val_mse: 179.1803\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 141.6373 - mse: 141.6373 - val_loss: 177.7305 - val_mse: 177.7305\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.9949 - mse: 143.9949 - val_loss: 183.3048 - val_mse: 183.3047\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 148.0588 - mse: 148.0588 - val_loss: 182.1902 - val_mse: 182.1902\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 144.1571 - mse: 144.1571 - val_loss: 186.3775 - val_mse: 186.3775\n",
      "[CV] ...................................... nl=1, nn=24, total=   9.0s\n",
      "[CV] nl=1, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 296us/sample - loss: 428.4110 - mse: 428.4109 - val_loss: 629.3016 - val_mse: 629.3016\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 327.6634 - mse: 327.6634 - val_loss: 497.1684 - val_mse: 497.1684\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 243.3980 - mse: 243.3980 - val_loss: 366.5047 - val_mse: 366.5047\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 204.2065 - mse: 204.2064 - val_loss: 310.5437 - val_mse: 310.5438\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 187.2297 - mse: 187.2298 - val_loss: 267.7234 - val_mse: 267.7235\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 174.6749 - mse: 174.6749 - val_loss: 255.0870 - val_mse: 255.0870\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 166.2253 - mse: 166.2253 - val_loss: 243.4810 - val_mse: 243.4810\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 165.1421 - mse: 165.142 - 0s 100us/sample - loss: 164.0112 - mse: 164.0112 - val_loss: 235.9456 - val_mse: 235.9456\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 166.8958 - mse: 166.8958 - val_loss: 226.4863 - val_mse: 226.4863\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 155.6966 - mse: 155.6966 - val_loss: 221.3119 - val_mse: 221.3119\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 164.2463 - mse: 164.2464 - val_loss: 218.7763 - val_mse: 218.7763\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.8739 - mse: 151.8739 - val_loss: 209.2569 - val_mse: 209.2569\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 161.2416 - mse: 161.2415 - val_loss: 206.9864 - val_mse: 206.9865\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 157.7452 - mse: 157.7452 - val_loss: 208.4059 - val_mse: 208.4059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 153.1384 - mse: 153.1385 - val_loss: 197.9578 - val_mse: 197.9579\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.8514 - mse: 151.8515 - val_loss: 197.0678 - val_mse: 197.0678\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 145.9838 - mse: 145.9838 - val_loss: 189.8821 - val_mse: 189.8821\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 145.9168 - mse: 145.9168 - val_loss: 197.4250 - val_mse: 197.4251\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 158us/sample - loss: 142.1070 - mse: 142.1069 - val_loss: 189.6380 - val_mse: 189.6380\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 139.2120 - mse: 139.2120 - val_loss: 189.2327 - val_mse: 189.2326\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 140.8074 - mse: 140.8074 - val_loss: 189.2163 - val_mse: 189.2163\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 141.0655 - mse: 141.0655 - val_loss: 182.4873 - val_mse: 182.4873\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 139.7454 - mse: 139.7454 - val_loss: 177.6593 - val_mse: 177.6593\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 145.2983 - mse: 145.2983 - val_loss: 181.4848 - val_mse: 181.4848\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 139.3845 - mse: 139.3845 - val_loss: 177.9617 - val_mse: 177.9617\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 131.0206 - mse: 131.0206 - val_loss: 188.1935 - val_mse: 188.1935\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.6826 - mse: 144.6826 - val_loss: 182.5415 - val_mse: 182.5416\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 137.4418 - mse: 137.4418 - val_loss: 178.6360 - val_mse: 178.6360\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 140.6242 - mse: 140.6241 - val_loss: 176.2674 - val_mse: 176.2675\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 137.3468 - mse: 137.3468 - val_loss: 176.4779 - val_mse: 176.4779\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 132.1308 - mse: 132.1309 - val_loss: 175.2653 - val_mse: 175.2653\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 139.0799 - mse: 139.0799 - val_loss: 173.0168 - val_mse: 173.0168\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 135.7045 - mse: 135.7045 - val_loss: 172.2545 - val_mse: 172.2545\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 135.7895 - mse: 135.7895 - val_loss: 174.2556 - val_mse: 174.2555\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 129.1677 - mse: 129.1677 - val_loss: 178.2368 - val_mse: 178.2368\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 136.4077 - mse: 136.4077 - val_loss: 176.0460 - val_mse: 176.0460\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 140.6613 - mse: 140.661 - 0s 100us/sample - loss: 135.1479 - mse: 135.1479 - val_loss: 176.1788 - val_mse: 176.1788\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 133.0525 - mse: 133.0525 - val_loss: 175.3570 - val_mse: 175.3569\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 130.4829 - mse: 130.4829 - val_loss: 174.1459 - val_mse: 174.1459\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 131.5465 - mse: 131.5466 - val_loss: 170.4777 - val_mse: 170.4777\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 127.5850 - mse: 127.5850 - val_loss: 183.1322 - val_mse: 183.1322\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 130.6059 - mse: 130.6060 - val_loss: 167.1205 - val_mse: 167.1205\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.6261 - mse: 134.6261 - val_loss: 170.9778 - val_mse: 170.9778\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 134.6730 - mse: 134.6730 - val_loss: 165.0014 - val_mse: 165.0014\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 126.3848 - mse: 126.3848 - val_loss: 176.3681 - val_mse: 176.3681\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 127.5201 - mse: 127.5201 - val_loss: 169.4150 - val_mse: 169.4150\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 129.6599 - mse: 129.6599 - val_loss: 175.7373 - val_mse: 175.7374\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 128.3095 - mse: 128.3095 - val_loss: 176.4822 - val_mse: 176.4823\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 130.9840 - mse: 130.9839 - val_loss: 165.9725 - val_mse: 165.9726\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 124.4240 - mse: 124.4239 - val_loss: 173.6908 - val_mse: 173.6908\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 124.4526 - mse: 124.4526 - val_loss: 173.3412 - val_mse: 173.3412\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 127.0080 - mse: 127.0080 - val_loss: 168.8290 - val_mse: 168.8290\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 122.8125 - mse: 122.8125 - val_loss: 165.2655 - val_mse: 165.2655\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 126.0402 - mse: 126.0402 - val_loss: 161.9781 - val_mse: 161.9781\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 131.8927 - mse: 131.8927 - val_loss: 166.6116 - val_mse: 166.6116\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 131.3323 - mse: 131.3323 - val_loss: 176.8763 - val_mse: 176.8763\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 126.5001 - mse: 126.5001 - val_loss: 169.3570 - val_mse: 169.3570\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 120.7255 - mse: 120.7256 - val_loss: 162.5870 - val_mse: 162.5870\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 125.6730 - mse: 125.6730 - val_loss: 172.4110 - val_mse: 172.4109\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 126.2417 - mse: 126.2417 - val_loss: 171.3222 - val_mse: 171.3222\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 124.0696 - mse: 124.0697 - val_loss: 166.5141 - val_mse: 166.5141\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 130.3923 - mse: 130.3923 - val_loss: 160.4233 - val_mse: 160.4233\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 129.8784 - mse: 129.8783 - val_loss: 161.3417 - val_mse: 161.3418\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 124.4706 - mse: 124.4706 - val_loss: 167.0135 - val_mse: 167.0135\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 125.8087 - mse: 125.8087 - val_loss: 170.3958 - val_mse: 170.3958\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 122.0236 - mse: 122.0236 - val_loss: 156.5265 - val_mse: 156.5265\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 121.9567 - mse: 121.9567 - val_loss: 161.6255 - val_mse: 161.6255\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 120.9290 - mse: 120.9290 - val_loss: 166.5886 - val_mse: 166.5885\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 119.0558 - mse: 119.0558 - val_loss: 159.6067 - val_mse: 159.6066\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 119.1106 - mse: 119.1106 - val_loss: 161.4036 - val_mse: 161.4036\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 116.6780 - mse: 116.6780 - val_loss: 163.0791 - val_mse: 163.0790\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 125.3148 - mse: 125.3148 - val_loss: 165.3200 - val_mse: 165.3200\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 122.4038 - mse: 122.4038 - val_loss: 168.5541 - val_mse: 168.5541\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 118.5083 - mse: 118.5083 - val_loss: 156.9194 - val_mse: 156.9194\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 119.4327 - mse: 119.4327 - val_loss: 174.5998 - val_mse: 174.5998\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 119.4468 - mse: 119.4468 - val_loss: 158.2740 - val_mse: 158.2740\n",
      "[CV] ...................................... nl=1, nn=24, total=  23.2s\n",
      "[CV] nl=1, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 291us/sample - loss: 464.9226 - mse: 464.9223 - val_loss: 621.0487 - val_mse: 621.0486\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 338.0872 - mse: 338.0872 - val_loss: 461.9029 - val_mse: 461.9028\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 251.1176 - mse: 251.1176 - val_loss: 298.9037 - val_mse: 298.9037\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 210.1464 - mse: 210.1465 - val_loss: 216.6019 - val_mse: 216.6019\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 184.9009 - mse: 184.9009 - val_loss: 201.7710 - val_mse: 201.7710\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 185.2949 - mse: 185.2948 - val_loss: 195.3593 - val_mse: 195.3593\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 181.6408 - mse: 181.6408 - val_loss: 193.3398 - val_mse: 193.3398\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 184.4989 - mse: 184.4989 - val_loss: 193.5714 - val_mse: 193.5714\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 174.8991 - mse: 174.8990 - val_loss: 187.9768 - val_mse: 187.9768\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 98us/sample - loss: 175.4959 - mse: 175.4959 - val_loss: 183.0584 - val_mse: 183.0584\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 98us/sample - loss: 169.6011 - mse: 169.6011 - val_loss: 184.5423 - val_mse: 184.5423\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 170.3847 - mse: 170.3846 - val_loss: 182.3005 - val_mse: 182.3005\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 175.4608 - mse: 175.4608 - val_loss: 185.6103 - val_mse: 185.6102\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 169.3861 - mse: 169.3860 - val_loss: 182.0509 - val_mse: 182.0509\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 168.1364 - mse: 168.1364 - val_loss: 182.8396 - val_mse: 182.8395\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 179.5440 - mse: 179.5440 - val_loss: 182.9834 - val_mse: 182.9834\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 169.6138 - mse: 169.6138 - val_loss: 182.5641 - val_mse: 182.5641\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 168.5023 - mse: 168.5023 - val_loss: 180.2163 - val_mse: 180.2163\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 173.8510 - mse: 173.8510 - val_loss: 183.9432 - val_mse: 183.9431\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 168.5745 - mse: 168.5745 - val_loss: 182.2635 - val_mse: 182.2635\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 163.4894 - mse: 163.4894 - val_loss: 175.5330 - val_mse: 175.5331\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 162.1753 - mse: 162.1753 - val_loss: 176.0836 - val_mse: 176.0837\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 163.2531 - mse: 163.2530 - val_loss: 190.8200 - val_mse: 190.8200\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 164.3921 - mse: 164.3921 - val_loss: 177.5903 - val_mse: 177.5903\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 164.5717 - mse: 164.5717 - val_loss: 184.4929 - val_mse: 184.4929\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 163.6446 - mse: 163.6446 - val_loss: 174.7567 - val_mse: 174.7567\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 161.5584 - mse: 161.5584 - val_loss: 176.7181 - val_mse: 176.7181\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 165.3106 - mse: 165.3106 - val_loss: 177.0616 - val_mse: 177.0616\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 160.1009 - mse: 160.1008 - val_loss: 186.5357 - val_mse: 186.5357\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 165.1988 - mse: 165.1989 - val_loss: 175.4905 - val_mse: 175.4905\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 159.2764 - mse: 159.2764 - val_loss: 177.7737 - val_mse: 177.7737\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 156.3698 - mse: 156.3697 - val_loss: 174.9300 - val_mse: 174.9300\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 160.6563 - mse: 160.6563 - val_loss: 176.0248 - val_mse: 176.0248\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 168.1804 - mse: 168.1804 - val_loss: 178.1440 - val_mse: 178.1440\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 165.3194 - mse: 165.3194 - val_loss: 183.2633 - val_mse: 183.2633\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 155.5606 - mse: 155.5606 - val_loss: 174.1356 - val_mse: 174.1356\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 133us/sample - loss: 151.1950 - mse: 151.1950 - val_loss: 175.2010 - val_mse: 175.2010\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 153.6030 - mse: 153.6030 - val_loss: 174.5137 - val_mse: 174.5137\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 154.6698 - mse: 154.6698 - val_loss: 175.5401 - val_mse: 175.5401\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 157.5579 - mse: 157.5578 - val_loss: 177.4085 - val_mse: 177.4085\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 160.3737 - mse: 160.3737 - val_loss: 175.2448 - val_mse: 175.2448\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 156.8444 - mse: 156.8444 - val_loss: 184.4087 - val_mse: 184.4087\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 159.7336 - mse: 159.7336 - val_loss: 177.2458 - val_mse: 177.2458\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 154.7534 - mse: 154.7534 - val_loss: 174.3154 - val_mse: 174.3154\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 162.9816 - mse: 162.9815 - val_loss: 176.8612 - val_mse: 176.8612\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 159.2463 - mse: 159.2463 - val_loss: 175.5899 - val_mse: 175.5899\n",
      "[CV] ...................................... nl=1, nn=24, total=  14.4s\n",
      "[CV] nl=1, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 380us/sample - loss: 433.6775 - mse: 433.6774 - val_loss: 511.8000 - val_mse: 511.8000\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 319.0498 - mse: 319.0497 - val_loss: 395.7379 - val_mse: 395.7379\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 239.3506 - mse: 239.3505 - val_loss: 250.7762 - val_mse: 250.7762\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 201.4360 - mse: 201.4360 - val_loss: 192.7469 - val_mse: 192.7469\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 192.3127 - mse: 192.3127 - val_loss: 190.8014 - val_mse: 190.8013\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 184.7802 - mse: 184.7802 - val_loss: 175.3986 - val_mse: 175.3986\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 181.1540 - mse: 181.1540 - val_loss: 167.1345 - val_mse: 167.1345\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 178.7739 - mse: 178.7739 - val_loss: 169.8506 - val_mse: 169.8506\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 171.9682 - mse: 171.9682 - val_loss: 162.5617 - val_mse: 162.5617\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 172.0789 - mse: 172.0789 - val_loss: 156.4990 - val_mse: 156.4990\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 169.3454 - mse: 169.3454 - val_loss: 152.4177 - val_mse: 152.4177\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 162.2586 - mse: 162.2586 - val_loss: 148.1481 - val_mse: 148.1481\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 164.2075 - mse: 164.2075 - val_loss: 146.4768 - val_mse: 146.4767\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 162.8228 - mse: 162.8228 - val_loss: 148.2686 - val_mse: 148.2685\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 159.9325 - mse: 159.9325 - val_loss: 148.1091 - val_mse: 148.1091\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 134us/sample - loss: 157.5947 - mse: 157.5947 - val_loss: 139.7865 - val_mse: 139.7865\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 156.9629 - mse: 156.9629 - val_loss: 140.7356 - val_mse: 140.7356\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 151.9677 - mse: 151.9678 - val_loss: 141.7308 - val_mse: 141.7308\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 154.4988 - mse: 154.4988 - val_loss: 142.3311 - val_mse: 142.3311\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 148.2907 - mse: 148.2907 - val_loss: 135.3315 - val_mse: 135.3315\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.9535 - mse: 151.9535 - val_loss: 136.9083 - val_mse: 136.9083\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 151.3911 - mse: 151.3912 - val_loss: 149.4613 - val_mse: 149.4613\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.4349 - mse: 142.4350 - val_loss: 141.3682 - val_mse: 141.3682\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 151.7019 - mse: 151.7020 - val_loss: 138.4218 - val_mse: 138.4218\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 152.2364 - mse: 152.2364 - val_loss: 138.5323 - val_mse: 138.5323\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 146.8080 - mse: 146.8079 - val_loss: 135.4324 - val_mse: 135.4324\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 146.0631 - mse: 146.0631 - val_loss: 134.3238 - val_mse: 134.3238\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 145.1938 - mse: 145.1937 - val_loss: 132.8409 - val_mse: 132.8409\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 141.7335 - mse: 141.7335 - val_loss: 130.7015 - val_mse: 130.7015\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.4182 - mse: 141.4182 - val_loss: 134.2913 - val_mse: 134.2913\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 147.4828 - mse: 147.4827 - val_loss: 132.8366 - val_mse: 132.8366\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 145.4328 - mse: 145.4328 - val_loss: 135.5805 - val_mse: 135.5805\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 141.5834 - mse: 141.5835 - val_loss: 133.4792 - val_mse: 133.4792\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 145.1847 - mse: 145.1847 - val_loss: 129.1099 - val_mse: 129.1099\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 140.3388 - mse: 140.3389 - val_loss: 133.2964 - val_mse: 133.2964\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 142.2324 - mse: 142.2324 - val_loss: 136.8364 - val_mse: 136.8364\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 139.5510 - mse: 139.5510 - val_loss: 132.7042 - val_mse: 132.7042\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.9439 - mse: 142.9439 - val_loss: 142.3154 - val_mse: 142.3154\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.2161 - mse: 134.2161 - val_loss: 132.8342 - val_mse: 132.8343\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 140.8640 - mse: 140.8640 - val_loss: 129.7076 - val_mse: 129.7076\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 140.6850 - mse: 140.6849 - val_loss: 130.6790 - val_mse: 130.6790\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.7365 - mse: 144.7365 - val_loss: 133.5920 - val_mse: 133.5920\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 145.7937 - mse: 145.7937 - val_loss: 131.2935 - val_mse: 131.2935\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.6468 - mse: 143.6468 - val_loss: 129.2916 - val_mse: 129.2915\n",
      "[CV] ...................................... nl=1, nn=24, total=  14.3s\n",
      "[CV] nl=1, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 304us/sample - loss: 434.9388 - mse: 434.9387 - val_loss: 608.0702 - val_mse: 608.0703\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 328.1845 - mse: 328.1846 - val_loss: 460.7820 - val_mse: 460.7820\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 246.2228 - mse: 246.2228 - val_loss: 325.5303 - val_mse: 325.5303\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 207.3499 - mse: 207.3500 - val_loss: 269.5624 - val_mse: 269.5624\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 191.6475 - mse: 191.6475 - val_loss: 253.2878 - val_mse: 253.2877\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 182.3502 - mse: 182.3502 - val_loss: 232.1590 - val_mse: 232.1590\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 180.1445 - mse: 180.1446 - val_loss: 231.3911 - val_mse: 231.3911\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 170.9826 - mse: 170.9827 - val_loss: 217.0447 - val_mse: 217.0448\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 166.9320 - mse: 166.9320 - val_loss: 211.7853 - val_mse: 211.7853\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 172.3043 - mse: 172.3043 - val_loss: 213.4583 - val_mse: 213.4582\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 165.3474 - mse: 165.3474 - val_loss: 205.2404 - val_mse: 205.2404\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 165.7895 - mse: 165.7895 - val_loss: 202.6373 - val_mse: 202.6373\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 158.6777 - mse: 158.6776 - val_loss: 196.5775 - val_mse: 196.5775\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 155.1690 - mse: 155.1689 - val_loss: 193.4995 - val_mse: 193.4995\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 162.8163 - mse: 162.8162 - val_loss: 195.1269 - val_mse: 195.1269\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 155.0975 - mse: 155.0975 - val_loss: 191.6155 - val_mse: 191.6155\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 160.1066 - mse: 160.1066 - val_loss: 192.4896 - val_mse: 192.4896\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 152.2154 - mse: 152.2154 - val_loss: 187.1239 - val_mse: 187.1239\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 152.9176 - mse: 152.9176 - val_loss: 192.9722 - val_mse: 192.9722\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 153.0942 - mse: 153.0942 - val_loss: 187.0645 - val_mse: 187.0645\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 146.7365 - mse: 146.7364 - val_loss: 182.7043 - val_mse: 182.7043\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 153.6057 - mse: 153.6057 - val_loss: 188.2205 - val_mse: 188.2205\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 151.1934 - mse: 151.1934 - val_loss: 180.3543 - val_mse: 180.3543\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 155.3543 - mse: 155.3543 - val_loss: 182.7917 - val_mse: 182.7917\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 149.1602 - mse: 149.1602 - val_loss: 181.8089 - val_mse: 181.8089\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 147.7940 - mse: 147.7941 - val_loss: 175.7137 - val_mse: 175.7138\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 147.3232 - mse: 147.3232 - val_loss: 176.0528 - val_mse: 176.0528\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 149.2340 - mse: 149.2340 - val_loss: 173.9067 - val_mse: 173.9067\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 147.9899 - mse: 147.9898 - val_loss: 173.6720 - val_mse: 173.6720\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 144.8611 - mse: 144.8611 - val_loss: 174.8856 - val_mse: 174.8856\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 145.1053 - mse: 145.1053 - val_loss: 174.3450 - val_mse: 174.3450\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.0270 - mse: 141.0271 - val_loss: 169.1834 - val_mse: 169.1834\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 146.6920 - mse: 146.6920 - val_loss: 168.8210 - val_mse: 168.8210\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 145.7941 - mse: 145.7941 - val_loss: 169.4135 - val_mse: 169.4135\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 140.3889 - mse: 140.3889 - val_loss: 172.2042 - val_mse: 172.2042\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 132.9469 - mse: 132.9469 - val_loss: 167.8917 - val_mse: 167.8917\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 149us/sample - loss: 144.3832 - mse: 144.3832 - val_loss: 168.5850 - val_mse: 168.5849\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 137.6182 - mse: 137.6182 - val_loss: 167.3492 - val_mse: 167.3492\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 147.2421 - mse: 147.2421 - val_loss: 164.9741 - val_mse: 164.9741\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 145.0957 - mse: 145.0957 - val_loss: 161.4542 - val_mse: 161.4542\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.2858 - mse: 142.2857 - val_loss: 166.3859 - val_mse: 166.3859\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 139.9081 - mse: 139.9081 - val_loss: 163.5919 - val_mse: 163.5919\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 139.1860 - mse: 139.1859 - val_loss: 161.4620 - val_mse: 161.4620\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 139.7559 - mse: 139.7558 - val_loss: 159.5229 - val_mse: 159.5229\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 137.6700 - mse: 137.6700 - val_loss: 160.6190 - val_mse: 160.6189\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.7676 - mse: 134.7675 - val_loss: 158.4561 - val_mse: 158.4561\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 138.3211 - mse: 138.3211 - val_loss: 160.2426 - val_mse: 160.2426\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 140.5674 - mse: 140.5674 - val_loss: 156.8684 - val_mse: 156.8685\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 135.5828 - mse: 135.5828 - val_loss: 158.6557 - val_mse: 158.6557\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 135.6368 - mse: 135.6369 - val_loss: 159.2394 - val_mse: 159.2394\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 133.3022 - mse: 133.3022 - val_loss: 159.3171 - val_mse: 159.3171\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 133.3961 - mse: 133.3961 - val_loss: 156.6422 - val_mse: 156.6421\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 132.0205 - mse: 132.0205 - val_loss: 156.6038 - val_mse: 156.6038\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 137.7776 - mse: 137.7776 - val_loss: 159.7071 - val_mse: 159.7071\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 139.4122 - mse: 139.4122 - val_loss: 163.4104 - val_mse: 163.4104\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 142.5849 - mse: 142.5850 - val_loss: 159.0366 - val_mse: 159.0366\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 135.5498 - mse: 135.5498 - val_loss: 157.6357 - val_mse: 157.6357\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 133.3918 - mse: 133.3917 - val_loss: 153.9254 - val_mse: 153.9254\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 140.6131 - mse: 140.6131 - val_loss: 153.1668 - val_mse: 153.1668\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 127.9923 - mse: 127.9923 - val_loss: 157.1078 - val_mse: 157.1078\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 130.4072 - mse: 130.4072 - val_loss: 153.3286 - val_mse: 153.3286\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 131.8762 - mse: 131.8762 - val_loss: 153.6553 - val_mse: 153.6552\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 128.1408 - mse: 128.1408 - val_loss: 155.5609 - val_mse: 155.5609\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 133.5643 - mse: 133.5643 - val_loss: 162.0925 - val_mse: 162.0925\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 127.9847 - mse: 127.9847 - val_loss: 152.9851 - val_mse: 152.9851\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 101us/sample - loss: 128.2078 - mse: 128.2078 - val_loss: 157.0925 - val_mse: 157.0925\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 137.0589 - mse: 137.0589 - val_loss: 151.3724 - val_mse: 151.3725\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 129.3500 - mse: 129.3500 - val_loss: 151.2859 - val_mse: 151.2859\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 136.1170 - mse: 136.1170 - val_loss: 154.5735 - val_mse: 154.5735\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 130.5165 - mse: 130.5165 - val_loss: 148.5910 - val_mse: 148.5910\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 123.9236 - mse: 123.9237 - val_loss: 152.1342 - val_mse: 152.1342\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 128.3588 - mse: 128.3588 - val_loss: 151.1666 - val_mse: 151.1665\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 127.9849 - mse: 127.9849 - val_loss: 158.7631 - val_mse: 158.7631\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 126.1746 - mse: 126.1746 - val_loss: 158.9932 - val_mse: 158.9932\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 123.1889 - mse: 123.1889 - val_loss: 144.4345 - val_mse: 144.4345\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 129.1908 - mse: 129.1908 - val_loss: 146.3140 - val_mse: 146.3140\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.6502 - mse: 134.6502 - val_loss: 154.0726 - val_mse: 154.0726\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 117.7390 - mse: 117.7390 - val_loss: 146.7332 - val_mse: 146.7332\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 127.1032 - mse: 127.1032 - val_loss: 149.0440 - val_mse: 149.0440\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 123.0891 - mse: 123.0891 - val_loss: 145.2548 - val_mse: 145.2548\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 121.5948 - mse: 121.5948 - val_loss: 169.2843 - val_mse: 169.2843\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 131.7731 - mse: 131.7731 - val_loss: 159.0419 - val_mse: 159.0419\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 122.8483 - mse: 122.8484 - val_loss: 141.7301 - val_mse: 141.7301\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 124.1915 - mse: 124.1915 - val_loss: 151.0088 - val_mse: 151.0088\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 125.1214 - mse: 125.1215 - val_loss: 142.8456 - val_mse: 142.8456\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 120.6037 - mse: 120.6037 - val_loss: 160.5879 - val_mse: 160.5879\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 123.5735 - mse: 123.5735 - val_loss: 141.6365 - val_mse: 141.6365\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 124.8371 - mse: 124.8371 - val_loss: 140.2932 - val_mse: 140.2932\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 114.6102 - mse: 114.6102 - val_loss: 143.8336 - val_mse: 143.8336\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 121.6305 - mse: 121.6305 - val_loss: 142.4260 - val_mse: 142.4260\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 119.1978 - mse: 119.1978 - val_loss: 136.0204 - val_mse: 136.0204\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 118.6520 - mse: 118.6520 - val_loss: 135.6845 - val_mse: 135.6845\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 119.8677 - mse: 119.8677 - val_loss: 155.2934 - val_mse: 155.2934\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 121.1528 - mse: 121.1528 - val_loss: 149.8426 - val_mse: 149.8425\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 122.2670 - mse: 122.2671 - val_loss: 161.7445 - val_mse: 161.7445\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 119.0183 - mse: 119.0183 - val_loss: 128.7900 - val_mse: 128.7900\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 116.6448 - mse: 116.6448 - val_loss: 142.2639 - val_mse: 142.2639\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 124.7674 - mse: 124.7674 - val_loss: 150.0207 - val_mse: 150.0207\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 116.3854 - mse: 116.3854 - val_loss: 128.2721 - val_mse: 128.2721\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 120.9452 - mse: 120.9452 - val_loss: 125.4561 - val_mse: 125.4561\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 111.8741 - mse: 111.8742 - val_loss: 128.3699 - val_mse: 128.3699\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 111.8423 - mse: 111.8423 - val_loss: 130.7691 - val_mse: 130.7691\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 112.7080 - mse: 112.7080 - val_loss: 137.7864 - val_mse: 137.7864\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 120.3609 - mse: 120.3609 - val_loss: 128.1006 - val_mse: 128.1006\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 0s 155us/sample - loss: 120.7102 - mse: 120.7102 - val_loss: 126.4018 - val_mse: 126.4017\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 0s 133us/sample - loss: 112.4234 - mse: 112.4233 - val_loss: 135.9935 - val_mse: 135.9935\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 111.5212 - mse: 111.5212 - val_loss: 121.9234 - val_mse: 121.9234\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 112.8926 - mse: 112.8926 - val_loss: 130.2306 - val_mse: 130.2306\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 114.1106 - mse: 114.1106 - val_loss: 123.4762 - val_mse: 123.4762\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 109.7996 - mse: 109.7995 - val_loss: 135.4055 - val_mse: 135.4055\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 117.4775 - mse: 117.4775 - val_loss: 124.7744 - val_mse: 124.7744\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 110.6900 - mse: 110.690 - 0s 103us/sample - loss: 109.2591 - mse: 109.2591 - val_loss: 126.9552 - val_mse: 126.9552\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 111.0438 - mse: 111.0439 - val_loss: 116.7294 - val_mse: 116.7294\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 109.0454 - mse: 109.0454 - val_loss: 153.8656 - val_mse: 153.8656\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 113.4336 - mse: 113.4336 - val_loss: 117.2172 - val_mse: 117.2172\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 107.9572 - mse: 107.9572 - val_loss: 143.6759 - val_mse: 143.6759\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 112.2678 - mse: 112.2678 - val_loss: 114.4312 - val_mse: 114.4312\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 107.6056 - mse: 107.6055 - val_loss: 118.9541 - val_mse: 118.9541\n",
      "Epoch 119/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 108.9445 - mse: 108.9445 - val_loss: 123.9197 - val_mse: 123.9197\n",
      "Epoch 120/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 110.0102 - mse: 110.0102 - val_loss: 122.2515 - val_mse: 122.2515\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 106us/sample - loss: 109.6062 - mse: 109.6062 - val_loss: 126.8003 - val_mse: 126.8003\n",
      "Epoch 122/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 112.9889 - mse: 112.9889 - val_loss: 123.2769 - val_mse: 123.2768\n",
      "Epoch 123/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 105.2602 - mse: 105.2602 - val_loss: 111.9152 - val_mse: 111.9152\n",
      "Epoch 124/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 110.7956 - mse: 110.7956 - val_loss: 118.3928 - val_mse: 118.3928\n",
      "Epoch 125/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 111.4756 - mse: 111.4756 - val_loss: 113.4637 - val_mse: 113.4637\n",
      "Epoch 126/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 102.3515 - mse: 102.3514 - val_loss: 116.2274 - val_mse: 116.2274\n",
      "Epoch 127/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 103.5564 - mse: 103.5564 - val_loss: 113.3791 - val_mse: 113.3791\n",
      "Epoch 128/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 105.1035 - mse: 105.1035 - val_loss: 108.7083 - val_mse: 108.7083\n",
      "Epoch 129/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 108.0277 - mse: 108.0277 - val_loss: 124.3759 - val_mse: 124.3759\n",
      "Epoch 130/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 104.3206 - mse: 104.3206 - val_loss: 112.8175 - val_mse: 112.8175\n",
      "Epoch 131/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 102.2249 - mse: 102.2250 - val_loss: 120.9888 - val_mse: 120.9888\n",
      "Epoch 132/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 106.7509 - mse: 106.7508 - val_loss: 109.8227 - val_mse: 109.8226\n",
      "Epoch 133/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 106.7339 - mse: 106.7339 - val_loss: 133.2014 - val_mse: 133.2014\n",
      "Epoch 134/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 113.4796 - mse: 113.4796 - val_loss: 115.2931 - val_mse: 115.2931\n",
      "Epoch 135/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 103.2748 - mse: 103.2748 - val_loss: 131.2574 - val_mse: 131.2574\n",
      "Epoch 136/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 104.7662 - mse: 104.7662 - val_loss: 110.7233 - val_mse: 110.7233\n",
      "Epoch 137/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 105.4600 - mse: 105.4600 - val_loss: 123.6252 - val_mse: 123.6252\n",
      "Epoch 138/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 107.2743 - mse: 107.2743 - val_loss: 112.9009 - val_mse: 112.9009\n",
      "[CV] ...................................... nl=1, nn=24, total=  42.2s\n",
      "[CV] nl=2, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 302us/sample - loss: 540.2141 - mse: 540.2141 - val_loss: 682.7573 - val_mse: 682.7573\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 532.5815 - mse: 532.5815 - val_loss: 675.7736 - val_mse: 675.7737\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 523.8705 - mse: 523.8705 - val_loss: 662.9163 - val_mse: 662.9162\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 507.8507 - mse: 507.8508 - val_loss: 641.8580 - val_mse: 641.8580\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 480.3208 - mse: 480.3210 - val_loss: 635.3006 - val_mse: 635.3006\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 425.6034 - mse: 425.6034 - val_loss: 561.8221 - val_mse: 561.8220\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 382.0382 - mse: 382.0381 - val_loss: 489.4033 - val_mse: 489.4034\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 346.6290 - mse: 346.6290 - val_loss: 447.0824 - val_mse: 447.0825\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 318.9345 - mse: 318.9345 - val_loss: 427.3300 - val_mse: 427.3299\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 294.3024 - mse: 294.3024 - val_loss: 388.6015 - val_mse: 388.6015\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 275.7468 - mse: 275.7469 - val_loss: 356.2231 - val_mse: 356.2232\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 264.5282 - mse: 264.5282 - val_loss: 331.4264 - val_mse: 331.4264\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 251.4974 - mse: 251.4974 - val_loss: 313.9560 - val_mse: 313.9559\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 245.4741 - mse: 245.4742 - val_loss: 305.8073 - val_mse: 305.8073\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 235.5190 - mse: 235.5190 - val_loss: 292.7971 - val_mse: 292.7971\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 237.9763 - mse: 237.9764 - val_loss: 289.7949 - val_mse: 289.7950\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 224.1002 - mse: 224.1002 - val_loss: 291.2716 - val_mse: 291.2716\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 225.5210 - mse: 225.5210 - val_loss: 268.0444 - val_mse: 268.0444\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 214.1714 - mse: 214.1714 - val_loss: 257.9623 - val_mse: 257.9623\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 216.8763 - mse: 216.8763 - val_loss: 244.4596 - val_mse: 244.4596\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 205.5031 - mse: 205.5031 - val_loss: 255.9150 - val_mse: 255.9149\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 213.8428 - mse: 213.8428 - val_loss: 248.3880 - val_mse: 248.3880\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 209.2678 - mse: 209.2678 - val_loss: 233.9220 - val_mse: 233.9220\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 203.7489 - mse: 203.7489 - val_loss: 238.3763 - val_mse: 238.3763\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 205.0770 - mse: 205.0770 - val_loss: 241.4443 - val_mse: 241.4442\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 197.6053 - mse: 197.6053 - val_loss: 226.4777 - val_mse: 226.4777\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 199.5361 - mse: 199.5361 - val_loss: 220.6366 - val_mse: 220.6366\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 195.6490 - mse: 195.6490 - val_loss: 215.9300 - val_mse: 215.9300\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 191.2771 - mse: 191.2771 - val_loss: 237.6556 - val_mse: 237.6557\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 195.3109 - mse: 195.3109 - val_loss: 220.8523 - val_mse: 220.8523\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 193.7494 - mse: 193.7493 - val_loss: 223.7333 - val_mse: 223.7333\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 187.7410 - mse: 187.7410 - val_loss: 213.4639 - val_mse: 213.4640\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 192.4724 - mse: 192.4724 - val_loss: 241.2711 - val_mse: 241.2711\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 192.3141 - mse: 192.3140 - val_loss: 233.2284 - val_mse: 233.2284\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 189.5739 - mse: 189.5739 - val_loss: 199.7792 - val_mse: 199.7791\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 193.4672 - mse: 193.4672 - val_loss: 229.9524 - val_mse: 229.9524\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 187.9711 - mse: 187.9711 - val_loss: 213.9490 - val_mse: 213.9490\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 187.1543 - mse: 187.1543 - val_loss: 221.0658 - val_mse: 221.0658\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 184.4901 - mse: 184.4900 - val_loss: 223.9754 - val_mse: 223.9754\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 189.4423 - mse: 189.4423 - val_loss: 206.9308 - val_mse: 206.9308\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 179.7069 - mse: 179.7068 - val_loss: 198.3823 - val_mse: 198.3823\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 188.7305 - mse: 188.730 - 0s 116us/sample - loss: 184.9116 - mse: 184.9116 - val_loss: 214.9764 - val_mse: 214.9763\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 183.2506 - mse: 183.2506 - val_loss: 201.6666 - val_mse: 201.6666\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 183.7285 - mse: 183.7285 - val_loss: 224.8362 - val_mse: 224.8362\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 177.5789 - mse: 177.5789 - val_loss: 206.5140 - val_mse: 206.5140\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 178.4573 - mse: 178.4573 - val_loss: 196.3005 - val_mse: 196.3006\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 184.1673 - mse: 184.1673 - val_loss: 193.5244 - val_mse: 193.5244\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 189.4609 - mse: 189.4609 - val_loss: 199.7979 - val_mse: 199.7979\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 181.4056 - mse: 181.4056 - val_loss: 196.6178 - val_mse: 196.6178\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 179.3105 - mse: 179.3105 - val_loss: 557.7884 - val_mse: 557.7885\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 177.1939 - mse: 177.1940 - val_loss: 219.5685 - val_mse: 219.5685\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 176.1941 - mse: 176.1941 - val_loss: 196.4042 - val_mse: 196.4043\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 172.8537 - mse: 172.8536 - val_loss: 194.0314 - val_mse: 194.0314\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 174.3840 - mse: 174.3840 - val_loss: 243.1142 - val_mse: 243.1141\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 173.1114 - mse: 173.1114 - val_loss: 193.7744 - val_mse: 193.7744\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 176.4144 - mse: 176.4144 - val_loss: 193.5315 - val_mse: 193.5315\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 171.4141 - mse: 171.4140 - val_loss: 195.4718 - val_mse: 195.4718\n",
      "[CV] ....................................... nl=2, nn=2, total=  18.3s\n",
      "[CV] nl=2, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 317us/sample - loss: 485.6947 - mse: 485.6948 - val_loss: 680.4436 - val_mse: 680.4437\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 475.4097 - mse: 475.4099 - val_loss: 665.8807 - val_mse: 665.8807\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 459.6787 - mse: 459.6787 - val_loss: 646.2185 - val_mse: 646.2185\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 440.6372 - mse: 440.6371 - val_loss: 623.8461 - val_mse: 623.8461\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 420.2055 - mse: 420.2054 - val_loss: 600.8584 - val_mse: 600.8584\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 400.8402 - mse: 400.8402 - val_loss: 579.8151 - val_mse: 579.8151\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 99us/sample - loss: 384.0628 - mse: 384.0627 - val_loss: 561.8722 - val_mse: 561.8721\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 370.6166 - mse: 370.6167 - val_loss: 547.7169 - val_mse: 547.7170\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 96us/sample - loss: 360.4955 - mse: 360.4954 - val_loss: 537.0193 - val_mse: 537.0193\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.5559 - mse: 353.5560 - val_loss: 529.4567 - val_mse: 529.4568\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 349.1666 - mse: 349.1666 - val_loss: 524.2946 - val_mse: 524.2947\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 346.5466 - mse: 346.5466 - val_loss: 521.2244 - val_mse: 521.2244\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 345.0324 - mse: 345.0325 - val_loss: 519.0417 - val_mse: 519.0419\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 344.2498 - mse: 344.2498 - val_loss: 517.8158 - val_mse: 517.8160\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.8342 - mse: 343.8343 - val_loss: 517.0400 - val_mse: 517.0400\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 343.6532 - mse: 343.6532 - val_loss: 516.6226 - val_mse: 516.6227\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.5341 - mse: 343.5341 - val_loss: 516.2673 - val_mse: 516.2672\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4875 - mse: 343.4875 - val_loss: 516.0315 - val_mse: 516.0314\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4560 - mse: 343.4560 - val_loss: 515.9038 - val_mse: 515.9038\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4585 - mse: 343.4584 - val_loss: 515.7623 - val_mse: 515.7623\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4570 - mse: 343.4569 - val_loss: 515.7312 - val_mse: 515.7313\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4449 - mse: 343.4449 - val_loss: 515.7044 - val_mse: 515.7045\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 343.4544 - mse: 343.4543 - val_loss: 515.7322 - val_mse: 515.7324\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4578 - mse: 343.4577 - val_loss: 515.6824 - val_mse: 515.6824\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4705 - mse: 343.4706 - val_loss: 515.6495 - val_mse: 515.6497\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4736 - mse: 343.4735 - val_loss: 515.7109 - val_mse: 515.7109\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 343.4434 - mse: 343.4433 - val_loss: 515.7429 - val_mse: 515.7429\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 343.4748 - mse: 343.4749 - val_loss: 515.7241 - val_mse: 515.7242\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 343.4602 - mse: 343.4601 - val_loss: 515.7459 - val_mse: 515.7458\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 343.4468 - mse: 343.4467 - val_loss: 515.7135 - val_mse: 515.7134\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4494 - mse: 343.4493 - val_loss: 515.6899 - val_mse: 515.6898\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4659 - mse: 343.4659 - val_loss: 515.6895 - val_mse: 515.6895\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4913 - mse: 343.4913 - val_loss: 515.7539 - val_mse: 515.7539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4651 - mse: 343.4650 - val_loss: 515.7351 - val_mse: 515.7352\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4523 - mse: 343.4523 - val_loss: 515.6348 - val_mse: 515.6349\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4749 - mse: 343.4749 - val_loss: 515.6181 - val_mse: 515.6182\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4631 - mse: 343.4631 - val_loss: 515.5140 - val_mse: 515.5140\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 343.4719 - mse: 343.4720 - val_loss: 515.5487 - val_mse: 515.5486\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4537 - mse: 343.4537 - val_loss: 515.5181 - val_mse: 515.5181\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4637 - mse: 343.4638 - val_loss: 515.6488 - val_mse: 515.6487\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4799 - mse: 343.4800 - val_loss: 515.6748 - val_mse: 515.6748\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 343.4590 - mse: 343.4590 - val_loss: 515.6354 - val_mse: 515.6354\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4570 - mse: 343.4570 - val_loss: 515.6204 - val_mse: 515.6203\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4735 - mse: 343.4735 - val_loss: 515.5528 - val_mse: 515.5527\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 358.7021 - mse: 358.702 - 0s 107us/sample - loss: 343.4901 - mse: 343.4900 - val_loss: 515.6126 - val_mse: 515.6127\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4543 - mse: 343.4542 - val_loss: 515.6467 - val_mse: 515.6467\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 343.4534 - mse: 343.4534 - val_loss: 515.6539 - val_mse: 515.6539\n",
      "[CV] ....................................... nl=2, nn=2, total=  15.3s\n",
      "[CV] nl=2, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 313us/sample - loss: 547.7884 - mse: 547.7883 - val_loss: 671.8969 - val_mse: 671.8970\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 522.9464 - mse: 522.9465 - val_loss: 641.8491 - val_mse: 641.8491\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 495.4352 - mse: 495.4355 - val_loss: 588.5617 - val_mse: 588.5616\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 464.7626 - mse: 464.7625 - val_loss: 558.7332 - val_mse: 558.7332\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 430.8132 - mse: 430.8132 - val_loss: 505.3742 - val_mse: 505.3742\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 402.0165 - mse: 402.0166 - val_loss: 468.3503 - val_mse: 468.3503\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 373.1702 - mse: 373.1701 - val_loss: 437.1666 - val_mse: 437.1667\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 346.8277 - mse: 346.8276 - val_loss: 424.9726 - val_mse: 424.9725\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 324.2719 - mse: 324.2719 - val_loss: 392.2648 - val_mse: 392.2649\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 304.9988 - mse: 304.9987 - val_loss: 367.6092 - val_mse: 367.6093\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 286.6944 - mse: 286.6944 - val_loss: 335.2805 - val_mse: 335.2805\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 280.2909 - mse: 280.2909 - val_loss: 318.3113 - val_mse: 318.3112\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 265.6286 - mse: 265.6286 - val_loss: 306.9015 - val_mse: 306.9015\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 266.6852 - mse: 266.685 - 0s 104us/sample - loss: 257.3532 - mse: 257.3531 - val_loss: 299.8867 - val_mse: 299.8868\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 248.4187 - mse: 248.4186 - val_loss: 277.0813 - val_mse: 277.0813\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 240.1763 - mse: 240.1763 - val_loss: 279.8355 - val_mse: 279.8354\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 234.5041 - mse: 234.5041 - val_loss: 267.7837 - val_mse: 267.7838\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 229.0816 - mse: 229.0816 - val_loss: 266.6747 - val_mse: 266.6747\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 224.8666 - mse: 224.8665 - val_loss: 255.4391 - val_mse: 255.4391\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 224.7464 - mse: 224.7464 - val_loss: 267.8839 - val_mse: 267.8839\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 219.9639 - mse: 219.9640 - val_loss: 234.6831 - val_mse: 234.6831\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 220.2527 - mse: 220.2527 - val_loss: 229.6234 - val_mse: 229.6234\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 220.5368 - mse: 220.5368 - val_loss: 225.1774 - val_mse: 225.1774\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 210.3240 - mse: 210.3241 - val_loss: 216.9909 - val_mse: 216.9909\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 203.7847 - mse: 203.7847 - val_loss: 224.6993 - val_mse: 224.6993\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 210.9636 - mse: 210.9637 - val_loss: 229.7693 - val_mse: 229.7693\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 202.3393 - mse: 202.3392 - val_loss: 218.2679 - val_mse: 218.2679\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 205.5910 - mse: 205.5910 - val_loss: 211.9419 - val_mse: 211.9419\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 206.8649 - mse: 206.8650 - val_loss: 211.0638 - val_mse: 211.0638\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 202.0674 - mse: 202.0673 - val_loss: 200.9887 - val_mse: 200.9887\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 206.5610 - mse: 206.5609 - val_loss: 242.8840 - val_mse: 242.8840\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 205.2773 - mse: 205.2774 - val_loss: 218.7900 - val_mse: 218.7900\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 198.0382 - mse: 198.0382 - val_loss: 197.6056 - val_mse: 197.6056\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 205.9857 - mse: 205.9858 - val_loss: 219.1120 - val_mse: 219.1119\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 196.6597 - mse: 196.6598 - val_loss: 208.2580 - val_mse: 208.2580\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 198.2500 - mse: 198.2500 - val_loss: 213.0854 - val_mse: 213.0854\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 201.6996 - mse: 201.6996 - val_loss: 192.2693 - val_mse: 192.2694\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 197.6931 - mse: 197.6931 - val_loss: 212.5813 - val_mse: 212.5813\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 199.9342 - mse: 199.9343 - val_loss: 240.8991 - val_mse: 240.8992\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 206.6445 - mse: 206.6445 - val_loss: 199.3670 - val_mse: 199.3670\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 191.5529 - mse: 191.5529 - val_loss: 189.9209 - val_mse: 189.9209\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 198.4579 - mse: 198.4579 - val_loss: 192.6140 - val_mse: 192.6140\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 183.8220 - mse: 183.8219 - val_loss: 191.9610 - val_mse: 191.9609\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 188.0257 - mse: 188.0257 - val_loss: 192.3841 - val_mse: 192.3841\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 191.1672 - mse: 191.1672 - val_loss: 192.1015 - val_mse: 192.1015\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 192.1949 - mse: 192.1949 - val_loss: 186.3230 - val_mse: 186.3230\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 187.1037 - mse: 187.1037 - val_loss: 204.1601 - val_mse: 204.1601\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 184.0609 - mse: 184.0609 - val_loss: 188.5209 - val_mse: 188.5209\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 186.6956 - mse: 186.695 - 0s 108us/sample - loss: 188.8900 - mse: 188.8900 - val_loss: 194.0474 - val_mse: 194.0474\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 190.5388 - mse: 190.5387 - val_loss: 195.3399 - val_mse: 195.3400\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 189.4801 - mse: 189.4801 - val_loss: 179.9867 - val_mse: 179.9867\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 189.9352 - mse: 189.9352 - val_loss: 205.7021 - val_mse: 205.7021\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 190.6276 - mse: 190.6276 - val_loss: 185.2102 - val_mse: 185.2102\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 188.5933 - mse: 188.5933 - val_loss: 182.4964 - val_mse: 182.4964\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 183.6705 - mse: 183.6705 - val_loss: 204.0709 - val_mse: 204.0709\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 185.8741 - mse: 185.8740 - val_loss: 179.0699 - val_mse: 179.0698\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 180.0682 - mse: 180.0683 - val_loss: 182.2967 - val_mse: 182.2967\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 175.3348 - mse: 175.3348 - val_loss: 193.3178 - val_mse: 193.3178\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 179.7911 - mse: 179.7911 - val_loss: 178.7350 - val_mse: 178.7350\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 180.8927 - mse: 180.8927 - val_loss: 186.1426 - val_mse: 186.1426\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 175.6676 - mse: 175.6676 - val_loss: 181.2412 - val_mse: 181.2412\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 177.5211 - mse: 177.5210 - val_loss: 177.8208 - val_mse: 177.8208\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 184.1261 - mse: 184.1261 - val_loss: 179.1444 - val_mse: 179.1444\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 178.9403 - mse: 178.9403 - val_loss: 186.0461 - val_mse: 186.0461\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 171.8176 - mse: 171.8175 - val_loss: 177.5924 - val_mse: 177.5924\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 175.0494 - mse: 175.0493 - val_loss: 180.7910 - val_mse: 180.7910\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 181.3306 - mse: 181.3306 - val_loss: 190.9247 - val_mse: 190.9247\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 178.6539 - mse: 178.6539 - val_loss: 177.6277 - val_mse: 177.6277\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 180.5824 - mse: 180.5824 - val_loss: 183.2605 - val_mse: 183.2605\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 174.8252 - mse: 174.8252 - val_loss: 177.7823 - val_mse: 177.7823\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 173.8978 - mse: 173.8979 - val_loss: 175.7510 - val_mse: 175.7509\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 177.8092 - mse: 177.8093 - val_loss: 180.4131 - val_mse: 180.4130\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 172.6920 - mse: 172.6920 - val_loss: 192.7736 - val_mse: 192.7736\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 169.8178 - mse: 169.8179 - val_loss: 183.8954 - val_mse: 183.8954\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 168.4937 - mse: 168.4937 - val_loss: 179.5928 - val_mse: 179.5928\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 170.5528 - mse: 170.5529 - val_loss: 179.2075 - val_mse: 179.2075\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 174.1483 - mse: 174.1482 - val_loss: 172.5620 - val_mse: 172.5620\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 176.2174 - mse: 176.2174 - val_loss: 183.7090 - val_mse: 183.7090\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 177.0437 - mse: 177.0438 - val_loss: 196.3089 - val_mse: 196.3088\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 174.6324 - mse: 174.6324 - val_loss: 171.5099 - val_mse: 171.5099\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 173.2877 - mse: 173.2878 - val_loss: 179.8380 - val_mse: 179.8380\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 167.2146 - mse: 167.2146 - val_loss: 172.4664 - val_mse: 172.4664\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 170.9237 - mse: 170.9237 - val_loss: 171.4937 - val_mse: 171.4937\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 178.2491 - mse: 178.2491 - val_loss: 190.7486 - val_mse: 190.7487\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 168.9230 - mse: 168.9230 - val_loss: 175.3536 - val_mse: 175.3536\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 172.4156 - mse: 172.4155 - val_loss: 183.2169 - val_mse: 183.2169\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 175.5634 - mse: 175.5634 - val_loss: 180.8200 - val_mse: 180.8199\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 176.2997 - mse: 176.2998 - val_loss: 173.3529 - val_mse: 173.3529\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 179.0939 - mse: 179.0939 - val_loss: 172.7630 - val_mse: 172.7630\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 177.7004 - mse: 177.7004 - val_loss: 176.2754 - val_mse: 176.2753\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 166.0098 - mse: 166.0098 - val_loss: 178.6522 - val_mse: 178.6522\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 167.8010 - mse: 167.8010 - val_loss: 172.0491 - val_mse: 172.0491\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 169.5352 - mse: 169.5352 - val_loss: 189.4793 - val_mse: 189.4794\n",
      "[CV] ....................................... nl=2, nn=2, total=  29.3s\n",
      "[CV] nl=2, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 309us/sample - loss: 478.0517 - mse: 478.0517 - val_loss: 541.4487 - val_mse: 541.4488\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 457.5898 - mse: 457.5898 - val_loss: 512.0566 - val_mse: 512.0565\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 433.3386 - mse: 433.3385 - val_loss: 473.5313 - val_mse: 473.5313\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 405.0419 - mse: 405.0419 - val_loss: 426.2392 - val_mse: 426.2392\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 373.8320 - mse: 373.8320 - val_loss: 376.0977 - val_mse: 376.0977\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 342.8396 - mse: 342.8396 - val_loss: 352.9827 - val_mse: 352.9827\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 313.5750 - mse: 313.5750 - val_loss: 332.3494 - val_mse: 332.3495\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 288.5943 - mse: 288.5942 - val_loss: 294.8255 - val_mse: 294.8254\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 268.8365 - mse: 268.8364 - val_loss: 271.4924 - val_mse: 271.4923\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 254.1969 - mse: 254.1968 - val_loss: 263.8036 - val_mse: 263.8036\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 238.9967 - mse: 238.9967 - val_loss: 238.5644 - val_mse: 238.5644\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 222.3935 - mse: 222.3934 - val_loss: 224.0964 - val_mse: 224.0964\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 217.4664 - mse: 217.4664 - val_loss: 216.0881 - val_mse: 216.0881\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 205.7153 - mse: 205.7153 - val_loss: 188.6719 - val_mse: 188.6718\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 201.0245 - mse: 201.0244 - val_loss: 187.5971 - val_mse: 187.5970\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 197.7397 - mse: 197.7396 - val_loss: 183.1729 - val_mse: 183.1728\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 191.5578 - mse: 191.5577 - val_loss: 182.7123 - val_mse: 182.7123\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 189.8218 - mse: 189.8219 - val_loss: 166.5784 - val_mse: 166.5784\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 184.2491 - mse: 184.2491 - val_loss: 169.1786 - val_mse: 169.1786\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 184.3932 - mse: 184.3932 - val_loss: 156.3304 - val_mse: 156.3304\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 184.9339 - mse: 184.9339 - val_loss: 162.2458 - val_mse: 162.2458\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 185.3750 - mse: 185.3750 - val_loss: 169.1316 - val_mse: 169.1316\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 179.1716 - mse: 179.1716 - val_loss: 149.9376 - val_mse: 149.9376\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 177.2751 - mse: 177.2751 - val_loss: 150.5616 - val_mse: 150.5616\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 171.0862 - mse: 171.0862 - val_loss: 146.6883 - val_mse: 146.6883\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 174.7861 - mse: 174.7860 - val_loss: 144.4637 - val_mse: 144.4637\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 168.1571 - mse: 168.1570 - val_loss: 140.8244 - val_mse: 140.8244\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 170.2015 - mse: 170.2015 - val_loss: 141.3734 - val_mse: 141.3734\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 163.5623 - mse: 163.5624 - val_loss: 143.0868 - val_mse: 143.0868\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 169.3419 - mse: 169.3419 - val_loss: 140.9371 - val_mse: 140.9371\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 170.5934 - mse: 170.5934 - val_loss: 142.2843 - val_mse: 142.2842\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 163.8497 - mse: 163.8497 - val_loss: 133.2605 - val_mse: 133.2605\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 161.5197 - mse: 161.5197 - val_loss: 132.1210 - val_mse: 132.1210\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 164.1986 - mse: 164.1986 - val_loss: 135.3998 - val_mse: 135.3998\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 170.1408 - mse: 170.1408 - val_loss: 131.6350 - val_mse: 131.6350\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 165.1026 - mse: 165.1026 - val_loss: 138.9605 - val_mse: 138.9604\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 160.9131 - mse: 160.9132 - val_loss: 129.7060 - val_mse: 129.7060\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 166.0837 - mse: 166.0837 - val_loss: 133.5155 - val_mse: 133.5154\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 164.1283 - mse: 164.1283 - val_loss: 129.8350 - val_mse: 129.8350\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 160.5051 - mse: 160.5051 - val_loss: 126.7766 - val_mse: 126.7766\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 157.9704 - mse: 157.9704 - val_loss: 129.8320 - val_mse: 129.8320\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 164.9065 - mse: 164.9064 - val_loss: 127.6120 - val_mse: 127.6120\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 165.2705 - mse: 165.2705 - val_loss: 129.7158 - val_mse: 129.7158\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 157.8070 - mse: 157.8070 - val_loss: 125.6741 - val_mse: 125.6741\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 157.3892 - mse: 157.3891 - val_loss: 127.2044 - val_mse: 127.2044\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 155.9562 - mse: 155.9562 - val_loss: 129.2655 - val_mse: 129.2655\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 153.2383 - mse: 153.2383 - val_loss: 125.5917 - val_mse: 125.5916\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 158.4995 - mse: 158.4994 - val_loss: 126.9681 - val_mse: 126.9681\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 157.2726 - mse: 157.2726 - val_loss: 130.7552 - val_mse: 130.7552\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 152.3638 - mse: 152.3638 - val_loss: 126.4378 - val_mse: 126.4378\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 156.7499 - mse: 156.7499 - val_loss: 126.4522 - val_mse: 126.4522\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 148.7943 - mse: 148.7943 - val_loss: 122.3744 - val_mse: 122.3744\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 150.8980 - mse: 150.8980 - val_loss: 124.8207 - val_mse: 124.8207\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 134us/sample - loss: 151.2492 - mse: 151.2493 - val_loss: 124.9330 - val_mse: 124.9330\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 149.2472 - mse: 149.2472 - val_loss: 123.1293 - val_mse: 123.1293\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 151.2401 - mse: 151.2401 - val_loss: 127.9240 - val_mse: 127.9239\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 153.4315 - mse: 153.4316 - val_loss: 124.2160 - val_mse: 124.2160\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.4800 - mse: 151.4800 - val_loss: 124.7683 - val_mse: 124.7683\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 146.1740 - mse: 146.1740 - val_loss: 130.8395 - val_mse: 130.8395\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 154.2561 - mse: 154.2561 - val_loss: 122.0858 - val_mse: 122.0858\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 148.6896 - mse: 148.6896 - val_loss: 122.2909 - val_mse: 122.2909\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 150.7751 - mse: 150.7751 - val_loss: 122.3075 - val_mse: 122.3075\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 154.2385 - mse: 154.2385 - val_loss: 124.9958 - val_mse: 124.9958\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 149.0114 - mse: 149.0114 - val_loss: 121.8921 - val_mse: 121.8921\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 150.1630 - mse: 150.1629 - val_loss: 125.0758 - val_mse: 125.0758\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 144.1694 - mse: 144.1694 - val_loss: 121.0504 - val_mse: 121.0504\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 148.9633 - mse: 148.9633 - val_loss: 121.4013 - val_mse: 121.4013\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 149.4240 - mse: 149.4240 - val_loss: 121.3112 - val_mse: 121.3112\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 150.8756 - mse: 150.8757 - val_loss: 122.5041 - val_mse: 122.5041\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.8874 - mse: 142.8874 - val_loss: 121.9624 - val_mse: 121.9624\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 141.0108 - mse: 141.0108 - val_loss: 121.5818 - val_mse: 121.5817\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 154.4276 - mse: 154.4276 - val_loss: 120.2231 - val_mse: 120.2230\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 147.5491 - mse: 147.5491 - val_loss: 125.8589 - val_mse: 125.8589\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 150.2537 - mse: 150.2537 - val_loss: 120.5949 - val_mse: 120.5949\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 144.2394 - mse: 144.2394 - val_loss: 123.4602 - val_mse: 123.4602\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.8849 - mse: 141.8849 - val_loss: 118.8990 - val_mse: 118.8990\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.1562 - mse: 151.1562 - val_loss: 119.9145 - val_mse: 119.9145\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 140.2861 - mse: 140.2861 - val_loss: 119.3706 - val_mse: 119.3707\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 139.8740 - mse: 139.8739 - val_loss: 120.2676 - val_mse: 120.2676\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 146.2350 - mse: 146.2351 - val_loss: 120.5380 - val_mse: 120.5380\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 144.1810 - mse: 144.1810 - val_loss: 120.7123 - val_mse: 120.7123\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 144.0571 - mse: 144.0571 - val_loss: 126.2859 - val_mse: 126.2859\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 145.8806 - mse: 145.8806 - val_loss: 123.1357 - val_mse: 123.1357\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 141.0391 - mse: 141.0390 - val_loss: 120.2875 - val_mse: 120.2876\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 144.1860 - mse: 144.1859 - val_loss: 119.0408 - val_mse: 119.0408\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 140.7271 - mse: 140.7271 - val_loss: 119.3422 - val_mse: 119.3422\n",
      "[CV] ....................................... nl=2, nn=2, total=  27.1s\n",
      "[CV] nl=2, nn=2 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 308us/sample - loss: 489.5944 - mse: 489.5943 - val_loss: 641.8635 - val_mse: 641.8635\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 473.1979 - mse: 473.1979 - val_loss: 622.2701 - val_mse: 622.2701\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 454.1381 - mse: 454.1382 - val_loss: 599.8719 - val_mse: 599.8719\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 433.7980 - mse: 433.7979 - val_loss: 577.0976 - val_mse: 577.0976\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 414.3716 - mse: 414.3716 - val_loss: 555.7775 - val_mse: 555.7775\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 397.1171 - mse: 397.1172 - val_loss: 537.3045 - val_mse: 537.3044\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 382.9297 - mse: 382.9297 - val_loss: 522.3508 - val_mse: 522.3509\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 138us/sample - loss: 372.0883 - mse: 372.0883 - val_loss: 510.7694 - val_mse: 510.7693\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 364.4655 - mse: 364.4656 - val_loss: 502.3624 - val_mse: 502.3625\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 173us/sample - loss: 359.5297 - mse: 359.5298 - val_loss: 497.1311 - val_mse: 497.1310\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 158us/sample - loss: 356.5648 - mse: 356.5648 - val_loss: 493.4156 - val_mse: 493.4156\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 354.9165 - mse: 354.9164 - val_loss: 491.3351 - val_mse: 491.3351\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 354.0475 - mse: 354.0476 - val_loss: 489.6635 - val_mse: 489.6635\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 353.5934 - mse: 353.5934 - val_loss: 488.9960 - val_mse: 488.9960\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.3816 - mse: 353.3816 - val_loss: 488.4832 - val_mse: 488.4831\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 353.2776 - mse: 353.2776 - val_loss: 488.0344 - val_mse: 488.0345\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 353.2508 - mse: 353.2507 - val_loss: 487.8849 - val_mse: 487.8849\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 353.2203 - mse: 353.2203 - val_loss: 487.8190 - val_mse: 487.8189\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 141us/sample - loss: 353.2127 - mse: 353.2126 - val_loss: 487.7627 - val_mse: 487.7628\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 353.2143 - mse: 353.2143 - val_loss: 487.7747 - val_mse: 487.7747\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.1916 - mse: 353.1917 - val_loss: 487.6312 - val_mse: 487.6312\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 353.1944 - mse: 353.1943 - val_loss: 487.5514 - val_mse: 487.5513\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 151us/sample - loss: 353.2532 - mse: 353.2534 - val_loss: 487.6335 - val_mse: 487.6335\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 143us/sample - loss: 353.1978 - mse: 353.1977 - val_loss: 487.6035 - val_mse: 487.6036\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.1981 - mse: 353.1980 - val_loss: 487.5562 - val_mse: 487.5561\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.2261 - mse: 353.2260 - val_loss: 487.5283 - val_mse: 487.5282\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 353.1961 - mse: 353.1961 - val_loss: 487.6684 - val_mse: 487.6684\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.2120 - mse: 353.2119 - val_loss: 487.6177 - val_mse: 487.6176\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 353.2006 - mse: 353.2007 - val_loss: 487.4664 - val_mse: 487.4665\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.1938 - mse: 353.1937 - val_loss: 487.5504 - val_mse: 487.5504\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 353.2053 - mse: 353.2052 - val_loss: 487.5812 - val_mse: 487.5813\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 353.1957 - mse: 353.1957 - val_loss: 487.5942 - val_mse: 487.5943\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 353.1811 - mse: 353.1811 - val_loss: 487.5151 - val_mse: 487.5152\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.1796 - mse: 353.1796 - val_loss: 487.5453 - val_mse: 487.5453\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 353.2050 - mse: 353.2049 - val_loss: 487.5057 - val_mse: 487.5058\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.1874 - mse: 353.1874 - val_loss: 487.5835 - val_mse: 487.5835\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.2120 - mse: 353.2119 - val_loss: 487.5743 - val_mse: 487.5743\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.1933 - mse: 353.1933 - val_loss: 487.5047 - val_mse: 487.5047\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 353.2142 - mse: 353.2142 - val_loss: 487.5507 - val_mse: 487.5507\n",
      "[CV] ....................................... nl=2, nn=2, total=  13.8s\n",
      "[CV] nl=2, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 452us/sample - loss: 499.3598 - mse: 499.3597 - val_loss: 663.9154 - val_mse: 663.9155\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 435.1556 - mse: 435.1556 - val_loss: 591.3163 - val_mse: 591.3164\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 371.1209 - mse: 371.1208 - val_loss: 475.4119 - val_mse: 475.4119\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 327.5551 - mse: 327.5551 - val_loss: 383.1569 - val_mse: 383.1568\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 133us/sample - loss: 291.2242 - mse: 291.2242 - val_loss: 337.4287 - val_mse: 337.4287\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 254.8641 - mse: 254.8641 - val_loss: 289.1201 - val_mse: 289.1201\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 236.9767 - mse: 236.9767 - val_loss: 252.9318 - val_mse: 252.9318\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 216.4994 - mse: 216.4994 - val_loss: 246.9891 - val_mse: 246.9891\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 198.8031 - mse: 198.8030 - val_loss: 221.1340 - val_mse: 221.1340\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 192.9520 - mse: 192.9520 - val_loss: 219.5137 - val_mse: 219.5136\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 188.5748 - mse: 188.5748 - val_loss: 201.0559 - val_mse: 201.0559\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 182.3824 - mse: 182.3823 - val_loss: 208.6883 - val_mse: 208.6882\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 183.3260 - mse: 183.3260 - val_loss: 195.0373 - val_mse: 195.0374\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 180.0427 - mse: 180.0428 - val_loss: 199.0564 - val_mse: 199.0563\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 177.2837 - mse: 177.2837 - val_loss: 187.4412 - val_mse: 187.4412\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 174.8100 - mse: 174.8100 - val_loss: 193.6368 - val_mse: 193.6368\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 171.3127 - mse: 171.3127 - val_loss: 190.2176 - val_mse: 190.2175\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 185.3722 - mse: 185.3722 - val_loss: 185.7024 - val_mse: 185.7025\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 183.7555 - mse: 183.7555 - val_loss: 190.1619 - val_mse: 190.1619\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 171.7473 - mse: 171.7473 - val_loss: 200.7704 - val_mse: 200.7704\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 175.1068 - mse: 175.1069 - val_loss: 205.2233 - val_mse: 205.2233\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 167.6113 - mse: 167.6113 - val_loss: 188.9312 - val_mse: 188.9312\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 172.8130 - mse: 172.8130 - val_loss: 181.2382 - val_mse: 181.2382\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 161.5267 - mse: 161.5267 - val_loss: 177.9047 - val_mse: 177.9047\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 156.8586 - mse: 156.8585 - val_loss: 190.5759 - val_mse: 190.5759\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 158.1523 - mse: 158.1524 - val_loss: 181.5173 - val_mse: 181.5173\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 156.6211 - mse: 156.6211 - val_loss: 173.4219 - val_mse: 173.4219\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 158.6183 - mse: 158.6183 - val_loss: 180.1487 - val_mse: 180.1487\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 159.2060 - mse: 159.2060 - val_loss: 174.0974 - val_mse: 174.0975\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 155.9531 - mse: 155.9531 - val_loss: 178.1993 - val_mse: 178.1992\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 154.4060 - mse: 154.4060 - val_loss: 178.4932 - val_mse: 178.4932\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 152.7951 - mse: 152.7950 - val_loss: 175.4227 - val_mse: 175.4227\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 159.1568 - mse: 159.1568 - val_loss: 214.7321 - val_mse: 214.7321\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 155.8871 - mse: 155.8871 - val_loss: 183.5561 - val_mse: 183.5561\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 164.6409 - mse: 164.6410 - val_loss: 174.0208 - val_mse: 174.0208\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 155.6089 - mse: 155.6089 - val_loss: 177.1693 - val_mse: 177.1693\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 151.5012 - mse: 151.5012 - val_loss: 189.3800 - val_mse: 189.3800\n",
      "[CV] ....................................... nl=2, nn=3, total=  12.5s\n",
      "[CV] nl=2, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 1s 310us/sample - loss: 482.6455 - mse: 482.6454 - val_loss: 673.9646 - val_mse: 673.9646\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 466.1199 - mse: 466.1200 - val_loss: 652.0058 - val_mse: 652.0060\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 443.7788 - mse: 443.7789 - val_loss: 625.0633 - val_mse: 625.0633\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 132us/sample - loss: 419.1807 - mse: 419.1808 - val_loss: 597.3978 - val_mse: 597.3979\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 396.0937 - mse: 396.0938 - val_loss: 572.5124 - val_mse: 572.5125\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 377.0212 - mse: 377.0212 - val_loss: 552.6888 - val_mse: 552.6889\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 363.0834 - mse: 363.0834 - val_loss: 538.5752 - val_mse: 538.5753\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.8855 - mse: 353.8855 - val_loss: 528.8351 - val_mse: 528.8352\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 348.5307 - mse: 348.5308 - val_loss: 523.1075 - val_mse: 523.1075\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 345.7635 - mse: 345.7636 - val_loss: 519.8742 - val_mse: 519.8743\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 344.4576 - mse: 344.4575 - val_loss: 518.1377 - val_mse: 518.1378\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.8428 - mse: 343.8427 - val_loss: 516.9827 - val_mse: 516.9828\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.6463 - mse: 343.6463 - val_loss: 516.4433 - val_mse: 516.4432\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 343.5194 - mse: 343.5195 - val_loss: 516.0950 - val_mse: 516.0950\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 343.4812 - mse: 343.4814 - val_loss: 515.8938 - val_mse: 515.8937\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4746 - mse: 343.4747 - val_loss: 515.6933 - val_mse: 515.6932\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 343.4721 - mse: 343.4721 - val_loss: 515.7312 - val_mse: 515.7313\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 343.4879 - mse: 343.4880 - val_loss: 515.7321 - val_mse: 515.7322\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4691 - mse: 343.4690 - val_loss: 515.7277 - val_mse: 515.7277\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4517 - mse: 343.4517 - val_loss: 515.7280 - val_mse: 515.7280\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4557 - mse: 343.4556 - val_loss: 515.7551 - val_mse: 515.7551\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4697 - mse: 343.4697 - val_loss: 515.7442 - val_mse: 515.7441\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4630 - mse: 343.4629 - val_loss: 515.7217 - val_mse: 515.7217\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4505 - mse: 343.4504 - val_loss: 515.7039 - val_mse: 515.7039\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4767 - mse: 343.4767 - val_loss: 515.7527 - val_mse: 515.7527\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4724 - mse: 343.4724 - val_loss: 515.6316 - val_mse: 515.6315\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4540 - mse: 343.4541 - val_loss: 515.6461 - val_mse: 515.6462\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4576 - mse: 343.4577 - val_loss: 515.6747 - val_mse: 515.6746\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 343.4656 - mse: 343.4655 - val_loss: 515.6908 - val_mse: 515.6909\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4507 - mse: 343.4507 - val_loss: 515.6865 - val_mse: 515.6864\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 343.4598 - mse: 343.4598 - val_loss: 515.6520 - val_mse: 515.6521\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4934 - mse: 343.4933 - val_loss: 515.6009 - val_mse: 515.6009\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4730 - mse: 343.4730 - val_loss: 515.6260 - val_mse: 515.6261\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 343.4776 - mse: 343.4777 - val_loss: 515.6675 - val_mse: 515.6674\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4709 - mse: 343.4710 - val_loss: 515.6548 - val_mse: 515.6547\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 343.4459 - mse: 343.4460 - val_loss: 515.5729 - val_mse: 515.5729\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 343.4624 - mse: 343.4624 - val_loss: 515.6193 - val_mse: 515.6193\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 343.4600 - mse: 343.4599 - val_loss: 515.7133 - val_mse: 515.7134\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 343.4736 - mse: 343.4736 - val_loss: 515.6635 - val_mse: 515.6635\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4540 - mse: 343.4540 - val_loss: 515.6321 - val_mse: 515.6323\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4646 - mse: 343.4646 - val_loss: 515.7278 - val_mse: 515.7278\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4478 - mse: 343.4479 - val_loss: 515.6108 - val_mse: 515.6108\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 343.4614 - mse: 343.4614 - val_loss: 515.7283 - val_mse: 515.7283\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 343.4545 - mse: 343.4544 - val_loss: 515.7441 - val_mse: 515.7440\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 343.4698 - mse: 343.4698 - val_loss: 515.6835 - val_mse: 515.6836\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 343.4648 - mse: 343.4647 - val_loss: 515.7188 - val_mse: 515.7189\n",
      "[CV] ....................................... nl=2, nn=3, total=  14.9s\n",
      "[CV] nl=2, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 308us/sample - loss: 550.6908 - mse: 550.6907 - val_loss: 677.6055 - val_mse: 677.6056\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 516.9839 - mse: 516.9839 - val_loss: 646.1232 - val_mse: 646.1230\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 474.4597 - mse: 474.4598 - val_loss: 571.0191 - val_mse: 571.0192\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 429.0716 - mse: 429.0714 - val_loss: 505.5571 - val_mse: 505.5570\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 385.4993 - mse: 385.4993 - val_loss: 455.7968 - val_mse: 455.7968\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 345.2360 - mse: 345.2361 - val_loss: 393.6116 - val_mse: 393.6116\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 309.3309 - mse: 309.3310 - val_loss: 372.7067 - val_mse: 372.7067\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 285.8914 - mse: 285.8914 - val_loss: 348.9228 - val_mse: 348.9229\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 268.0504 - mse: 268.0504 - val_loss: 301.5710 - val_mse: 301.5711\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 248.3334 - mse: 248.3333 - val_loss: 294.8813 - val_mse: 294.8813\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 243.6957 - mse: 243.6957 - val_loss: 270.9799 - val_mse: 270.9798\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 232.9108 - mse: 232.9108 - val_loss: 269.7939 - val_mse: 269.7939\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 221.1696 - mse: 221.1695 - val_loss: 250.3968 - val_mse: 250.3968\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 226.4084 - mse: 226.4084 - val_loss: 249.8386 - val_mse: 249.8385\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 225.3656 - mse: 225.3656 - val_loss: 234.7236 - val_mse: 234.7236\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 211.1299 - mse: 211.1299 - val_loss: 245.9111 - val_mse: 245.9111\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 212.5684 - mse: 212.5683 - val_loss: 231.2718 - val_mse: 231.2719\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 214.3468 - mse: 214.3468 - val_loss: 230.6102 - val_mse: 230.6102\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 210.8259 - mse: 210.8259 - val_loss: 228.5880 - val_mse: 228.5880\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 213.2998 - mse: 213.2999 - val_loss: 225.6208 - val_mse: 225.6208\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 205.9812 - mse: 205.9813 - val_loss: 220.9877 - val_mse: 220.9877\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 133us/sample - loss: 211.1068 - mse: 211.1068 - val_loss: 223.1322 - val_mse: 223.1322\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 203.7183 - mse: 203.7184 - val_loss: 210.6829 - val_mse: 210.6829\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 204.9746 - mse: 204.9746 - val_loss: 219.6905 - val_mse: 219.6905\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 207.8770 - mse: 207.8771 - val_loss: 210.7309 - val_mse: 210.7309\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 195.9288 - mse: 195.9288 - val_loss: 215.2972 - val_mse: 215.2972\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 207.5913 - mse: 207.5912 - val_loss: 216.2080 - val_mse: 216.2080\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 206.3689 - mse: 206.3689 - val_loss: 204.1669 - val_mse: 204.1669\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 156us/sample - loss: 206.5107 - mse: 206.5107 - val_loss: 208.8317 - val_mse: 208.8317\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 205.2132 - mse: 205.2132 - val_loss: 203.8460 - val_mse: 203.8460\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 204.5949 - mse: 204.5949 - val_loss: 207.5252 - val_mse: 207.5251\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 194.1617 - mse: 194.1617 - val_loss: 207.4487 - val_mse: 207.4487\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 207.2314 - mse: 207.2314 - val_loss: 200.7441 - val_mse: 200.7441\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 197.1468 - mse: 197.1468 - val_loss: 207.2141 - val_mse: 207.2141\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 197.8976 - mse: 197.8976 - val_loss: 206.2898 - val_mse: 206.2898\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 200.9167 - mse: 200.9167 - val_loss: 199.4186 - val_mse: 199.4186\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 197.4108 - mse: 197.4108 - val_loss: 198.4488 - val_mse: 198.4488\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 195.5271 - mse: 195.5271 - val_loss: 194.0068 - val_mse: 194.0068\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 190.5053 - mse: 190.5054 - val_loss: 205.6029 - val_mse: 205.6028\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 128us/sample - loss: 201.6852 - mse: 201.6853 - val_loss: 204.2204 - val_mse: 204.2204\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 192.8158 - mse: 192.8158 - val_loss: 202.8967 - val_mse: 202.8967\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 191.8291 - mse: 191.8290 - val_loss: 203.3671 - val_mse: 203.3671\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 195.7704 - mse: 195.7704 - val_loss: 194.9511 - val_mse: 194.9511\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 191.5098 - mse: 191.5098 - val_loss: 192.4003 - val_mse: 192.4003\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 193.4340 - mse: 193.4340 - val_loss: 197.0289 - val_mse: 197.0289\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 185.2424 - mse: 185.2423 - val_loss: 197.3606 - val_mse: 197.3606\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 196.6589 - mse: 196.6589 - val_loss: 188.6975 - val_mse: 188.6974\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 188.2050 - mse: 188.2051 - val_loss: 200.8750 - val_mse: 200.8750\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 130us/sample - loss: 189.3258 - mse: 189.3258 - val_loss: 198.8313 - val_mse: 198.8313\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 130us/sample - loss: 186.2020 - mse: 186.2020 - val_loss: 198.7513 - val_mse: 198.7513\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 190.2602 - mse: 190.2603 - val_loss: 192.1980 - val_mse: 192.1981\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 193.9983 - mse: 193.9983 - val_loss: 186.3737 - val_mse: 186.3737\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 191.1861 - mse: 191.1861 - val_loss: 187.1027 - val_mse: 187.1027\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 186.6658 - mse: 186.6658 - val_loss: 193.9398 - val_mse: 193.9397\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 194.6227 - mse: 194.6227 - val_loss: 190.2949 - val_mse: 190.2950\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 188.7425 - mse: 188.7426 - val_loss: 192.9178 - val_mse: 192.9178\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 183.8613 - mse: 183.8614 - val_loss: 185.7927 - val_mse: 185.7927\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 183.3018 - mse: 183.3018 - val_loss: 198.2541 - val_mse: 198.2541\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 188.8705 - mse: 188.8705 - val_loss: 189.6735 - val_mse: 189.6735\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 184.3112 - mse: 184.3113 - val_loss: 181.9694 - val_mse: 181.9693\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 185.8949 - mse: 185.8949 - val_loss: 191.6582 - val_mse: 191.6582\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 182.5816 - mse: 182.5816 - val_loss: 189.5466 - val_mse: 189.5466\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 186.4290 - mse: 186.4291 - val_loss: 188.2263 - val_mse: 188.2263\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 106us/sample - loss: 175.0065 - mse: 175.0065 - val_loss: 185.2535 - val_mse: 185.2535\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 189.4020 - mse: 189.4020 - val_loss: 186.9720 - val_mse: 186.9720\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 185.3605 - mse: 185.3605 - val_loss: 183.2754 - val_mse: 183.2754\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 173.8462 - mse: 173.8462 - val_loss: 185.8836 - val_mse: 185.8836\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 181.2222 - mse: 181.2223 - val_loss: 181.1145 - val_mse: 181.1145\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 186.6755 - mse: 186.6755 - val_loss: 189.7127 - val_mse: 189.7128\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 175.0620 - mse: 175.0620 - val_loss: 183.4158 - val_mse: 183.4158\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 177.7387 - mse: 177.7387 - val_loss: 184.4832 - val_mse: 184.4832\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 183.1536 - mse: 183.1535 - val_loss: 183.7713 - val_mse: 183.7713\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 175.1878 - mse: 175.1878 - val_loss: 182.0005 - val_mse: 182.0005\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 181.5891 - mse: 181.5892 - val_loss: 176.7220 - val_mse: 176.7219\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 176.1670 - mse: 176.1669 - val_loss: 180.4683 - val_mse: 180.4683\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 175.8735 - mse: 175.8736 - val_loss: 178.0652 - val_mse: 178.0652\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 177.5063 - mse: 177.5063 - val_loss: 176.6736 - val_mse: 176.6736\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 173.0581 - mse: 173.0580 - val_loss: 183.8932 - val_mse: 183.8932\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 179.8307 - mse: 179.8307 - val_loss: 180.0928 - val_mse: 180.0928\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 178.0273 - mse: 178.0273 - val_loss: 186.4764 - val_mse: 186.4765\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 180.9044 - mse: 180.9043 - val_loss: 185.9333 - val_mse: 185.9333\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 177.9517 - mse: 177.9518 - val_loss: 181.5282 - val_mse: 181.5282\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 174.2880 - mse: 174.2881 - val_loss: 183.5790 - val_mse: 183.5790\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 174.0177 - mse: 174.0177 - val_loss: 175.6933 - val_mse: 175.6933\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 174.1289 - mse: 174.1289 - val_loss: 178.1259 - val_mse: 178.1259\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 168.5063 - mse: 168.5063 - val_loss: 181.3588 - val_mse: 181.3588\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 171.7153 - mse: 171.7153 - val_loss: 193.7995 - val_mse: 193.7995\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 174.4813 - mse: 174.4813 - val_loss: 177.3859 - val_mse: 177.3859\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 0s 157us/sample - loss: 169.4365 - mse: 169.4365 - val_loss: 172.6760 - val_mse: 172.6759\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 174.8421 - mse: 174.8421 - val_loss: 177.0926 - val_mse: 177.0926\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 178.6465 - mse: 178.6465 - val_loss: 179.5658 - val_mse: 179.5658\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 169.0684 - mse: 169.0685 - val_loss: 184.3360 - val_mse: 184.3359\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 175.3457 - mse: 175.3457 - val_loss: 176.7679 - val_mse: 176.7679\n",
      "Epoch 94/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 169.0692 - mse: 169.0692 - val_loss: 174.7870 - val_mse: 174.7870\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 176.7756 - mse: 176.7756 - val_loss: 173.9242 - val_mse: 173.9241\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 172.3822 - mse: 172.3822 - val_loss: 196.6186 - val_mse: 196.6186\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 173.7251 - mse: 173.7251 - val_loss: 173.0088 - val_mse: 173.0088\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 167.6432 - mse: 167.6433 - val_loss: 175.3075 - val_mse: 175.3075\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 179.4355 - mse: 179.4354 - val_loss: 183.9141 - val_mse: 183.9140\n",
      "[CV] ....................................... nl=2, nn=3, total=  31.9s\n",
      "[CV] nl=2, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 397us/sample - loss: 491.7954 - mse: 491.7953 - val_loss: 544.0269 - val_mse: 544.0270\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 476.1641 - mse: 476.1641 - val_loss: 524.1338 - val_mse: 524.1335\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 454.6934 - mse: 454.6934 - val_loss: 499.6446 - val_mse: 499.6446\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 430.5481 - mse: 430.5480 - val_loss: 474.3391 - val_mse: 474.3392\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 407.3350 - mse: 407.3350 - val_loss: 451.4593 - val_mse: 451.4593\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 387.6529 - mse: 387.6530 - val_loss: 432.8026 - val_mse: 432.8025\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 373.2145 - mse: 373.2145 - val_loss: 420.0828 - val_mse: 420.0828\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 363.8035 - mse: 363.8036 - val_loss: 411.8718 - val_mse: 411.8718\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 358.3692 - mse: 358.3692 - val_loss: 407.1368 - val_mse: 407.1367\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 355.5288 - mse: 355.5289 - val_loss: 404.7679 - val_mse: 404.7679\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 354.2137 - mse: 354.2137 - val_loss: 403.4580 - val_mse: 403.4580\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 353.6560 - mse: 353.6560 - val_loss: 402.7448 - val_mse: 402.7448\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.3371 - mse: 353.3370 - val_loss: 402.4705 - val_mse: 402.4705\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.2787 - mse: 353.2787 - val_loss: 402.3470 - val_mse: 402.3470\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.2110 - mse: 353.2112 - val_loss: 402.2255 - val_mse: 402.2255\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2055 - mse: 353.2055 - val_loss: 402.2518 - val_mse: 402.2519\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.2105 - mse: 353.2106 - val_loss: 402.1485 - val_mse: 402.1485\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 353.2317 - mse: 353.2317 - val_loss: 402.1553 - val_mse: 402.1552\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.1930 - mse: 353.1930 - val_loss: 402.1505 - val_mse: 402.1505\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 353.1918 - mse: 353.1917 - val_loss: 402.1417 - val_mse: 402.1417\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.1981 - mse: 353.1980 - val_loss: 402.1378 - val_mse: 402.1378\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.1858 - mse: 353.1859 - val_loss: 402.1627 - val_mse: 402.1627\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2060 - mse: 353.2059 - val_loss: 402.1337 - val_mse: 402.1337\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.2059 - mse: 353.2060 - val_loss: 402.1327 - val_mse: 402.1327\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.2086 - mse: 353.2086 - val_loss: 402.1424 - val_mse: 402.1423\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 353.1947 - mse: 353.1946 - val_loss: 402.1359 - val_mse: 402.1359\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 353.2058 - mse: 353.2057 - val_loss: 402.1417 - val_mse: 402.1418\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 353.2121 - mse: 353.2121 - val_loss: 402.1489 - val_mse: 402.1489\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 353.2054 - mse: 353.2056 - val_loss: 402.1232 - val_mse: 402.1232\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.1989 - mse: 353.1989 - val_loss: 402.1152 - val_mse: 402.1152\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.2018 - mse: 353.2018 - val_loss: 402.1841 - val_mse: 402.1841\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2127 - mse: 353.2126 - val_loss: 402.1391 - val_mse: 402.1392\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2018 - mse: 353.2018 - val_loss: 402.1612 - val_mse: 402.1612\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 130us/sample - loss: 353.2062 - mse: 353.2064 - val_loss: 402.1080 - val_mse: 402.1081\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 353.1920 - mse: 353.1921 - val_loss: 402.1561 - val_mse: 402.1561\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.1996 - mse: 353.1996 - val_loss: 402.1646 - val_mse: 402.1646\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.1997 - mse: 353.1997 - val_loss: 402.1575 - val_mse: 402.1576\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2197 - mse: 353.2197 - val_loss: 402.1359 - val_mse: 402.1358\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2181 - mse: 353.2181 - val_loss: 402.1457 - val_mse: 402.1458\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 353.2012 - mse: 353.2012 - val_loss: 402.1182 - val_mse: 402.1181\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2265 - mse: 353.2265 - val_loss: 402.1591 - val_mse: 402.1591\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2014 - mse: 353.2014 - val_loss: 402.1428 - val_mse: 402.1429\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2066 - mse: 353.2067 - val_loss: 402.1495 - val_mse: 402.1495\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.2018 - mse: 353.2019 - val_loss: 402.1276 - val_mse: 402.1277\n",
      "[CV] ....................................... nl=2, nn=3, total=  14.5s\n",
      "[CV] nl=2, nn=3 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 311us/sample - loss: 490.6970 - mse: 490.6970 - val_loss: 643.2124 - val_mse: 643.2124\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 473.1095 - mse: 473.1095 - val_loss: 620.4816 - val_mse: 620.4815\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 140us/sample - loss: 450.6884 - mse: 450.6884 - val_loss: 594.0629 - val_mse: 594.0629\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 426.7103 - mse: 426.7102 - val_loss: 566.7615 - val_mse: 566.7615\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 403.9866 - mse: 403.9867 - val_loss: 542.5183 - val_mse: 542.5184\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 385.2351 - mse: 385.2351 - val_loss: 523.0639 - val_mse: 523.0639\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 371.6470 - mse: 371.6469 - val_loss: 509.1392 - val_mse: 509.1391\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 362.8403 - mse: 362.8403 - val_loss: 500.0859 - val_mse: 500.0859\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 357.7590 - mse: 357.7590 - val_loss: 494.4417 - val_mse: 494.4417\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 355.2332 - mse: 355.2332 - val_loss: 491.4517 - val_mse: 491.4517\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 354.0190 - mse: 354.0190 - val_loss: 489.6977 - val_mse: 489.6977\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.5560 - mse: 353.5560 - val_loss: 488.8004 - val_mse: 488.8005\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.3659 - mse: 353.3660 - val_loss: 488.1873 - val_mse: 488.1874\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.2778 - mse: 353.2779 - val_loss: 488.0036 - val_mse: 488.0036\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.2361 - mse: 353.2361 - val_loss: 487.7344 - val_mse: 487.7344\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.2180 - mse: 353.2179 - val_loss: 487.6460 - val_mse: 487.6460\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.2043 - mse: 353.2043 - val_loss: 487.5982 - val_mse: 487.5983\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.1948 - mse: 353.1947 - val_loss: 487.5137 - val_mse: 487.5137\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.1889 - mse: 353.1888 - val_loss: 487.5362 - val_mse: 487.5362\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.2009 - mse: 353.2008 - val_loss: 487.5845 - val_mse: 487.5845\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2013 - mse: 353.2013 - val_loss: 487.4687 - val_mse: 487.4688\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 142us/sample - loss: 353.2160 - mse: 353.2160 - val_loss: 487.4966 - val_mse: 487.4965\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.2371 - mse: 353.2371 - val_loss: 487.4937 - val_mse: 487.4937\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2138 - mse: 353.2137 - val_loss: 487.4304 - val_mse: 487.4304\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.1865 - mse: 353.1864 - val_loss: 487.5368 - val_mse: 487.5367\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.1911 - mse: 353.1911 - val_loss: 487.4779 - val_mse: 487.4779\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 353.2077 - mse: 353.2078 - val_loss: 487.5318 - val_mse: 487.5319\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2065 - mse: 353.2065 - val_loss: 487.4901 - val_mse: 487.4901\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 111us/sample - loss: 353.2127 - mse: 353.2127 - val_loss: 487.4774 - val_mse: 487.4774\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2003 - mse: 353.2003 - val_loss: 487.3881 - val_mse: 487.3882\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2251 - mse: 353.2251 - val_loss: 487.3702 - val_mse: 487.3701\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 353.2014 - mse: 353.2014 - val_loss: 487.4422 - val_mse: 487.4422\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 353.2310 - mse: 353.2309 - val_loss: 487.4341 - val_mse: 487.4341\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 353.1937 - mse: 353.1937 - val_loss: 487.5232 - val_mse: 487.5232\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 353.2039 - mse: 353.2039 - val_loss: 487.5485 - val_mse: 487.5486\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 353.2369 - mse: 353.2369 - val_loss: 487.4286 - val_mse: 487.4286\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2302 - mse: 353.2302 - val_loss: 487.5440 - val_mse: 487.5439\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 353.1987 - mse: 353.1987 - val_loss: 487.4947 - val_mse: 487.4947\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 353.2174 - mse: 353.2173 - val_loss: 487.5628 - val_mse: 487.5628\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 353.2097 - mse: 353.2098 - val_loss: 487.4651 - val_mse: 487.4650\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 353.1923 - mse: 353.1923 - val_loss: 487.5053 - val_mse: 487.5055\n",
      "[CV] ....................................... nl=2, nn=3, total=  13.4s\n",
      "[CV] nl=2, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 309us/sample - loss: 505.0397 - mse: 505.0395 - val_loss: 655.4367 - val_mse: 655.4368\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 433.0402 - mse: 433.0402 - val_loss: 581.8592 - val_mse: 581.8594\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 369.9996 - mse: 369.9996 - val_loss: 473.5921 - val_mse: 473.5921\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 311.9404 - mse: 311.9405 - val_loss: 374.5120 - val_mse: 374.5120\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 269.0342 - mse: 269.0342 - val_loss: 317.4817 - val_mse: 317.4817\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 236.8381 - mse: 236.8381 - val_loss: 292.6846 - val_mse: 292.6846\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 223.1576 - mse: 223.1575 - val_loss: 259.0982 - val_mse: 259.0982\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 209.9058 - mse: 209.9059 - val_loss: 236.8483 - val_mse: 236.8483\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 205.6232 - mse: 205.6232 - val_loss: 233.6766 - val_mse: 233.6766\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 199.4953 - mse: 199.4953 - val_loss: 233.9637 - val_mse: 233.9637\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 196.4012 - mse: 196.4011 - val_loss: 216.9109 - val_mse: 216.9109\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 182.8628 - mse: 182.8628 - val_loss: 216.3765 - val_mse: 216.3765\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 188.6697 - mse: 188.6697 - val_loss: 207.8298 - val_mse: 207.8298\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 185.5982 - mse: 185.5982 - val_loss: 208.4076 - val_mse: 208.4076\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 182.5112 - mse: 182.5112 - val_loss: 207.0285 - val_mse: 207.0285\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 128us/sample - loss: 183.5549 - mse: 183.5550 - val_loss: 206.1260 - val_mse: 206.1259\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 180.7034 - mse: 180.7034 - val_loss: 205.5526 - val_mse: 205.5526\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 177.8639 - mse: 177.8638 - val_loss: 207.8104 - val_mse: 207.8104\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 176.8696 - mse: 176.8696 - val_loss: 194.0167 - val_mse: 194.0167\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 177.1116 - mse: 177.1115 - val_loss: 197.1925 - val_mse: 197.1925\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 163.1762 - mse: 163.1762 - val_loss: 198.9675 - val_mse: 198.9675\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 169.6487 - mse: 169.6486 - val_loss: 197.2853 - val_mse: 197.2852\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 170.5822 - mse: 170.5823 - val_loss: 202.9500 - val_mse: 202.9500\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 164.1696 - mse: 164.1696 - val_loss: 190.7053 - val_mse: 190.7053\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 154.3748 - mse: 154.3748 - val_loss: 190.7701 - val_mse: 190.7701\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 165.6580 - mse: 165.6580 - val_loss: 194.3983 - val_mse: 194.3983\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 156.0533 - mse: 156.0533 - val_loss: 188.6713 - val_mse: 188.6713\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 150us/sample - loss: 160.2873 - mse: 160.2873 - val_loss: 188.4547 - val_mse: 188.4547\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 154.8951 - mse: 154.8951 - val_loss: 186.9137 - val_mse: 186.9137\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 151.6424 - mse: 151.6423 - val_loss: 189.9370 - val_mse: 189.9370\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 155.3979 - mse: 155.3979 - val_loss: 194.4125 - val_mse: 194.4125\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.0299 - mse: 144.0299 - val_loss: 187.8145 - val_mse: 187.8145\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.1910 - mse: 143.1910 - val_loss: 195.9330 - val_mse: 195.9330\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 152.8407 - mse: 152.8408 - val_loss: 199.5693 - val_mse: 199.5693\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 146.6000 - mse: 146.6000 - val_loss: 192.2674 - val_mse: 192.2674\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 138.8312 - mse: 138.8312 - val_loss: 196.7169 - val_mse: 196.7169\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 150.4501 - mse: 150.4501 - val_loss: 201.2643 - val_mse: 201.2643\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 140.5527 - mse: 140.5527 - val_loss: 217.0150 - val_mse: 217.0150\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 143.0216 - mse: 143.0216 - val_loss: 195.8236 - val_mse: 195.8235\n",
      "[CV] ....................................... nl=2, nn=6, total=  12.6s\n",
      "[CV] nl=2, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 409us/sample - loss: 449.1733 - mse: 449.1733 - val_loss: 661.3882 - val_mse: 661.3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 398.8775 - mse: 398.8776 - val_loss: 603.9947 - val_mse: 603.9947\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 342.9867 - mse: 342.9868 - val_loss: 511.7424 - val_mse: 511.7423\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 291.8410 - mse: 291.8412 - val_loss: 426.4253 - val_mse: 426.4254\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 253.8840 - mse: 253.8840 - val_loss: 361.1686 - val_mse: 361.1686\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 228.6474 - mse: 228.6474 - val_loss: 336.7023 - val_mse: 336.7021\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 200.3021 - mse: 200.3022 - val_loss: 286.6437 - val_mse: 286.6437\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 183.1854 - mse: 183.1853 - val_loss: 272.5262 - val_mse: 272.5262\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 177.7790 - mse: 177.7790 - val_loss: 258.3676 - val_mse: 258.3675\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 171.8776 - mse: 171.8776 - val_loss: 244.8519 - val_mse: 244.8519\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 166.6854 - mse: 166.685 - 0s 106us/sample - loss: 165.3907 - mse: 165.3907 - val_loss: 241.1840 - val_mse: 241.1840\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 138.5167 - mse: 138.516 - 0s 109us/sample - loss: 169.4152 - mse: 169.4153 - val_loss: 238.6367 - val_mse: 238.6367\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 160.8317 - mse: 160.8317 - val_loss: 230.5813 - val_mse: 230.5813\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 167.0635 - mse: 167.0635 - val_loss: 219.9957 - val_mse: 219.9958\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 156.0688 - mse: 156.0688 - val_loss: 213.8521 - val_mse: 213.8521\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 157.4370 - mse: 157.4371 - val_loss: 217.3008 - val_mse: 217.3008\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 162.2562 - mse: 162.2562 - val_loss: 227.4849 - val_mse: 227.4849\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 151.2724 - mse: 151.2724 - val_loss: 208.3631 - val_mse: 208.3631\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 149.6360 - mse: 149.6359 - val_loss: 210.8918 - val_mse: 210.8918\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 148.7660 - mse: 148.7661 - val_loss: 206.4026 - val_mse: 206.4025\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 148.6742 - mse: 148.6742 - val_loss: 201.6844 - val_mse: 201.6844\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 152.1974 - mse: 152.1974 - val_loss: 205.0162 - val_mse: 205.0162\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 148.1596 - mse: 148.1596 - val_loss: 203.2591 - val_mse: 203.2590\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 151.8207 - mse: 151.8207 - val_loss: 196.6395 - val_mse: 196.6395\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 150.3153 - mse: 150.3153 - val_loss: 194.6936 - val_mse: 194.6936\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 155.4430 - mse: 155.4430 - val_loss: 199.2167 - val_mse: 199.2168\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 143.8691 - mse: 143.8691 - val_loss: 188.6912 - val_mse: 188.6913\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 150.6706 - mse: 150.6705 - val_loss: 197.2151 - val_mse: 197.2151\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 142.0637 - mse: 142.0637 - val_loss: 193.0614 - val_mse: 193.0615\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 140.4522 - mse: 140.4522 - val_loss: 190.7588 - val_mse: 190.7588\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.3853 - mse: 142.3853 - val_loss: 184.6104 - val_mse: 184.6104\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 142.1841 - mse: 142.1841 - val_loss: 185.4021 - val_mse: 185.4021\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 146.3349 - mse: 146.3349 - val_loss: 185.0106 - val_mse: 185.0107\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 140.1624 - mse: 140.1624 - val_loss: 182.2450 - val_mse: 182.2449\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.3818 - mse: 141.3817 - val_loss: 197.0471 - val_mse: 197.0471\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 146.1012 - mse: 146.1012 - val_loss: 197.0271 - val_mse: 197.0271\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 146.2803 - mse: 146.2804 - val_loss: 188.7817 - val_mse: 188.7818\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 136.8723 - mse: 136.8724 - val_loss: 187.7182 - val_mse: 187.7182\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 133.1679 - mse: 133.1679 - val_loss: 182.9877 - val_mse: 182.9877\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 145.4775 - mse: 145.4775 - val_loss: 188.5856 - val_mse: 188.5856\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 137.2037 - mse: 137.2037 - val_loss: 173.0185 - val_mse: 173.0185\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 141.8768 - mse: 141.8768 - val_loss: 179.2133 - val_mse: 179.2133\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 142.7656 - mse: 142.7656 - val_loss: 178.2888 - val_mse: 178.2888\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 135.1955 - mse: 135.1955 - val_loss: 192.6562 - val_mse: 192.6562\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 131us/sample - loss: 139.6632 - mse: 139.6632 - val_loss: 189.1242 - val_mse: 189.1242\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.6644 - mse: 134.6644 - val_loss: 193.5294 - val_mse: 193.5294\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 131.9540 - mse: 131.9540 - val_loss: 187.1083 - val_mse: 187.1083\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 137.0085 - mse: 137.0085 - val_loss: 188.6309 - val_mse: 188.6308\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 138.6862 - mse: 138.6862 - val_loss: 194.3144 - val_mse: 194.3144\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 140.2111 - mse: 140.2111 - val_loss: 187.5263 - val_mse: 187.5262\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 133.1810 - mse: 133.1810 - val_loss: 189.7566 - val_mse: 189.7566\n",
      "[CV] ....................................... nl=2, nn=6, total=  16.9s\n",
      "[CV] nl=2, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 313us/sample - loss: 518.7498 - mse: 518.7499 - val_loss: 655.5600 - val_mse: 655.5601\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 454.8835 - mse: 454.8836 - val_loss: 568.3209 - val_mse: 568.3208\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 385.8723 - mse: 385.8723 - val_loss: 420.1057 - val_mse: 420.1057\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 326.4134 - mse: 326.4133 - val_loss: 356.3999 - val_mse: 356.4000\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 283.8869 - mse: 283.8869 - val_loss: 296.8928 - val_mse: 296.8928\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 248.5419 - mse: 248.5420 - val_loss: 270.1454 - val_mse: 270.1455\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 230.9027 - mse: 230.9027 - val_loss: 255.2824 - val_mse: 255.2825\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 228.3697 - mse: 228.3697 - val_loss: 227.6619 - val_mse: 227.6619\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 209.0666 - mse: 209.0665 - val_loss: 220.9866 - val_mse: 220.9866\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 202.1109 - mse: 202.1109 - val_loss: 220.8936 - val_mse: 220.8936\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 208.4823 - mse: 208.4824 - val_loss: 215.0240 - val_mse: 215.0240\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 203.0427 - mse: 203.0427 - val_loss: 213.8720 - val_mse: 213.8720\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 196.6920 - mse: 196.6920 - val_loss: 197.9975 - val_mse: 197.9975\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 199.0700 - mse: 199.0700 - val_loss: 206.9513 - val_mse: 206.9513\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 198.6273 - mse: 198.6273 - val_loss: 204.7927 - val_mse: 204.7927\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 195.9053 - mse: 195.9052 - val_loss: 188.5606 - val_mse: 188.5606\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 187.5321 - mse: 187.5320 - val_loss: 191.8205 - val_mse: 191.8204\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 189.0740 - mse: 189.0740 - val_loss: 189.3112 - val_mse: 189.3112\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 186.0018 - mse: 186.0018 - val_loss: 188.2328 - val_mse: 188.2328\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 183.1001 - mse: 183.1001 - val_loss: 182.6612 - val_mse: 182.6611\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 175.1489 - mse: 175.1489 - val_loss: 178.0288 - val_mse: 178.0288\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 173.9294 - mse: 173.9294 - val_loss: 199.2298 - val_mse: 199.2298\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 172.3493 - mse: 172.3493 - val_loss: 191.1606 - val_mse: 191.1606\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 169.0492 - mse: 169.0492 - val_loss: 193.8174 - val_mse: 193.8174\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 169.9435 - mse: 169.9435 - val_loss: 180.0266 - val_mse: 180.0266\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 177.4628 - mse: 177.4628 - val_loss: 178.8491 - val_mse: 178.8491\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 167.4346 - mse: 167.4346 - val_loss: 185.5828 - val_mse: 185.5828\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 171.8906 - mse: 171.8906 - val_loss: 183.9851 - val_mse: 183.9850\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 159.6785 - mse: 159.6785 - val_loss: 191.4580 - val_mse: 191.4580\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 123us/sample - loss: 176.6287 - mse: 176.6286 - val_loss: 183.9231 - val_mse: 183.9232\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 163.4801 - mse: 163.4801 - val_loss: 179.6218 - val_mse: 179.6217\n",
      "[CV] ....................................... nl=2, nn=6, total=  10.7s\n",
      "[CV] nl=2, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 310us/sample - loss: 464.0300 - mse: 464.0298 - val_loss: 526.0941 - val_mse: 526.0940\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 393.8332 - mse: 393.8331 - val_loss: 443.6398 - val_mse: 443.6398\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 339.3383 - mse: 339.3382 - val_loss: 336.9845 - val_mse: 336.9843\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 281.9232 - mse: 281.9232 - val_loss: 270.1108 - val_mse: 270.1109\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 239.8716 - mse: 239.8716 - val_loss: 224.2383 - val_mse: 224.2383\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 212.9426 - mse: 212.9426 - val_loss: 202.5001 - val_mse: 202.5001\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 189.5823 - mse: 189.5822 - val_loss: 165.6507 - val_mse: 165.6507\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 183.7288 - mse: 183.7287 - val_loss: 163.6272 - val_mse: 163.6272\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 176.7427 - mse: 176.7427 - val_loss: 157.7424 - val_mse: 157.7424\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 179.6187 - mse: 179.6187 - val_loss: 152.1473 - val_mse: 152.1473\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 174.4732 - mse: 174.4732 - val_loss: 139.0092 - val_mse: 139.0092\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 173.0724 - mse: 173.0724 - val_loss: 146.1269 - val_mse: 146.1269\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 167.1709 - mse: 167.1710 - val_loss: 144.2147 - val_mse: 144.2147\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 160.6565 - mse: 160.6564 - val_loss: 139.8802 - val_mse: 139.8802\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 162.9237 - mse: 162.9238 - val_loss: 132.4834 - val_mse: 132.4834\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 164.0386 - mse: 164.0386 - val_loss: 137.9820 - val_mse: 137.9820\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 158.6490 - mse: 158.6490 - val_loss: 135.9894 - val_mse: 135.9894\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 160.0273 - mse: 160.0273 - val_loss: 134.0044 - val_mse: 134.0044\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 156.4642 - mse: 156.4642 - val_loss: 130.5346 - val_mse: 130.5346\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 157.4917 - mse: 157.4917 - val_loss: 133.0633 - val_mse: 133.0633\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 157.9625 - mse: 157.9625 - val_loss: 123.1476 - val_mse: 123.1476\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 152.1285 - mse: 152.1285 - val_loss: 122.1155 - val_mse: 122.1155\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 146.3480 - mse: 146.3480 - val_loss: 125.4810 - val_mse: 125.4810\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 148.8346 - mse: 148.8346 - val_loss: 123.2800 - val_mse: 123.2800\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 149.2180 - mse: 149.2180 - val_loss: 131.8141 - val_mse: 131.8141\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 151.5756 - mse: 151.5756 - val_loss: 129.9303 - val_mse: 129.9303\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 104us/sample - loss: 145.0446 - mse: 145.0447 - val_loss: 122.6803 - val_mse: 122.6803\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 155.4202 - mse: 155.4202 - val_loss: 122.2898 - val_mse: 122.2898\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 156.1386 - mse: 156.1386 - val_loss: 128.9361 - val_mse: 128.9361\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 145.3453 - mse: 145.3453 - val_loss: 123.0248 - val_mse: 123.0248\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 150.3389 - mse: 150.3389 - val_loss: 121.9413 - val_mse: 121.9413\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 146.7127 - mse: 146.7127 - val_loss: 118.3261 - val_mse: 118.3261\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 149.6499 - mse: 149.6500 - val_loss: 122.8179 - val_mse: 122.8179\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 147.4806 - mse: 147.4807 - val_loss: 121.2278 - val_mse: 121.2277\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 145.6301 - mse: 145.6302 - val_loss: 125.9892 - val_mse: 125.9892\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 149.2830 - mse: 149.2830 - val_loss: 123.7325 - val_mse: 123.7325\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 147.0214 - mse: 147.0214 - val_loss: 123.3214 - val_mse: 123.3214\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 135.1601 - mse: 135.1601 - val_loss: 124.6086 - val_mse: 124.6086\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 143.3356 - mse: 143.3356 - val_loss: 123.6653 - val_mse: 123.6653\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 148.5316 - mse: 148.5316 - val_loss: 129.5608 - val_mse: 129.5608\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 137.8970 - mse: 137.8970 - val_loss: 126.4523 - val_mse: 126.4523\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 139.7816 - mse: 139.7817 - val_loss: 125.0391 - val_mse: 125.0391\n",
      "[CV] ....................................... nl=2, nn=6, total=  13.5s\n",
      "[CV] nl=2, nn=6 ......................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 307us/sample - loss: 471.7995 - mse: 471.7995 - val_loss: 636.5411 - val_mse: 636.5411\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 397.4740 - mse: 397.4742 - val_loss: 550.6342 - val_mse: 550.6342\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 339.6307 - mse: 339.6308 - val_loss: 433.9339 - val_mse: 433.9339\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 282.2299 - mse: 282.2299 - val_loss: 355.6752 - val_mse: 355.6752\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 240.9561 - mse: 240.9561 - val_loss: 297.1611 - val_mse: 297.1611\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 207.4497 - mse: 207.4497 - val_loss: 265.3818 - val_mse: 265.3817\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 198.8323 - mse: 198.8323 - val_loss: 238.6653 - val_mse: 238.6653\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 180.2905 - mse: 180.2905 - val_loss: 240.1954 - val_mse: 240.1954\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 181.2134 - mse: 181.2134 - val_loss: 221.9094 - val_mse: 221.9094\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 172.3791 - mse: 172.3791 - val_loss: 218.9623 - val_mse: 218.9624\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 173.2270 - mse: 173.2269 - val_loss: 214.1343 - val_mse: 214.1343\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 175.8767 - mse: 175.8767 - val_loss: 216.1655 - val_mse: 216.1656\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 166.4745 - mse: 166.4744 - val_loss: 207.9429 - val_mse: 207.9429\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 170.3670 - mse: 170.3670 - val_loss: 204.1760 - val_mse: 204.1759\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 156.8269 - mse: 156.8269 - val_loss: 192.1634 - val_mse: 192.1634\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 162.5865 - mse: 162.5864 - val_loss: 188.8024 - val_mse: 188.8024\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 157.9681 - mse: 157.9680 - val_loss: 184.1107 - val_mse: 184.1107\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 160.1328 - mse: 160.1328 - val_loss: 190.5448 - val_mse: 190.5448\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 156.6386 - mse: 156.6386 - val_loss: 184.3776 - val_mse: 184.3776\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 161.8358 - mse: 161.8358 - val_loss: 178.4189 - val_mse: 178.4188\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.2430 - mse: 151.2431 - val_loss: 181.8834 - val_mse: 181.8833\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 163.7257 - mse: 163.7258 - val_loss: 176.5799 - val_mse: 176.5799\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 153.3566 - mse: 153.3566 - val_loss: 171.6892 - val_mse: 171.6892\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 155.7438 - mse: 155.7438 - val_loss: 173.1242 - val_mse: 173.1242\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 153.0886 - mse: 153.0886 - val_loss: 174.1753 - val_mse: 174.1753\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 149.9585 - mse: 149.9585 - val_loss: 173.9683 - val_mse: 173.9683\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 151.4278 - mse: 151.4278 - val_loss: 180.7637 - val_mse: 180.7637\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.4113 - mse: 143.4114 - val_loss: 169.4891 - val_mse: 169.4891\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 145.3928 - mse: 145.3927 - val_loss: 175.7715 - val_mse: 175.7715\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 150.0620 - mse: 150.0620 - val_loss: 168.0100 - val_mse: 168.0100\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 149.4725 - mse: 149.4725 - val_loss: 170.4400 - val_mse: 170.4399\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 146.5423 - mse: 146.5423 - val_loss: 167.2073 - val_mse: 167.2073\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 144.5280 - mse: 144.5280 - val_loss: 166.7670 - val_mse: 166.7670\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 144.7699 - mse: 144.7699 - val_loss: 166.3194 - val_mse: 166.3194\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 149.2643 - mse: 149.2642 - val_loss: 172.1691 - val_mse: 172.1691\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 143.8658 - mse: 143.8657 - val_loss: 164.3360 - val_mse: 164.3360\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 149.3156 - mse: 149.3156 - val_loss: 166.6650 - val_mse: 166.6650\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.8843 - mse: 142.8843 - val_loss: 163.4657 - val_mse: 163.4657\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 147.2527 - mse: 147.2527 - val_loss: 162.1565 - val_mse: 162.1565\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 150.8471 - mse: 150.8471 - val_loss: 162.3483 - val_mse: 162.3483\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 140.5230 - mse: 140.5230 - val_loss: 166.9206 - val_mse: 166.9206\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 144.6506 - mse: 144.6506 - val_loss: 160.2709 - val_mse: 160.2709\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 143.6164 - mse: 143.6165 - val_loss: 161.0806 - val_mse: 161.0806\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 145.1359 - mse: 145.1360 - val_loss: 159.6632 - val_mse: 159.6632\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 145.5645 - mse: 145.5645 - val_loss: 159.0680 - val_mse: 159.0680\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 142.2637 - mse: 142.2637 - val_loss: 170.4734 - val_mse: 170.4734\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 143.8175 - mse: 143.8175 - val_loss: 161.7587 - val_mse: 161.7587\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 153.8442 - mse: 153.8443 - val_loss: 160.9915 - val_mse: 160.9915\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 141.6159 - mse: 141.6159 - val_loss: 165.0677 - val_mse: 165.0677\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 142.5507 - mse: 142.5506 - val_loss: 161.8502 - val_mse: 161.8502\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 139.2309 - mse: 139.2310 - val_loss: 162.5013 - val_mse: 162.5013\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 124us/sample - loss: 150.3144 - mse: 150.3143 - val_loss: 168.5383 - val_mse: 168.5383\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 145.8576 - mse: 145.8576 - val_loss: 161.2854 - val_mse: 161.2854\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 135.3492 - mse: 135.3492 - val_loss: 157.6010 - val_mse: 157.6010\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 137.6575 - mse: 137.6575 - val_loss: 163.6215 - val_mse: 163.6215\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 139.5452 - mse: 139.5452 - val_loss: 163.8902 - val_mse: 163.8902\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.5393 - mse: 134.5393 - val_loss: 159.8506 - val_mse: 159.8506\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 138.7600 - mse: 138.7600 - val_loss: 166.0397 - val_mse: 166.0397\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 137.8728 - mse: 137.8728 - val_loss: 161.1275 - val_mse: 161.1275\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 142.0542 - mse: 142.0543 - val_loss: 158.8412 - val_mse: 158.8412\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 139.4367 - mse: 139.4367 - val_loss: 158.6080 - val_mse: 158.6080\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 139.3501 - mse: 139.3501 - val_loss: 157.5263 - val_mse: 157.5263\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 138.9385 - mse: 138.9385 - val_loss: 155.2745 - val_mse: 155.2745\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 134.0535 - mse: 134.0535 - val_loss: 157.4747 - val_mse: 157.4747\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 137.9955 - mse: 137.9955 - val_loss: 158.1429 - val_mse: 158.1429\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 134.4079 - mse: 134.4079 - val_loss: 157.4315 - val_mse: 157.4316\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 137.0873 - mse: 137.0874 - val_loss: 154.8643 - val_mse: 154.8643\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 133.1292 - mse: 133.1292 - val_loss: 156.0486 - val_mse: 156.0487\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 142.6485 - mse: 142.6485 - val_loss: 160.2922 - val_mse: 160.2922\n",
      "Epoch 70/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 137.3378 - mse: 137.3379 - val_loss: 154.7579 - val_mse: 154.7579\n",
      "Epoch 71/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 135.0975 - mse: 135.0974 - val_loss: 154.7760 - val_mse: 154.7760\n",
      "Epoch 72/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 135.0533 - mse: 135.0533 - val_loss: 160.1539 - val_mse: 160.1539\n",
      "Epoch 73/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 134.7605 - mse: 134.7605 - val_loss: 157.5885 - val_mse: 157.5885\n",
      "Epoch 74/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 132.6270 - mse: 132.6270 - val_loss: 154.0247 - val_mse: 154.0247\n",
      "Epoch 75/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 132.4981 - mse: 132.4981 - val_loss: 154.3141 - val_mse: 154.3141\n",
      "Epoch 76/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 130.6291 - mse: 130.6291 - val_loss: 154.6041 - val_mse: 154.6041\n",
      "Epoch 77/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 130.9614 - mse: 130.9613 - val_loss: 153.9018 - val_mse: 153.9018\n",
      "Epoch 78/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 129.9079 - mse: 129.9079 - val_loss: 149.2335 - val_mse: 149.2335\n",
      "Epoch 79/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 134.4659 - mse: 134.4659 - val_loss: 153.1713 - val_mse: 153.1713\n",
      "Epoch 80/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 125.3663 - mse: 125.3663 - val_loss: 148.8851 - val_mse: 148.8851\n",
      "Epoch 81/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 128.7044 - mse: 128.7044 - val_loss: 160.0879 - val_mse: 160.0879\n",
      "Epoch 82/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 132.3808 - mse: 132.3808 - val_loss: 148.0277 - val_mse: 148.0277\n",
      "Epoch 83/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 132.1360 - mse: 132.1360 - val_loss: 157.7270 - val_mse: 157.7270\n",
      "Epoch 84/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 126.5651 - mse: 126.5651 - val_loss: 149.5295 - val_mse: 149.5295\n",
      "Epoch 85/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 131.9643 - mse: 131.9644 - val_loss: 147.2920 - val_mse: 147.2920\n",
      "Epoch 86/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 135.4525 - mse: 135.4525 - val_loss: 160.0839 - val_mse: 160.0839\n",
      "Epoch 87/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 131.4964 - mse: 131.4964 - val_loss: 156.5373 - val_mse: 156.5373\n",
      "Epoch 88/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 133.0625 - mse: 133.0625 - val_loss: 145.2883 - val_mse: 145.2883\n",
      "Epoch 89/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 131.0705 - mse: 131.0705 - val_loss: 145.8154 - val_mse: 145.8154\n",
      "Epoch 90/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 125.4080 - mse: 125.4080 - val_loss: 145.0560 - val_mse: 145.0560\n",
      "Epoch 91/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 121.3196 - mse: 121.3197 - val_loss: 151.1918 - val_mse: 151.1919\n",
      "Epoch 92/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 124.0560 - mse: 124.0561 - val_loss: 145.3938 - val_mse: 145.3938\n",
      "Epoch 93/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 124.8311 - mse: 124.8311 - val_loss: 150.0323 - val_mse: 150.0323\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - ETA: 0s - loss: 127.6835 - mse: 127.683 - 0s 102us/sample - loss: 124.5074 - mse: 124.5074 - val_loss: 142.3732 - val_mse: 142.3732\n",
      "Epoch 95/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 122.3563 - mse: 122.3562 - val_loss: 154.6465 - val_mse: 154.6465\n",
      "Epoch 96/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 135.0408 - mse: 135.0408 - val_loss: 143.6147 - val_mse: 143.6147\n",
      "Epoch 97/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 121.5324 - mse: 121.5324 - val_loss: 147.5626 - val_mse: 147.5626\n",
      "Epoch 98/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 128.8841 - mse: 128.8841 - val_loss: 140.9988 - val_mse: 140.9989\n",
      "Epoch 99/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 137.6183 - mse: 137.6183 - val_loss: 143.2285 - val_mse: 143.2285\n",
      "Epoch 100/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 126.9458 - mse: 126.9458 - val_loss: 153.8942 - val_mse: 153.8942\n",
      "Epoch 101/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 128.3645 - mse: 128.3645 - val_loss: 146.2297 - val_mse: 146.2297\n",
      "Epoch 102/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 125.8171 - mse: 125.8171 - val_loss: 139.0015 - val_mse: 139.0015\n",
      "Epoch 103/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 122.1518 - mse: 122.1518 - val_loss: 143.2949 - val_mse: 143.2949\n",
      "Epoch 104/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 124.2875 - mse: 124.2876 - val_loss: 144.5468 - val_mse: 144.5468\n",
      "Epoch 105/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 127.0208 - mse: 127.0208 - val_loss: 145.8623 - val_mse: 145.8624\n",
      "Epoch 106/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 121.0155 - mse: 121.0154 - val_loss: 144.9017 - val_mse: 144.9017\n",
      "Epoch 107/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 118.6070 - mse: 118.6070 - val_loss: 141.0155 - val_mse: 141.0155\n",
      "Epoch 108/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 114.6911 - mse: 114.6911 - val_loss: 138.6890 - val_mse: 138.6890\n",
      "Epoch 109/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 122.1837 - mse: 122.1837 - val_loss: 155.1132 - val_mse: 155.1132\n",
      "Epoch 110/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 127.4252 - mse: 127.4252 - val_loss: 150.0574 - val_mse: 150.0574\n",
      "Epoch 111/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 122.8929 - mse: 122.8929 - val_loss: 162.9741 - val_mse: 162.9741\n",
      "Epoch 112/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 122.2347 - mse: 122.2347 - val_loss: 142.5501 - val_mse: 142.5501\n",
      "Epoch 113/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 117.1210 - mse: 117.1210 - val_loss: 148.9836 - val_mse: 148.9836\n",
      "Epoch 114/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 125.4803 - mse: 125.4803 - val_loss: 151.2494 - val_mse: 151.2495\n",
      "Epoch 115/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 125.7315 - mse: 125.7315 - val_loss: 157.7114 - val_mse: 157.7114\n",
      "Epoch 116/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 159.4411 - mse: 159.4411 - val_loss: 197.3977 - val_mse: 197.3976\n",
      "Epoch 117/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 156.3185 - mse: 156.3185 - val_loss: 210.3372 - val_mse: 210.3372\n",
      "Epoch 118/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 153.0630 - mse: 153.0630 - val_loss: 186.8321 - val_mse: 186.8322\n",
      "[CV] ....................................... nl=2, nn=6, total=  36.6s\n",
      "[CV] nl=2, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 407us/sample - loss: 477.7245 - mse: 477.7246 - val_loss: 641.9061 - val_mse: 641.9060\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 401.7274 - mse: 401.7274 - val_loss: 540.2798 - val_mse: 540.2797\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 328.9831 - mse: 328.9831 - val_loss: 399.6818 - val_mse: 399.6819\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 267.1598 - mse: 267.1598 - val_loss: 310.2433 - val_mse: 310.2433\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 232.2350 - mse: 232.2350 - val_loss: 268.3782 - val_mse: 268.3782\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 210.0923 - mse: 210.0923 - val_loss: 273.6884 - val_mse: 273.6884\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 204.8551 - mse: 204.8551 - val_loss: 248.4280 - val_mse: 248.4280\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 197.2371 - mse: 197.2371 - val_loss: 237.5394 - val_mse: 237.5394\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 184.4293 - mse: 184.4293 - val_loss: 210.8123 - val_mse: 210.8123\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 185.2911 - mse: 185.2911 - val_loss: 221.2269 - val_mse: 221.2270\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 183.8971 - mse: 183.8971 - val_loss: 225.1736 - val_mse: 225.1736\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 176.3145 - mse: 176.3145 - val_loss: 211.8628 - val_mse: 211.8627\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 183.5660 - mse: 183.5661 - val_loss: 189.9122 - val_mse: 189.9122\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 169.7218 - mse: 169.7217 - val_loss: 228.6744 - val_mse: 228.6743\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 171.4063 - mse: 171.4063 - val_loss: 346.5509 - val_mse: 346.5508\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 167.4743 - mse: 167.4743 - val_loss: 198.1481 - val_mse: 198.1481\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 164.4065 - mse: 164.4064 - val_loss: 190.6898 - val_mse: 190.6898\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 164.0763 - mse: 164.0763 - val_loss: 220.2779 - val_mse: 220.2779\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 154.5443 - mse: 154.5443 - val_loss: 190.2834 - val_mse: 190.2834\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 155.8937 - mse: 155.8937 - val_loss: 193.2821 - val_mse: 193.2821\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 150.2974 - mse: 150.2974 - val_loss: 207.5195 - val_mse: 207.5195\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 159.9621 - mse: 159.9622 - val_loss: 184.7385 - val_mse: 184.7385\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 151.1935 - mse: 151.1934 - val_loss: 190.8948 - val_mse: 190.8948\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 149us/sample - loss: 146.5864 - mse: 146.5864 - val_loss: 214.3610 - val_mse: 214.3610\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 149.0329 - mse: 149.0329 - val_loss: 207.3737 - val_mse: 207.3737\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 148.4137 - mse: 148.4137 - val_loss: 200.7024 - val_mse: 200.7024\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 143.2448 - mse: 143.2448 - val_loss: 205.5030 - val_mse: 205.5030\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 151.5724 - mse: 151.5724 - val_loss: 229.9448 - val_mse: 229.9448\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 150.9290 - mse: 150.9290 - val_loss: 199.8189 - val_mse: 199.8189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 143.4152 - mse: 143.4152 - val_loss: 200.4888 - val_mse: 200.4888\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 139.5993 - mse: 139.5993 - val_loss: 210.4676 - val_mse: 210.4676\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 140.0514 - mse: 140.0514 - val_loss: 198.3183 - val_mse: 198.3183\n",
      "[CV] ...................................... nl=2, nn=12, total=  11.3s\n",
      "[CV] nl=2, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 310us/sample - loss: 452.8854 - mse: 452.8855 - val_loss: 642.6128 - val_mse: 642.6128\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 363.7946 - mse: 363.7946 - val_loss: 528.5818 - val_mse: 528.5818\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 278.4482 - mse: 278.4482 - val_loss: 385.7916 - val_mse: 385.7916\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 213.5054 - mse: 213.5053 - val_loss: 297.3456 - val_mse: 297.3456\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 184.5124 - mse: 184.5124 - val_loss: 238.3748 - val_mse: 238.3748\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 164.2042 - mse: 164.2043 - val_loss: 227.2460 - val_mse: 227.2459\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 163.5560 - mse: 163.5560 - val_loss: 213.5873 - val_mse: 213.5873\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 156.8089 - mse: 156.8089 - val_loss: 198.6671 - val_mse: 198.6671\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 156.9876 - mse: 156.9876 - val_loss: 190.7212 - val_mse: 190.7212\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 151.7166 - mse: 151.7166 - val_loss: 199.2303 - val_mse: 199.2303\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.7360 - mse: 151.7360 - val_loss: 186.1541 - val_mse: 186.1542\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 148.8498 - mse: 148.8499 - val_loss: 185.3497 - val_mse: 185.3497\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 153.0412 - mse: 153.0413 - val_loss: 185.8078 - val_mse: 185.8078\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 151.1314 - mse: 151.1314 - val_loss: 184.2358 - val_mse: 184.2358\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 150.7694 - mse: 150.7694 - val_loss: 191.6145 - val_mse: 191.6145\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 140.5264 - mse: 140.5264 - val_loss: 180.0017 - val_mse: 180.0017\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 145.9384 - mse: 145.9384 - val_loss: 180.4793 - val_mse: 180.4793\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 143.1781 - mse: 143.1781 - val_loss: 178.4122 - val_mse: 178.4122\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 143.9216 - mse: 143.9216 - val_loss: 181.3211 - val_mse: 181.3211\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 140.5244 - mse: 140.5244 - val_loss: 191.9141 - val_mse: 191.9142\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 134.5367 - mse: 134.5367 - val_loss: 171.6239 - val_mse: 171.6239\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 132.0583 - mse: 132.0583 - val_loss: 173.4247 - val_mse: 173.4247\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 135.8355 - mse: 135.8355 - val_loss: 172.6539 - val_mse: 172.6540\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 137.0005 - mse: 137.0005 - val_loss: 169.4400 - val_mse: 169.4400\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 137.8268 - mse: 137.8268 - val_loss: 180.4156 - val_mse: 180.4156\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 127.7343 - mse: 127.7344 - val_loss: 172.0485 - val_mse: 172.0485\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 128.6415 - mse: 128.6414 - val_loss: 173.2220 - val_mse: 173.2220\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 126.1266 - mse: 126.1266 - val_loss: 175.3253 - val_mse: 175.3253\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 134.3728 - mse: 134.3728 - val_loss: 172.1670 - val_mse: 172.1671\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 130.3029 - mse: 130.3028 - val_loss: 168.6752 - val_mse: 168.6752\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 126.0049 - mse: 126.0049 - val_loss: 174.2141 - val_mse: 174.2141\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 125.4523 - mse: 125.4523 - val_loss: 175.4807 - val_mse: 175.4807\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 127.4894 - mse: 127.4893 - val_loss: 172.0136 - val_mse: 172.0136\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 128.0863 - mse: 128.0862 - val_loss: 166.0953 - val_mse: 166.0953\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 123.5623 - mse: 123.5624 - val_loss: 177.1219 - val_mse: 177.1219\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 118.7109 - mse: 118.7109 - val_loss: 176.0205 - val_mse: 176.0205\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 124.8593 - mse: 124.8593 - val_loss: 170.7036 - val_mse: 170.7036\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 126.9681 - mse: 126.9681 - val_loss: 177.7789 - val_mse: 177.7789\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 121.9324 - mse: 121.9324 - val_loss: 174.1026 - val_mse: 174.1026\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 125.0733 - mse: 125.0733 - val_loss: 174.4054 - val_mse: 174.4054\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 119.6517 - mse: 119.6517 - val_loss: 175.6587 - val_mse: 175.6587\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 122.4979 - mse: 122.4979 - val_loss: 177.2729 - val_mse: 177.2730\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 117.0999 - mse: 117.0999 - val_loss: 184.1861 - val_mse: 184.1861\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 118.8683 - mse: 118.8683 - val_loss: 175.7705 - val_mse: 175.7705\n",
      "[CV] ...................................... nl=2, nn=12, total=  14.1s\n",
      "[CV] nl=2, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 314us/sample - loss: 479.7695 - mse: 479.7694 - val_loss: 632.2462 - val_mse: 632.2462\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 396.9225 - mse: 396.9225 - val_loss: 508.5066 - val_mse: 508.5065\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 319.6037 - mse: 319.6037 - val_loss: 390.9829 - val_mse: 390.9829\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 261.2423 - mse: 261.2422 - val_loss: 291.8358 - val_mse: 291.8357\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 232.3999 - mse: 232.4000 - val_loss: 255.1854 - val_mse: 255.1855\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 207.0524 - mse: 207.0524 - val_loss: 235.1559 - val_mse: 235.1559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 202.5473 - mse: 202.5472 - val_loss: 231.5262 - val_mse: 231.5262\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 196.5863 - mse: 196.5864 - val_loss: 225.7505 - val_mse: 225.7505\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 193.4375 - mse: 193.4375 - val_loss: 218.1632 - val_mse: 218.1632\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 186.6870 - mse: 186.6869 - val_loss: 244.4756 - val_mse: 244.4756\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 191.5317 - mse: 191.5317 - val_loss: 218.5343 - val_mse: 218.5343\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - ETA: 0s - loss: 153.2869 - mse: 153.286 - 0s 106us/sample - loss: 182.8939 - mse: 182.8939 - val_loss: 261.2469 - val_mse: 261.2469\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 177.0041 - mse: 177.0042 - val_loss: 226.1450 - val_mse: 226.1450\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 178.9986 - mse: 178.9986 - val_loss: 207.4252 - val_mse: 207.4251\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 175.1908 - mse: 175.1908 - val_loss: 208.6701 - val_mse: 208.6701\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 182.6757 - mse: 182.6757 - val_loss: 204.0540 - val_mse: 204.0540\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 169.5121 - mse: 169.5121 - val_loss: 211.5624 - val_mse: 211.5625\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 169.4881 - mse: 169.4881 - val_loss: 204.5807 - val_mse: 204.5807\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 181.0893 - mse: 181.0893 - val_loss: 206.4901 - val_mse: 206.4901\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 170.4826 - mse: 170.4826 - val_loss: 207.6521 - val_mse: 207.6521\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 166.7894 - mse: 166.7894 - val_loss: 199.2808 - val_mse: 199.2808\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 162.6308 - mse: 162.6308 - val_loss: 204.9736 - val_mse: 204.9736\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 166.7465 - mse: 166.7465 - val_loss: 200.5725 - val_mse: 200.5724\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 164.6943 - mse: 164.6943 - val_loss: 196.7398 - val_mse: 196.7398\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 162.5471 - mse: 162.5470 - val_loss: 194.3384 - val_mse: 194.3384\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 173.5133 - mse: 173.5133 - val_loss: 216.7728 - val_mse: 216.7728\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 165.4764 - mse: 165.4763 - val_loss: 189.2408 - val_mse: 189.2408\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 163.5047 - mse: 163.5047 - val_loss: 299.8390 - val_mse: 299.8390\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 172.7845 - mse: 172.7846 - val_loss: 219.1953 - val_mse: 219.1953\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 164.5944 - mse: 164.5944 - val_loss: 183.1443 - val_mse: 183.1443\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 156.2426 - mse: 156.2426 - val_loss: 237.6729 - val_mse: 237.6728\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 161.3313 - mse: 161.3313 - val_loss: 203.2477 - val_mse: 203.2477\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 156.1708 - mse: 156.1708 - val_loss: 196.6916 - val_mse: 196.6916\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 157.9084 - mse: 157.9084 - val_loss: 181.5128 - val_mse: 181.5128\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 149.7334 - mse: 149.7333 - val_loss: 175.6477 - val_mse: 175.6477\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 152.3327 - mse: 152.3327 - val_loss: 238.7195 - val_mse: 238.7195\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 152.3406 - mse: 152.3406 - val_loss: 186.0249 - val_mse: 186.0249\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 150.7458 - mse: 150.7458 - val_loss: 177.1531 - val_mse: 177.1531\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 151.7291 - mse: 151.7291 - val_loss: 174.6034 - val_mse: 174.6034\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 149.1377 - mse: 149.1376 - val_loss: 213.8175 - val_mse: 213.8175\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 150.9389 - mse: 150.9388 - val_loss: 248.4609 - val_mse: 248.4609\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 141.2074 - mse: 141.2075 - val_loss: 167.7704 - val_mse: 167.7704\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 156.2680 - mse: 156.2679 - val_loss: 174.5480 - val_mse: 174.5480\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 149.8667 - mse: 149.8667 - val_loss: 214.6739 - val_mse: 214.6739\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.7212 - mse: 143.7212 - val_loss: 223.2864 - val_mse: 223.2865\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 144.6127 - mse: 144.6127 - val_loss: 202.1455 - val_mse: 202.1455\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 144.6819 - mse: 144.6819 - val_loss: 195.2043 - val_mse: 195.2043\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 147.9806 - mse: 147.9806 - val_loss: 175.7609 - val_mse: 175.7609\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 141.4628 - mse: 141.4628 - val_loss: 230.4159 - val_mse: 230.4160\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 138.6402 - mse: 138.6402 - val_loss: 203.8443 - val_mse: 203.8443\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 145.4334 - mse: 145.4334 - val_loss: 152.8517 - val_mse: 152.8517\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 133.4371 - mse: 133.4371 - val_loss: 164.8350 - val_mse: 164.8350\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 131.0086 - mse: 131.0085 - val_loss: 180.0206 - val_mse: 180.0206\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 149.4195 - mse: 149.4195 - val_loss: 154.0730 - val_mse: 154.0729\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 138.4975 - mse: 138.4975 - val_loss: 197.2738 - val_mse: 197.2739\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 145.8287 - mse: 145.8286 - val_loss: 341.5289 - val_mse: 341.5289\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 130.7219 - mse: 130.7219 - val_loss: 178.1146 - val_mse: 178.1146\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 134.9204 - mse: 134.9204 - val_loss: 278.8376 - val_mse: 278.8376\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 130.7024 - mse: 130.7024 - val_loss: 159.7208 - val_mse: 159.7208\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 127.9190 - mse: 127.9190 - val_loss: 171.8668 - val_mse: 171.8668\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 130.5045 - mse: 130.5045 - val_loss: 157.8283 - val_mse: 157.8283\n",
      "[CV] ...................................... nl=2, nn=12, total=  19.2s\n",
      "[CV] nl=2, nn=12 .....................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 312us/sample - loss: 425.1736 - mse: 425.1736 - val_loss: 505.2871 - val_mse: 505.2872\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 330.7803 - mse: 330.7804 - val_loss: 381.7177 - val_mse: 381.7177\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 262.5906 - mse: 262.5906 - val_loss: 250.7785 - val_mse: 250.7785\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 215.1794 - mse: 215.1794 - val_loss: 176.0951 - val_mse: 176.0952\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 183.4881 - mse: 183.4882 - val_loss: 154.4788 - val_mse: 154.4788\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 173.5784 - mse: 173.5783 - val_loss: 145.3359 - val_mse: 145.3359\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 174.2178 - mse: 174.2178 - val_loss: 144.7403 - val_mse: 144.7403\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 163.9959 - mse: 163.9958 - val_loss: 147.3320 - val_mse: 147.3320\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 166.5889 - mse: 166.5889 - val_loss: 138.6584 - val_mse: 138.6584\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 161.2984 - mse: 161.2984 - val_loss: 141.3349 - val_mse: 141.3349\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 161.6595 - mse: 161.6595 - val_loss: 128.9893 - val_mse: 128.9893\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 154.4420 - mse: 154.4420 - val_loss: 126.8067 - val_mse: 126.8067\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 160.1508 - mse: 160.1508 - val_loss: 131.8139 - val_mse: 131.8140\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 155.6474 - mse: 155.6474 - val_loss: 134.2472 - val_mse: 134.2472\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 150.4905 - mse: 150.4905 - val_loss: 128.0554 - val_mse: 128.0555\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 149.8963 - mse: 149.8963 - val_loss: 120.6478 - val_mse: 120.6478\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 155.2950 - mse: 155.2950 - val_loss: 128.2915 - val_mse: 128.2915\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 100us/sample - loss: 151.4998 - mse: 151.4999 - val_loss: 128.7546 - val_mse: 128.7546\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 156.5094 - mse: 156.5094 - val_loss: 119.8401 - val_mse: 119.8401\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.6019 - mse: 143.6018 - val_loss: 134.3257 - val_mse: 134.3257\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 140.5132 - mse: 140.5132 - val_loss: 129.6524 - val_mse: 129.6524\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 145.1553 - mse: 145.1553 - val_loss: 126.9821 - val_mse: 126.9821\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 139.9564 - mse: 139.9564 - val_loss: 116.7969 - val_mse: 116.7969\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 141.5383 - mse: 141.5383 - val_loss: 119.3120 - val_mse: 119.3120\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 141.0382 - mse: 141.0382 - val_loss: 122.8160 - val_mse: 122.8160\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.7130 - mse: 144.7130 - val_loss: 120.3926 - val_mse: 120.3926\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 138.9875 - mse: 138.9875 - val_loss: 124.3105 - val_mse: 124.3105\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 144.3566 - mse: 144.3566 - val_loss: 129.3832 - val_mse: 129.3832\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 136.1348 - mse: 136.1348 - val_loss: 116.4201 - val_mse: 116.4201\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 130.5062 - mse: 130.5062 - val_loss: 124.2118 - val_mse: 124.2118\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 101us/sample - loss: 140.2238 - mse: 140.2237 - val_loss: 119.3903 - val_mse: 119.3903\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 138.3292 - mse: 138.3291 - val_loss: 113.8299 - val_mse: 113.8299\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 133.5183 - mse: 133.5182 - val_loss: 116.2464 - val_mse: 116.2464\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 136.5416 - mse: 136.5416 - val_loss: 116.9081 - val_mse: 116.9080\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 143.6369 - mse: 143.6370 - val_loss: 128.9030 - val_mse: 128.9030\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 138.3437 - mse: 138.3437 - val_loss: 114.6610 - val_mse: 114.6610\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 140.0495 - mse: 140.0495 - val_loss: 125.3468 - val_mse: 125.3468\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 137.4483 - mse: 137.4483 - val_loss: 117.4128 - val_mse: 117.4128\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 137.2012 - mse: 137.2012 - val_loss: 116.0551 - val_mse: 116.0551\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 131.4981 - mse: 131.4981 - val_loss: 117.4585 - val_mse: 117.4585\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 138.3233 - mse: 138.3233 - val_loss: 123.3457 - val_mse: 123.3457\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 124us/sample - loss: 127.2743 - mse: 127.2743 - val_loss: 115.7756 - val_mse: 115.7756\n",
      "[CV] ...................................... nl=2, nn=12, total=  13.4s\n",
      "[CV] nl=2, nn=12 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 425us/sample - loss: 451.8018 - mse: 451.8017 - val_loss: 617.1329 - val_mse: 617.1329\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 369.8148 - mse: 369.8148 - val_loss: 491.5764 - val_mse: 491.5764\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 290.5340 - mse: 290.5341 - val_loss: 377.9502 - val_mse: 377.9503\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 231.3791 - mse: 231.3790 - val_loss: 255.3092 - val_mse: 255.3092\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 196.1895 - mse: 196.1895 - val_loss: 237.9588 - val_mse: 237.9588\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 176.3599 - mse: 176.3599 - val_loss: 213.6660 - val_mse: 213.6660\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 173.8721 - mse: 173.8722 - val_loss: 205.6514 - val_mse: 205.6514\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 167.6342 - mse: 167.6342 - val_loss: 204.7218 - val_mse: 204.7218\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 160.3520 - mse: 160.3520 - val_loss: 197.7686 - val_mse: 197.7686\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 165.3727 - mse: 165.3728 - val_loss: 193.0566 - val_mse: 193.0566\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 158.4056 - mse: 158.4055 - val_loss: 189.5838 - val_mse: 189.5838\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 164.3842 - mse: 164.3842 - val_loss: 193.3157 - val_mse: 193.3157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 161.8875 - mse: 161.8876 - val_loss: 195.1028 - val_mse: 195.1028\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 162.1656 - mse: 162.1656 - val_loss: 185.4161 - val_mse: 185.4161\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 156.5168 - mse: 156.5168 - val_loss: 188.3359 - val_mse: 188.3359\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 162.3994 - mse: 162.3993 - val_loss: 190.5418 - val_mse: 190.5418\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 158.5306 - mse: 158.5305 - val_loss: 184.5452 - val_mse: 184.5452\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 154.2588 - mse: 154.2588 - val_loss: 182.8198 - val_mse: 182.8198\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 154.0563 - mse: 154.0563 - val_loss: 182.6143 - val_mse: 182.6143\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 155.9933 - mse: 155.9933 - val_loss: 180.9438 - val_mse: 180.9438\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 151.6157 - mse: 151.6157 - val_loss: 181.9250 - val_mse: 181.9251\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 147.7252 - mse: 147.7252 - val_loss: 176.3269 - val_mse: 176.3268\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 144.4496 - mse: 144.4496 - val_loss: 188.6805 - val_mse: 188.6804\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 151.9735 - mse: 151.9734 - val_loss: 176.4372 - val_mse: 176.4372\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 145.9782 - mse: 145.9782 - val_loss: 171.5975 - val_mse: 171.5975\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 146.7225 - mse: 146.7225 - val_loss: 166.1145 - val_mse: 166.1145\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 144.6413 - mse: 144.6413 - val_loss: 172.3759 - val_mse: 172.3759\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 140.0952 - mse: 140.0952 - val_loss: 162.4610 - val_mse: 162.4610\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.8533 - mse: 143.8532 - val_loss: 167.0706 - val_mse: 167.0707\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 143.5228 - mse: 143.5227 - val_loss: 165.9682 - val_mse: 165.9682\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 143.7790 - mse: 143.7790 - val_loss: 161.1713 - val_mse: 161.1713\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 142.9647 - mse: 142.9646 - val_loss: 160.4051 - val_mse: 160.4051\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 143.0959 - mse: 143.0958 - val_loss: 167.3777 - val_mse: 167.3777\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 135.8169 - mse: 135.8168 - val_loss: 159.7732 - val_mse: 159.7732\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 139.9396 - mse: 139.9396 - val_loss: 160.8495 - val_mse: 160.8494\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 139.5620 - mse: 139.5620 - val_loss: 163.0647 - val_mse: 163.0647\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 134.0377 - mse: 134.0377 - val_loss: 172.5563 - val_mse: 172.5563\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 135.5595 - mse: 135.5595 - val_loss: 158.4866 - val_mse: 158.4866\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 138.3572 - mse: 138.3572 - val_loss: 164.2873 - val_mse: 164.2872\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 141.6533 - mse: 141.6533 - val_loss: 166.2104 - val_mse: 166.2104\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 140.4867 - mse: 140.4867 - val_loss: 159.9052 - val_mse: 159.9052\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 135.9710 - mse: 135.9710 - val_loss: 161.9797 - val_mse: 161.9796\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 135.2113 - mse: 135.2112 - val_loss: 156.5457 - val_mse: 156.5457\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 134.3463 - mse: 134.3462 - val_loss: 160.8461 - val_mse: 160.8461\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 138.9702 - mse: 138.9702 - val_loss: 157.4295 - val_mse: 157.4295\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 131.3552 - mse: 131.3552 - val_loss: 157.0744 - val_mse: 157.0744\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 132.5344 - mse: 132.5344 - val_loss: 158.5251 - val_mse: 158.5251\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 103us/sample - loss: 136.5566 - mse: 136.5567 - val_loss: 160.8292 - val_mse: 160.8292\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 129.5484 - mse: 129.5484 - val_loss: 167.3208 - val_mse: 167.3208\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 102us/sample - loss: 133.0061 - mse: 133.0061 - val_loss: 160.9084 - val_mse: 160.9084\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 134.4007 - mse: 134.4007 - val_loss: 159.9067 - val_mse: 159.9067\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 133.5189 - mse: 133.5190 - val_loss: 159.9055 - val_mse: 159.9055\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 133.0711 - mse: 133.0711 - val_loss: 158.3738 - val_mse: 158.3739\n",
      "[CV] ...................................... nl=2, nn=12, total=  17.6s\n",
      "[CV] nl=2, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 316us/sample - loss: 438.3751 - mse: 438.3751 - val_loss: 623.7804 - val_mse: 623.7805\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 318.3405 - mse: 318.3405 - val_loss: 426.3575 - val_mse: 426.3575\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 243.2424 - mse: 243.2424 - val_loss: 289.2961 - val_mse: 289.2960\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 213.4025 - mse: 213.4024 - val_loss: 253.3655 - val_mse: 253.3655\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 195.6399 - mse: 195.6399 - val_loss: 244.9517 - val_mse: 244.9517\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 185.6518 - mse: 185.6518 - val_loss: 224.6193 - val_mse: 224.6192\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 179.3372 - mse: 179.3372 - val_loss: 207.3518 - val_mse: 207.3519\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 175.6722 - mse: 175.6722 - val_loss: 200.0957 - val_mse: 200.0957\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 174.6294 - mse: 174.6293 - val_loss: 204.1618 - val_mse: 204.1618\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 166.4850 - mse: 166.4850 - val_loss: 204.4604 - val_mse: 204.4604\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 167.0905 - mse: 167.0905 - val_loss: 212.1685 - val_mse: 212.1685\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 159.2142 - mse: 159.2142 - val_loss: 208.0920 - val_mse: 208.0920\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 161.7990 - mse: 161.7990 - val_loss: 209.2106 - val_mse: 209.2106\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 152.1782 - mse: 152.1782 - val_loss: 201.0743 - val_mse: 201.0742\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 154.3992 - mse: 154.3992 - val_loss: 214.6026 - val_mse: 214.6026\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 157.5026 - mse: 157.5026 - val_loss: 224.2639 - val_mse: 224.2639\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 149.4520 - mse: 149.4520 - val_loss: 221.3919 - val_mse: 221.3919\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 155.2587 - mse: 155.2587 - val_loss: 215.4572 - val_mse: 215.4573\n",
      "[CV] ...................................... nl=2, nn=24, total=   6.4s\n",
      "[CV] nl=2, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 441us/sample - loss: 417.4478 - mse: 417.4477 - val_loss: 611.6569 - val_mse: 611.6570\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 145us/sample - loss: 305.7596 - mse: 305.7595 - val_loss: 499.8443 - val_mse: 499.8441\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 205.0267 - mse: 205.0267 - val_loss: 340.6607 - val_mse: 340.6607\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 171.9235 - mse: 171.9235 - val_loss: 243.4223 - val_mse: 243.4223\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 162.1057 - mse: 162.1057 - val_loss: 231.8816 - val_mse: 231.8816\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 157.0956 - mse: 157.0956 - val_loss: 220.0258 - val_mse: 220.0258\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 159.4378 - mse: 159.4378 - val_loss: 214.8837 - val_mse: 214.8837\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 154.4569 - mse: 154.4569 - val_loss: 209.0490 - val_mse: 209.0490\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 158.0853 - mse: 158.0853 - val_loss: 215.7412 - val_mse: 215.7412\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 150.2772 - mse: 150.2772 - val_loss: 215.9567 - val_mse: 215.9566\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 152.2929 - mse: 152.2928 - val_loss: 197.5893 - val_mse: 197.5893\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 144.2561 - mse: 144.2561 - val_loss: 208.8472 - val_mse: 208.8472\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 139.0425 - mse: 139.0425 - val_loss: 199.2256 - val_mse: 199.2256\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 118us/sample - loss: 144.2919 - mse: 144.2919 - val_loss: 191.9387 - val_mse: 191.9387\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 138.8767 - mse: 138.8767 - val_loss: 189.2199 - val_mse: 189.2199\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 133us/sample - loss: 135.5787 - mse: 135.5788 - val_loss: 189.2368 - val_mse: 189.2368\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 139.5108 - mse: 139.5108 - val_loss: 191.6869 - val_mse: 191.6870\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 137.8191 - mse: 137.8191 - val_loss: 187.0889 - val_mse: 187.0889\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 137.0848 - mse: 137.0849 - val_loss: 210.1765 - val_mse: 210.1766\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 143.1676 - mse: 143.1676 - val_loss: 199.3728 - val_mse: 199.3728\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 133.6508 - mse: 133.6509 - val_loss: 188.5072 - val_mse: 188.5072\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 141.7979 - mse: 141.7980 - val_loss: 189.7760 - val_mse: 189.7760\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 130.4478 - mse: 130.4478 - val_loss: 187.2631 - val_mse: 187.2631\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 138.6993 - mse: 138.6993 - val_loss: 186.7187 - val_mse: 186.7187\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 133.8436 - mse: 133.8435 - val_loss: 192.5872 - val_mse: 192.5872\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 132.8192 - mse: 132.8192 - val_loss: 194.6632 - val_mse: 194.6633\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 120us/sample - loss: 128.1217 - mse: 128.1217 - val_loss: 188.6811 - val_mse: 188.6811\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 135.3638 - mse: 135.3638 - val_loss: 193.1794 - val_mse: 193.1794\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 129.7440 - mse: 129.7439 - val_loss: 198.9166 - val_mse: 198.9166\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 124us/sample - loss: 131.7648 - mse: 131.7648 - val_loss: 189.4796 - val_mse: 189.4796\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 125.1011 - mse: 125.1011 - val_loss: 183.2084 - val_mse: 183.2084\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 128us/sample - loss: 128.7505 - mse: 128.7505 - val_loss: 191.1472 - val_mse: 191.1472\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 127.2807 - mse: 127.2807 - val_loss: 187.4600 - val_mse: 187.4601\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 127.2285 - mse: 127.2285 - val_loss: 194.0175 - val_mse: 194.0175\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 134.1683 - mse: 134.1682 - val_loss: 191.6003 - val_mse: 191.6003\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 124.5584 - mse: 124.5584 - val_loss: 192.1715 - val_mse: 192.1715\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 127.4016 - mse: 127.4016 - val_loss: 182.3520 - val_mse: 182.3521\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 130.9090 - mse: 130.9090 - val_loss: 183.1007 - val_mse: 183.1007\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 127.6330 - mse: 127.6330 - val_loss: 179.8882 - val_mse: 179.8882\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 124.6782 - mse: 124.6782 - val_loss: 193.0356 - val_mse: 193.0356\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 123.9552 - mse: 123.9553 - val_loss: 187.6429 - val_mse: 187.6429\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 130.0706 - mse: 130.0706 - val_loss: 188.8708 - val_mse: 188.8708\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 123.3760 - mse: 123.3759 - val_loss: 186.8511 - val_mse: 186.8511\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 126.1010 - mse: 126.1010 - val_loss: 179.0563 - val_mse: 179.0563\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 127.4933 - mse: 127.4933 - val_loss: 178.0095 - val_mse: 178.0095\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 120.9707 - mse: 120.9707 - val_loss: 183.7010 - val_mse: 183.7010\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 116.3706 - mse: 116.3705 - val_loss: 197.3395 - val_mse: 197.3395\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 121.9887 - mse: 121.9887 - val_loss: 192.7700 - val_mse: 192.7701\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 117.3932 - mse: 117.3932 - val_loss: 174.5596 - val_mse: 174.5595\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 113us/sample - loss: 119.0069 - mse: 119.0069 - val_loss: 175.8424 - val_mse: 175.8424\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 124.3312 - mse: 124.3312 - val_loss: 176.7317 - val_mse: 176.7317\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 127.6798 - mse: 127.6798 - val_loss: 172.3977 - val_mse: 172.3977\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 116.6338 - mse: 116.6338 - val_loss: 176.4649 - val_mse: 176.4649\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 117.9149 - mse: 117.9149 - val_loss: 188.8967 - val_mse: 188.8967\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 115.1428 - mse: 115.1428 - val_loss: 194.4415 - val_mse: 194.4415\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 126.5460 - mse: 126.5460 - val_loss: 183.8526 - val_mse: 183.8526\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 114.9456 - mse: 114.9456 - val_loss: 191.3572 - val_mse: 191.3572\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 116.9218 - mse: 116.9218 - val_loss: 173.1195 - val_mse: 173.1196\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 118.2848 - mse: 118.2848 - val_loss: 163.0918 - val_mse: 163.0918\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 116.5096 - mse: 116.5095 - val_loss: 173.2669 - val_mse: 173.2668\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 134us/sample - loss: 118.6533 - mse: 118.6533 - val_loss: 171.8549 - val_mse: 171.8549\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 107.1526 - mse: 107.1526 - val_loss: 210.1240 - val_mse: 210.1240\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 115.9471 - mse: 115.9471 - val_loss: 178.4240 - val_mse: 178.4240\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 110.9709 - mse: 110.9709 - val_loss: 179.9747 - val_mse: 179.9747\n",
      "Epoch 65/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 112.6715 - mse: 112.6715 - val_loss: 174.5332 - val_mse: 174.5332\n",
      "Epoch 66/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 110.9689 - mse: 110.9689 - val_loss: 172.6571 - val_mse: 172.6572\n",
      "Epoch 67/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 119.6594 - mse: 119.6594 - val_loss: 164.6006 - val_mse: 164.6006\n",
      "Epoch 68/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 111.0861 - mse: 111.0861 - val_loss: 187.1282 - val_mse: 187.1282\n",
      "Epoch 69/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 114.1715 - mse: 114.1715 - val_loss: 197.8747 - val_mse: 197.8747\n",
      "[CV] ...................................... nl=2, nn=24, total=  23.7s\n",
      "[CV] nl=2, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 325us/sample - loss: 499.9087 - mse: 499.9086 - val_loss: 636.0622 - val_mse: 636.0621\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 351.8619 - mse: 351.8619 - val_loss: 492.7937 - val_mse: 492.7937\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 257.9703 - mse: 257.9703 - val_loss: 324.5322 - val_mse: 324.5322\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 204.9981 - mse: 204.9981 - val_loss: 234.4892 - val_mse: 234.4891\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 196.5485 - mse: 196.5486 - val_loss: 208.7027 - val_mse: 208.7027\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 185.5072 - mse: 185.5072 - val_loss: 207.6374 - val_mse: 207.6375\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 182.4394 - mse: 182.4394 - val_loss: 190.4663 - val_mse: 190.4663\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 176.2729 - mse: 176.2729 - val_loss: 193.4050 - val_mse: 193.4050\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 183.6679 - mse: 183.6679 - val_loss: 176.9957 - val_mse: 176.9957\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 177.9863 - mse: 177.9863 - val_loss: 190.5703 - val_mse: 190.5703\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 166.1360 - mse: 166.1360 - val_loss: 193.3593 - val_mse: 193.3593\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 169.0176 - mse: 169.0176 - val_loss: 194.3768 - val_mse: 194.3768\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 163.9975 - mse: 163.9975 - val_loss: 168.9715 - val_mse: 168.9715\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 167.5432 - mse: 167.5432 - val_loss: 178.5983 - val_mse: 178.5983\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 123us/sample - loss: 167.7104 - mse: 167.7104 - val_loss: 163.3876 - val_mse: 163.3876\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 162.4711 - mse: 162.4711 - val_loss: 161.5190 - val_mse: 161.5190\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 158.6651 - mse: 158.6651 - val_loss: 158.4080 - val_mse: 158.4080\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 165.3607 - mse: 165.3607 - val_loss: 152.3218 - val_mse: 152.3218\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 159.9323 - mse: 159.9323 - val_loss: 158.7431 - val_mse: 158.7431\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 151.7799 - mse: 151.7798 - val_loss: 157.2297 - val_mse: 157.2298\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 161.9578 - mse: 161.9578 - val_loss: 191.3966 - val_mse: 191.3966\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 158.7430 - mse: 158.7431 - val_loss: 153.9948 - val_mse: 153.9949\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 154.1013 - mse: 154.1013 - val_loss: 162.5765 - val_mse: 162.5765\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 158.2488 - mse: 158.2488 - val_loss: 153.7275 - val_mse: 153.7275\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 159.0387 - mse: 159.0387 - val_loss: 155.4812 - val_mse: 155.4812\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 145.1153 - mse: 145.1153 - val_loss: 139.8962 - val_mse: 139.8962\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 148.5962 - mse: 148.5962 - val_loss: 139.9748 - val_mse: 139.9747\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 141.3483 - mse: 141.3483 - val_loss: 164.5483 - val_mse: 164.5482\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 151.8046 - mse: 151.8046 - val_loss: 128.5827 - val_mse: 128.5827\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 146.3943 - mse: 146.3942 - val_loss: 141.0726 - val_mse: 141.0726\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 142.5244 - mse: 142.5244 - val_loss: 132.7102 - val_mse: 132.7102\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 148.8820 - mse: 148.8820 - val_loss: 125.8114 - val_mse: 125.8114\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 137.7607 - mse: 137.7607 - val_loss: 126.9049 - val_mse: 126.9049\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 141.6116 - mse: 141.6115 - val_loss: 141.8542 - val_mse: 141.8541\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 137.0435 - mse: 137.0435 - val_loss: 117.3208 - val_mse: 117.3208\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 131.5274 - mse: 131.5274 - val_loss: 153.8990 - val_mse: 153.8990\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 139.3955 - mse: 139.3954 - val_loss: 123.8855 - val_mse: 123.8856\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 137.0365 - mse: 137.0365 - val_loss: 140.8972 - val_mse: 140.8972\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 129.8619 - mse: 129.8618 - val_loss: 114.1147 - val_mse: 114.1147\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 131.7785 - mse: 131.7785 - val_loss: 121.1775 - val_mse: 121.1775\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 136.6744 - mse: 136.6744 - val_loss: 149.5789 - val_mse: 149.5788\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 104us/sample - loss: 135.1774 - mse: 135.1774 - val_loss: 113.7357 - val_mse: 113.7357\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 128.6229 - mse: 128.6229 - val_loss: 136.4159 - val_mse: 136.4159\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 130us/sample - loss: 132.4880 - mse: 132.4881 - val_loss: 233.1058 - val_mse: 233.1058\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 137us/sample - loss: 131.8925 - mse: 131.8925 - val_loss: 149.7867 - val_mse: 149.7868\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 132.9915 - mse: 132.9915 - val_loss: 170.6561 - val_mse: 170.6561\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 131.4864 - mse: 131.4864 - val_loss: 114.5636 - val_mse: 114.5636\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 130.5971 - mse: 130.5970 - val_loss: 153.5403 - val_mse: 153.5403\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 130.5897 - mse: 130.5897 - val_loss: 127.4686 - val_mse: 127.4686\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 123.2708 - mse: 123.2708 - val_loss: 132.0508 - val_mse: 132.0508\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 123.1547 - mse: 123.1547 - val_loss: 239.1715 - val_mse: 239.1715\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 134.6032 - mse: 134.6032 - val_loss: 118.1594 - val_mse: 118.1594\n",
      "[CV] ...................................... nl=2, nn=24, total=  17.4s\n",
      "[CV] nl=2, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 315us/sample - loss: 399.8364 - mse: 399.8362 - val_loss: 502.7945 - val_mse: 502.7945\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 277.3554 - mse: 277.3555 - val_loss: 351.6210 - val_mse: 351.6211\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 206.4658 - mse: 206.4658 - val_loss: 207.5281 - val_mse: 207.5281\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 174.1054 - mse: 174.1054 - val_loss: 155.2268 - val_mse: 155.2268\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 172.8334 - mse: 172.8334 - val_loss: 148.8415 - val_mse: 148.8415\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 163.6873 - mse: 163.6874 - val_loss: 135.3948 - val_mse: 135.3948\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 126us/sample - loss: 155.8230 - mse: 155.8230 - val_loss: 139.5559 - val_mse: 139.5560\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 119us/sample - loss: 154.3675 - mse: 154.3675 - val_loss: 130.5416 - val_mse: 130.5415\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 124us/sample - loss: 151.4483 - mse: 151.4482 - val_loss: 129.7839 - val_mse: 129.7840\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 127us/sample - loss: 149.3737 - mse: 149.3737 - val_loss: 130.7814 - val_mse: 130.7814\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 145.6894 - mse: 145.6894 - val_loss: 124.9391 - val_mse: 124.9391\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 124us/sample - loss: 140.9668 - mse: 140.9668 - val_loss: 130.6503 - val_mse: 130.6504\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 143.8984 - mse: 143.8984 - val_loss: 120.8238 - val_mse: 120.8238\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 123us/sample - loss: 149.1649 - mse: 149.1648 - val_loss: 121.8789 - val_mse: 121.8789\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 140.4771 - mse: 140.4771 - val_loss: 138.2469 - val_mse: 138.2469\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 143.2239 - mse: 143.2239 - val_loss: 123.9606 - val_mse: 123.9606\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 144.0892 - mse: 144.0891 - val_loss: 123.0694 - val_mse: 123.0694\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 139.8655 - mse: 139.8655 - val_loss: 124.5629 - val_mse: 124.5629\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 122us/sample - loss: 139.5101 - mse: 139.5101 - val_loss: 118.4086 - val_mse: 118.4086\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 140.3810 - mse: 140.3810 - val_loss: 118.7729 - val_mse: 118.7729\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 117us/sample - loss: 136.7295 - mse: 136.7296 - val_loss: 121.1887 - val_mse: 121.1887\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 132us/sample - loss: 140.6175 - mse: 140.6175 - val_loss: 129.6971 - val_mse: 129.6971\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 125us/sample - loss: 137.7061 - mse: 137.7061 - val_loss: 124.7741 - val_mse: 124.7741\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 139.4838 - mse: 139.4838 - val_loss: 125.4411 - val_mse: 125.4411\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 135.2674 - mse: 135.2674 - val_loss: 127.2543 - val_mse: 127.2544\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 139.8142 - mse: 139.8142 - val_loss: 128.3701 - val_mse: 128.3701\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 138.0994 - mse: 138.0994 - val_loss: 129.3098 - val_mse: 129.3098\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 139.6572 - mse: 139.6572 - val_loss: 116.1613 - val_mse: 116.1613\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 132.3124 - mse: 132.3124 - val_loss: 117.2259 - val_mse: 117.2260\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 133.9081 - mse: 133.9081 - val_loss: 135.6641 - val_mse: 135.6641\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 129.1352 - mse: 129.1352 - val_loss: 112.8311 - val_mse: 112.8311\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 130.3953 - mse: 130.3953 - val_loss: 136.9048 - val_mse: 136.9048\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 131.9858 - mse: 131.9858 - val_loss: 133.4680 - val_mse: 133.4680\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 130.1443 - mse: 130.1443 - val_loss: 117.1466 - val_mse: 117.1466\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 121.3089 - mse: 121.3089 - val_loss: 122.2237 - val_mse: 122.2237\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 130.9818 - mse: 130.9818 - val_loss: 129.4116 - val_mse: 129.4116\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858/2858 [==============================] - 0s 106us/sample - loss: 133.4445 - mse: 133.4445 - val_loss: 118.0944 - val_mse: 118.0944\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 139.3488 - mse: 139.3488 - val_loss: 119.6576 - val_mse: 119.6576\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 127.6723 - mse: 127.6723 - val_loss: 117.5207 - val_mse: 117.5207\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 128.6568 - mse: 128.6569 - val_loss: 132.0846 - val_mse: 132.0846\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 130.7456 - mse: 130.7456 - val_loss: 125.3204 - val_mse: 125.3204\n",
      "[CV] ...................................... nl=2, nn=24, total=  14.2s\n",
      "[CV] nl=2, nn=24 .....................................................\n",
      "Train on 2858 samples, validate on 1226 samples\n",
      "Epoch 1/200\n",
      "2858/2858 [==============================] - 1s 308us/sample - loss: 406.7617 - mse: 406.7617 - val_loss: 587.5273 - val_mse: 587.5274\n",
      "Epoch 2/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 290.3060 - mse: 290.3061 - val_loss: 417.2116 - val_mse: 417.2117\n",
      "Epoch 3/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 212.7739 - mse: 212.7738 - val_loss: 279.0873 - val_mse: 279.0873\n",
      "Epoch 4/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 185.3608 - mse: 185.3608 - val_loss: 217.2433 - val_mse: 217.2433\n",
      "Epoch 5/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 168.3810 - mse: 168.3810 - val_loss: 206.6485 - val_mse: 206.6485\n",
      "Epoch 6/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 165.4920 - mse: 165.4919 - val_loss: 193.6911 - val_mse: 193.6911\n",
      "Epoch 7/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 165.6388 - mse: 165.6387 - val_loss: 195.0900 - val_mse: 195.0900\n",
      "Epoch 8/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 155.4181 - mse: 155.4181 - val_loss: 187.0532 - val_mse: 187.0531\n",
      "Epoch 9/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 151.7108 - mse: 151.7108 - val_loss: 175.8648 - val_mse: 175.8648\n",
      "Epoch 10/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 149.9769 - mse: 149.9769 - val_loss: 192.5902 - val_mse: 192.5901\n",
      "Epoch 11/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 151.4675 - mse: 151.4675 - val_loss: 174.1936 - val_mse: 174.1936\n",
      "Epoch 12/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 147.8365 - mse: 147.8366 - val_loss: 173.9760 - val_mse: 173.9760\n",
      "Epoch 13/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 149.0912 - mse: 149.0912 - val_loss: 168.1738 - val_mse: 168.1738\n",
      "Epoch 14/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 142.9254 - mse: 142.9254 - val_loss: 172.7386 - val_mse: 172.7386\n",
      "Epoch 15/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 144.5817 - mse: 144.5817 - val_loss: 165.7290 - val_mse: 165.7290\n",
      "Epoch 16/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 146.5074 - mse: 146.5075 - val_loss: 171.1361 - val_mse: 171.1361\n",
      "Epoch 17/200\n",
      "2858/2858 [==============================] - 0s 140us/sample - loss: 140.1134 - mse: 140.1135 - val_loss: 174.2845 - val_mse: 174.2845\n",
      "Epoch 18/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 140.2651 - mse: 140.2652 - val_loss: 182.4296 - val_mse: 182.4296\n",
      "Epoch 19/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 142.8426 - mse: 142.8425 - val_loss: 188.7907 - val_mse: 188.7907\n",
      "Epoch 20/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 142.6972 - mse: 142.6972 - val_loss: 165.2515 - val_mse: 165.2516\n",
      "Epoch 21/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 141.5021 - mse: 141.5021 - val_loss: 167.3974 - val_mse: 167.3974\n",
      "Epoch 22/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 139.2749 - mse: 139.2748 - val_loss: 167.2153 - val_mse: 167.2153\n",
      "Epoch 23/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 137.2952 - mse: 137.2951 - val_loss: 169.8206 - val_mse: 169.8205\n",
      "Epoch 24/200\n",
      "2858/2858 [==============================] - 0s 115us/sample - loss: 141.0448 - mse: 141.0448 - val_loss: 167.9975 - val_mse: 167.9974\n",
      "Epoch 25/200\n",
      "2858/2858 [==============================] - 0s 116us/sample - loss: 136.5813 - mse: 136.5813 - val_loss: 167.4012 - val_mse: 167.4012\n",
      "Epoch 26/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 135.6683 - mse: 135.6683 - val_loss: 161.6809 - val_mse: 161.6809\n",
      "Epoch 27/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 141.1662 - mse: 141.1661 - val_loss: 166.3576 - val_mse: 166.3576\n",
      "Epoch 28/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 144.1843 - mse: 144.1842 - val_loss: 166.6066 - val_mse: 166.6066\n",
      "Epoch 29/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 145.4840 - mse: 145.4840 - val_loss: 183.5863 - val_mse: 183.5863\n",
      "Epoch 30/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 139.4206 - mse: 139.4206 - val_loss: 166.0973 - val_mse: 166.0973\n",
      "Epoch 31/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 140.9403 - mse: 140.9403 - val_loss: 159.8974 - val_mse: 159.8974\n",
      "Epoch 32/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 136.8174 - mse: 136.8174 - val_loss: 169.0810 - val_mse: 169.0810\n",
      "Epoch 33/200\n",
      "2858/2858 [==============================] - 0s 113us/sample - loss: 138.1014 - mse: 138.1014 - val_loss: 158.2026 - val_mse: 158.2026\n",
      "Epoch 34/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 140.6727 - mse: 140.6727 - val_loss: 160.1982 - val_mse: 160.1982\n",
      "Epoch 35/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 135.7316 - mse: 135.7316 - val_loss: 161.9249 - val_mse: 161.9249\n",
      "Epoch 36/200\n",
      "2858/2858 [==============================] - 0s 123us/sample - loss: 135.9433 - mse: 135.9433 - val_loss: 175.2390 - val_mse: 175.2390\n",
      "Epoch 37/200\n",
      "2858/2858 [==============================] - 0s 131us/sample - loss: 135.2221 - mse: 135.2221 - val_loss: 172.9267 - val_mse: 172.9266\n",
      "Epoch 38/200\n",
      "2858/2858 [==============================] - 0s 132us/sample - loss: 136.3324 - mse: 136.3323 - val_loss: 161.5311 - val_mse: 161.5311\n",
      "Epoch 39/200\n",
      "2858/2858 [==============================] - 0s 127us/sample - loss: 134.5027 - mse: 134.5027 - val_loss: 156.6997 - val_mse: 156.6997\n",
      "Epoch 40/200\n",
      "2858/2858 [==============================] - 0s 121us/sample - loss: 129.3869 - mse: 129.3869 - val_loss: 154.7841 - val_mse: 154.7841\n",
      "Epoch 41/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 127.6100 - mse: 127.6100 - val_loss: 155.5113 - val_mse: 155.5113\n",
      "Epoch 42/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 131.0343 - mse: 131.0343 - val_loss: 172.7655 - val_mse: 172.7655\n",
      "Epoch 43/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 126.9414 - mse: 126.9415 - val_loss: 169.5154 - val_mse: 169.5154\n",
      "Epoch 44/200\n",
      "2858/2858 [==============================] - 0s 109us/sample - loss: 137.6200 - mse: 137.6200 - val_loss: 172.6096 - val_mse: 172.6096\n",
      "Epoch 45/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 135.9282 - mse: 135.9283 - val_loss: 190.5038 - val_mse: 190.5038\n",
      "Epoch 46/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 129.5811 - mse: 129.5811 - val_loss: 152.5490 - val_mse: 152.5489\n",
      "Epoch 47/200\n",
      "2858/2858 [==============================] - 0s 135us/sample - loss: 133.2802 - mse: 133.2802 - val_loss: 151.5190 - val_mse: 151.5190\n",
      "Epoch 48/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 127.7414 - mse: 127.7414 - val_loss: 157.5229 - val_mse: 157.5229\n",
      "Epoch 49/200\n",
      "2858/2858 [==============================] - 0s 107us/sample - loss: 125.2017 - mse: 125.2018 - val_loss: 151.5407 - val_mse: 151.5407\n",
      "Epoch 50/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 127.8000 - mse: 127.8000 - val_loss: 149.4255 - val_mse: 149.4255\n",
      "Epoch 51/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 122.3569 - mse: 122.3569 - val_loss: 162.7884 - val_mse: 162.7884\n",
      "Epoch 52/200\n",
      "2858/2858 [==============================] - 0s 129us/sample - loss: 129.2991 - mse: 129.2991 - val_loss: 150.5956 - val_mse: 150.5956\n",
      "Epoch 53/200\n",
      "2858/2858 [==============================] - 0s 114us/sample - loss: 125.1735 - mse: 125.1735 - val_loss: 155.1197 - val_mse: 155.1197\n",
      "Epoch 54/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 124.9655 - mse: 124.9655 - val_loss: 145.4459 - val_mse: 145.4459\n",
      "Epoch 55/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 127.1730 - mse: 127.1730 - val_loss: 165.7998 - val_mse: 165.7998\n",
      "Epoch 56/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 129.2917 - mse: 129.2918 - val_loss: 175.4613 - val_mse: 175.4613\n",
      "Epoch 57/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 127.5032 - mse: 127.5032 - val_loss: 169.8464 - val_mse: 169.8465\n",
      "Epoch 58/200\n",
      "2858/2858 [==============================] - 0s 111us/sample - loss: 125.1222 - mse: 125.1222 - val_loss: 148.7776 - val_mse: 148.7776\n",
      "Epoch 59/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 133.2513 - mse: 133.2513 - val_loss: 157.1108 - val_mse: 157.1108\n",
      "Epoch 60/200\n",
      "2858/2858 [==============================] - 0s 108us/sample - loss: 124.5886 - mse: 124.5886 - val_loss: 151.0722 - val_mse: 151.0722\n",
      "Epoch 61/200\n",
      "2858/2858 [==============================] - 0s 106us/sample - loss: 123.7123 - mse: 123.7123 - val_loss: 148.4560 - val_mse: 148.4560\n",
      "Epoch 62/200\n",
      "2858/2858 [==============================] - 0s 105us/sample - loss: 125.9237 - mse: 125.9237 - val_loss: 161.3173 - val_mse: 161.3174\n",
      "Epoch 63/200\n",
      "2858/2858 [==============================] - 0s 110us/sample - loss: 120.1436 - mse: 120.1436 - val_loss: 166.3453 - val_mse: 166.3453\n",
      "Epoch 64/200\n",
      "2858/2858 [==============================] - 0s 112us/sample - loss: 129.2653 - mse: 129.2653 - val_loss: 165.8406 - val_mse: 165.8407\n",
      "[CV] ...................................... nl=2, nn=24, total=  21.2s\n",
      "Train on 3573 samples, validate on 1532 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 51.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3573/3573 [==============================] - 1s 272us/sample - loss: 450.8528 - mse: 450.8527 - val_loss: 625.5251 - val_mse: 625.5251\n",
      "Epoch 2/200\n",
      "3573/3573 [==============================] - 0s 109us/sample - loss: 347.0096 - mse: 347.0096 - val_loss: 487.0838 - val_mse: 487.0838\n",
      "Epoch 3/200\n",
      "3573/3573 [==============================] - 0s 108us/sample - loss: 257.6057 - mse: 257.6057 - val_loss: 317.9705 - val_mse: 317.9705\n",
      "Epoch 4/200\n",
      "3573/3573 [==============================] - 0s 131us/sample - loss: 209.1744 - mse: 209.1745 - val_loss: 255.3499 - val_mse: 255.3499\n",
      "Epoch 5/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 188.4215 - mse: 188.4216 - val_loss: 239.5734 - val_mse: 239.5733\n",
      "Epoch 6/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 182.2243 - mse: 182.2242 - val_loss: 217.0201 - val_mse: 217.0201\n",
      "Epoch 7/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 171.6034 - mse: 171.6034 - val_loss: 205.7749 - val_mse: 205.7749\n",
      "Epoch 8/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 170.3274 - mse: 170.3274 - val_loss: 211.9610 - val_mse: 211.9610\n",
      "Epoch 9/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 165.4502 - mse: 165.4502 - val_loss: 197.6503 - val_mse: 197.6503\n",
      "Epoch 10/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 157.9970 - mse: 157.9970 - val_loss: 179.3707 - val_mse: 179.3707\n",
      "Epoch 11/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 156.3697 - mse: 156.3697 - val_loss: 170.8037 - val_mse: 170.8037\n",
      "Epoch 12/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 149.7550 - mse: 149.7550 - val_loss: 184.3943 - val_mse: 184.3943\n",
      "Epoch 13/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 163.8837 - mse: 163.8837 - val_loss: 170.0209 - val_mse: 170.0209\n",
      "Epoch 14/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 152.1394 - mse: 152.1394 - val_loss: 190.7675 - val_mse: 190.7675\n",
      "Epoch 15/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 149.5933 - mse: 149.5933 - val_loss: 166.4472 - val_mse: 166.4472\n",
      "Epoch 16/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 153.2513 - mse: 153.2513 - val_loss: 290.0360 - val_mse: 290.0360\n",
      "Epoch 17/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 149.9430 - mse: 149.9430 - val_loss: 191.8487 - val_mse: 191.8487\n",
      "Epoch 18/200\n",
      "3573/3573 [==============================] - 0s 113us/sample - loss: 146.4008 - mse: 146.4008 - val_loss: 203.7331 - val_mse: 203.7332\n",
      "Epoch 19/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 144.2915 - mse: 144.2915 - val_loss: 173.7264 - val_mse: 173.7263\n",
      "Epoch 20/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 142.8192 - mse: 142.8192 - val_loss: 178.1649 - val_mse: 178.1649\n",
      "Epoch 21/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 143.2426 - mse: 143.2426 - val_loss: 174.8258 - val_mse: 174.8258\n",
      "Epoch 22/200\n",
      "3573/3573 [==============================] - 0s 116us/sample - loss: 141.2605 - mse: 141.2606 - val_loss: 179.3262 - val_mse: 179.3262\n",
      "Epoch 23/200\n",
      "3573/3573 [==============================] - 0s 131us/sample - loss: 140.2348 - mse: 140.2348 - val_loss: 169.6329 - val_mse: 169.6329\n",
      "Epoch 24/200\n",
      "3573/3573 [==============================] - 0s 111us/sample - loss: 142.0027 - mse: 142.0026 - val_loss: 174.3182 - val_mse: 174.3182\n",
      "Epoch 25/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 142.7192 - mse: 142.7191 - val_loss: 164.7204 - val_mse: 164.7204\n",
      "Epoch 26/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 151.5472 - mse: 151.5471 - val_loss: 170.9187 - val_mse: 170.9187\n",
      "Epoch 27/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 139.4181 - mse: 139.4181 - val_loss: 166.7529 - val_mse: 166.7529\n",
      "Epoch 28/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 131.2467 - mse: 131.2467 - val_loss: 153.3333 - val_mse: 153.3333\n",
      "Epoch 29/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 132.6021 - mse: 132.6021 - val_loss: 145.8270 - val_mse: 145.8270\n",
      "Epoch 30/200\n",
      "3573/3573 [==============================] - 0s 100us/sample - loss: 134.5917 - mse: 134.5917 - val_loss: 152.2390 - val_mse: 152.2390\n",
      "Epoch 31/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 132.8859 - mse: 132.8859 - val_loss: 147.7049 - val_mse: 147.7048\n",
      "Epoch 32/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 125.9362 - mse: 125.9361 - val_loss: 224.9470 - val_mse: 224.9470\n",
      "Epoch 33/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 122.5575 - mse: 122.5575 - val_loss: 153.1015 - val_mse: 153.1015\n",
      "Epoch 34/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 132.9312 - mse: 132.9312 - val_loss: 154.9166 - val_mse: 154.9166\n",
      "Epoch 35/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 125.0513 - mse: 125.0513 - val_loss: 177.6217 - val_mse: 177.6217\n",
      "Epoch 36/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 124.1305 - mse: 124.1305 - val_loss: 194.2662 - val_mse: 194.2662\n",
      "Epoch 37/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 130.2888 - mse: 130.2888 - val_loss: 159.9461 - val_mse: 159.9461\n",
      "Epoch 38/200\n",
      "3573/3573 [==============================] - 0s 100us/sample - loss: 119.4601 - mse: 119.4601 - val_loss: 160.9776 - val_mse: 160.9777\n",
      "Epoch 39/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 113.6834 - mse: 113.6835 - val_loss: 140.7780 - val_mse: 140.7781\n",
      "Epoch 40/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 124.1865 - mse: 124.1864 - val_loss: 156.0363 - val_mse: 156.0363\n",
      "Epoch 41/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 115.7895 - mse: 115.7895 - val_loss: 159.5026 - val_mse: 159.5026\n",
      "Epoch 42/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 118.1368 - mse: 118.1369 - val_loss: 203.4352 - val_mse: 203.4352\n",
      "Epoch 43/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 115.1979 - mse: 115.1979 - val_loss: 147.4168 - val_mse: 147.4168\n",
      "Epoch 44/200\n",
      "3573/3573 [==============================] - 0s 120us/sample - loss: 112.9315 - mse: 112.9316 - val_loss: 168.3572 - val_mse: 168.3572\n",
      "Epoch 45/200\n",
      "3573/3573 [==============================] - 0s 103us/sample - loss: 116.6714 - mse: 116.6715 - val_loss: 170.3420 - val_mse: 170.3420\n",
      "Epoch 46/200\n",
      "3573/3573 [==============================] - 0s 102us/sample - loss: 112.5761 - mse: 112.5761 - val_loss: 261.6250 - val_mse: 261.6250\n",
      "Epoch 47/200\n",
      "3573/3573 [==============================] - 0s 104us/sample - loss: 104.4422 - mse: 104.4422 - val_loss: 154.6179 - val_mse: 154.6179\n",
      "Epoch 48/200\n",
      "3573/3573 [==============================] - 0s 140us/sample - loss: 110.3100 - mse: 110.3100 - val_loss: 162.6972 - val_mse: 162.6972\n",
      "Epoch 49/200\n",
      "3573/3573 [==============================] - 0s 101us/sample - loss: 111.4644 - mse: 111.4644 - val_loss: 227.4206 - val_mse: 227.4206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000002708C1F0088>,\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'nl': [0, 1, 2], 'nn': [2, 3, 6, 12, 24]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.fit(scaled_X_train, ny_train,validation_split=0.3 ,callbacks=[EarlyStopping(patience=10)])\n",
    "#Note that GridSearchCV uses K-fold cross-validation, so it does not use validation_split, These are just used for early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nl': 2, 'nn': 12}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the best hyperparameter values for ANN_model\n",
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ann=grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4084 samples, validate on 1021 samples\n",
      "Epoch 1/200\n",
      "4084/4084 - 2s - loss: 490.2738 - mse: 490.2738 - val_loss: 539.2373 - val_mse: 539.2372\n",
      "Epoch 2/200\n",
      "4084/4084 - 1s - loss: 381.3166 - mse: 381.3166 - val_loss: 383.9311 - val_mse: 383.9312\n",
      "Epoch 3/200\n",
      "4084/4084 - 0s - loss: 286.5208 - mse: 286.5207 - val_loss: 271.3666 - val_mse: 271.3666\n",
      "Epoch 4/200\n",
      "4084/4084 - 1s - loss: 238.2246 - mse: 238.2246 - val_loss: 218.5735 - val_mse: 218.5735\n",
      "Epoch 5/200\n",
      "4084/4084 - 1s - loss: 216.5760 - mse: 216.5760 - val_loss: 194.1380 - val_mse: 194.1380\n",
      "Epoch 6/200\n",
      "4084/4084 - 0s - loss: 204.2273 - mse: 204.2274 - val_loss: 155.9676 - val_mse: 155.9675\n",
      "Epoch 7/200\n",
      "4084/4084 - 0s - loss: 193.4069 - mse: 193.4069 - val_loss: 148.3370 - val_mse: 148.3370\n",
      "Epoch 8/200\n",
      "4084/4084 - 0s - loss: 192.7216 - mse: 192.7216 - val_loss: 141.1311 - val_mse: 141.1311\n",
      "Epoch 9/200\n",
      "4084/4084 - 0s - loss: 186.5586 - mse: 186.5587 - val_loss: 144.1300 - val_mse: 144.1300\n",
      "Epoch 10/200\n",
      "4084/4084 - 0s - loss: 180.9368 - mse: 180.9369 - val_loss: 133.3410 - val_mse: 133.3410\n",
      "Epoch 11/200\n",
      "4084/4084 - 0s - loss: 189.2312 - mse: 189.2311 - val_loss: 137.4259 - val_mse: 137.4259\n",
      "Epoch 12/200\n",
      "4084/4084 - 0s - loss: 178.0968 - mse: 178.0968 - val_loss: 134.8644 - val_mse: 134.8644\n",
      "Epoch 13/200\n",
      "4084/4084 - 0s - loss: 174.1579 - mse: 174.1579 - val_loss: 160.1439 - val_mse: 160.1439\n",
      "Epoch 14/200\n",
      "4084/4084 - 0s - loss: 174.0206 - mse: 174.0206 - val_loss: 137.1572 - val_mse: 137.1572\n",
      "Epoch 15/200\n",
      "4084/4084 - 0s - loss: 173.8055 - mse: 173.8055 - val_loss: 129.5332 - val_mse: 129.5332\n",
      "Epoch 16/200\n",
      "4084/4084 - 0s - loss: 173.5643 - mse: 173.5644 - val_loss: 128.3173 - val_mse: 128.3173\n",
      "Epoch 17/200\n",
      "4084/4084 - 0s - loss: 170.1521 - mse: 170.1521 - val_loss: 131.3246 - val_mse: 131.3245\n",
      "Epoch 18/200\n",
      "4084/4084 - 0s - loss: 170.6710 - mse: 170.6710 - val_loss: 135.5572 - val_mse: 135.5572\n",
      "Epoch 19/200\n",
      "4084/4084 - 0s - loss: 164.2308 - mse: 164.2309 - val_loss: 129.7072 - val_mse: 129.7072\n",
      "Epoch 20/200\n",
      "4084/4084 - 0s - loss: 172.1186 - mse: 172.1186 - val_loss: 156.2207 - val_mse: 156.2207\n",
      "Epoch 21/200\n",
      "4084/4084 - 0s - loss: 160.7106 - mse: 160.7106 - val_loss: 128.2382 - val_mse: 128.2383\n",
      "Epoch 22/200\n",
      "4084/4084 - 0s - loss: 160.7364 - mse: 160.7364 - val_loss: 129.9829 - val_mse: 129.9829\n",
      "Epoch 23/200\n",
      "4084/4084 - 0s - loss: 154.9544 - mse: 154.9543 - val_loss: 129.2855 - val_mse: 129.2855\n",
      "Epoch 24/200\n",
      "4084/4084 - 0s - loss: 158.1453 - mse: 158.1452 - val_loss: 123.8342 - val_mse: 123.8342\n",
      "Epoch 25/200\n",
      "4084/4084 - 0s - loss: 159.5829 - mse: 159.5830 - val_loss: 126.0085 - val_mse: 126.0085\n",
      "Epoch 26/200\n",
      "4084/4084 - 0s - loss: 157.7522 - mse: 157.7522 - val_loss: 148.0158 - val_mse: 148.0158\n",
      "Epoch 27/200\n",
      "4084/4084 - 0s - loss: 156.7287 - mse: 156.7288 - val_loss: 125.2791 - val_mse: 125.2791\n",
      "Epoch 28/200\n",
      "4084/4084 - 0s - loss: 156.3970 - mse: 156.3970 - val_loss: 148.3647 - val_mse: 148.3647\n",
      "Epoch 29/200\n",
      "4084/4084 - 0s - loss: 162.8013 - mse: 162.8013 - val_loss: 134.2475 - val_mse: 134.2475\n",
      "Epoch 30/200\n",
      "4084/4084 - 0s - loss: 156.1870 - mse: 156.1871 - val_loss: 122.6681 - val_mse: 122.6681\n",
      "Epoch 31/200\n",
      "4084/4084 - 1s - loss: 154.6633 - mse: 154.6634 - val_loss: 121.9161 - val_mse: 121.9161\n",
      "Epoch 32/200\n",
      "4084/4084 - 1s - loss: 155.1033 - mse: 155.1032 - val_loss: 146.6555 - val_mse: 146.6556\n",
      "Epoch 33/200\n",
      "4084/4084 - 0s - loss: 155.2731 - mse: 155.2731 - val_loss: 124.6646 - val_mse: 124.6646\n",
      "Epoch 34/200\n",
      "4084/4084 - 0s - loss: 159.2592 - mse: 159.2591 - val_loss: 125.2657 - val_mse: 125.2657\n",
      "Epoch 35/200\n",
      "4084/4084 - 0s - loss: 150.1832 - mse: 150.1832 - val_loss: 119.4479 - val_mse: 119.4479\n",
      "Epoch 36/200\n",
      "4084/4084 - 0s - loss: 154.1976 - mse: 154.1976 - val_loss: 126.8689 - val_mse: 126.8689\n",
      "Epoch 37/200\n",
      "4084/4084 - 0s - loss: 159.4903 - mse: 159.4903 - val_loss: 125.0906 - val_mse: 125.0906\n",
      "Epoch 38/200\n",
      "4084/4084 - 0s - loss: 150.4866 - mse: 150.4866 - val_loss: 127.6453 - val_mse: 127.6453\n",
      "Epoch 39/200\n",
      "4084/4084 - 0s - loss: 147.6359 - mse: 147.6359 - val_loss: 122.7448 - val_mse: 122.7448\n",
      "Epoch 40/200\n",
      "4084/4084 - 0s - loss: 145.9483 - mse: 145.9483 - val_loss: 155.1491 - val_mse: 155.1491\n",
      "Epoch 41/200\n",
      "4084/4084 - 0s - loss: 152.7840 - mse: 152.7840 - val_loss: 130.8056 - val_mse: 130.8056\n",
      "Epoch 42/200\n",
      "4084/4084 - 1s - loss: 151.4197 - mse: 151.4198 - val_loss: 116.3262 - val_mse: 116.3262\n",
      "Epoch 43/200\n",
      "4084/4084 - 0s - loss: 153.2055 - mse: 153.2055 - val_loss: 138.9935 - val_mse: 138.9935\n",
      "Epoch 44/200\n",
      "4084/4084 - 0s - loss: 146.9097 - mse: 146.9097 - val_loss: 151.2339 - val_mse: 151.2339\n",
      "Epoch 45/200\n",
      "4084/4084 - 0s - loss: 145.8878 - mse: 145.8878 - val_loss: 138.8795 - val_mse: 138.8795\n",
      "Epoch 46/200\n",
      "4084/4084 - 0s - loss: 146.2760 - mse: 146.2760 - val_loss: 133.1165 - val_mse: 133.1165\n",
      "Epoch 47/200\n",
      "4084/4084 - 0s - loss: 147.4985 - mse: 147.4984 - val_loss: 158.1972 - val_mse: 158.1973\n",
      "Epoch 48/200\n",
      "4084/4084 - 0s - loss: 158.3027 - mse: 158.3027 - val_loss: 157.8037 - val_mse: 157.8037\n",
      "Epoch 49/200\n",
      "4084/4084 - 0s - loss: 143.6192 - mse: 143.6191 - val_loss: 153.8690 - val_mse: 153.8690\n",
      "Epoch 50/200\n",
      "4084/4084 - 0s - loss: 150.1428 - mse: 150.1428 - val_loss: 112.9372 - val_mse: 112.9372\n",
      "Epoch 51/200\n",
      "4084/4084 - 0s - loss: 148.0414 - mse: 148.0414 - val_loss: 125.3810 - val_mse: 125.3810\n",
      "Epoch 52/200\n",
      "4084/4084 - 1s - loss: 142.5135 - mse: 142.5135 - val_loss: 112.1094 - val_mse: 112.1094\n",
      "Epoch 53/200\n",
      "4084/4084 - 0s - loss: 145.5658 - mse: 145.5659 - val_loss: 119.9523 - val_mse: 119.9523\n",
      "Epoch 54/200\n",
      "4084/4084 - 0s - loss: 149.5462 - mse: 149.5463 - val_loss: 123.0471 - val_mse: 123.0472\n",
      "Epoch 55/200\n",
      "4084/4084 - 0s - loss: 153.1772 - mse: 153.1772 - val_loss: 137.7138 - val_mse: 137.7138\n",
      "Epoch 56/200\n",
      "4084/4084 - 0s - loss: 146.6429 - mse: 146.6429 - val_loss: 133.8544 - val_mse: 133.8544\n",
      "Epoch 57/200\n",
      "4084/4084 - 0s - loss: 148.1770 - mse: 148.1770 - val_loss: 112.0597 - val_mse: 112.0596\n",
      "Epoch 58/200\n",
      "4084/4084 - 0s - loss: 151.4351 - mse: 151.4352 - val_loss: 141.8928 - val_mse: 141.8928\n",
      "Epoch 59/200\n",
      "4084/4084 - 0s - loss: 150.6413 - mse: 150.6412 - val_loss: 120.2248 - val_mse: 120.2248\n",
      "Epoch 60/200\n",
      "4084/4084 - 0s - loss: 151.7840 - mse: 151.7841 - val_loss: 118.1851 - val_mse: 118.1851\n",
      "Epoch 61/200\n",
      "4084/4084 - 0s - loss: 150.0082 - mse: 150.0082 - val_loss: 119.2169 - val_mse: 119.2170\n",
      "Epoch 62/200\n",
      "4084/4084 - 0s - loss: 149.3740 - mse: 149.3740 - val_loss: 110.0497 - val_mse: 110.0497\n",
      "Epoch 63/200\n",
      "4084/4084 - 0s - loss: 147.5454 - mse: 147.5454 - val_loss: 115.5443 - val_mse: 115.5443\n",
      "Epoch 64/200\n",
      "4084/4084 - 0s - loss: 153.2227 - mse: 153.2227 - val_loss: 120.9514 - val_mse: 120.9514\n",
      "Epoch 65/200\n",
      "4084/4084 - 1s - loss: 145.5096 - mse: 145.5096 - val_loss: 106.2459 - val_mse: 106.2459\n",
      "Epoch 66/200\n",
      "4084/4084 - 0s - loss: 141.3664 - mse: 141.3663 - val_loss: 125.6559 - val_mse: 125.6559\n",
      "Epoch 67/200\n",
      "4084/4084 - 0s - loss: 140.1935 - mse: 140.1934 - val_loss: 116.6122 - val_mse: 116.6122\n",
      "Epoch 68/200\n",
      "4084/4084 - 0s - loss: 137.4242 - mse: 137.4242 - val_loss: 210.0736 - val_mse: 210.0737\n",
      "Epoch 69/200\n",
      "4084/4084 - 0s - loss: 141.4515 - mse: 141.4515 - val_loss: 129.3126 - val_mse: 129.3126\n",
      "Epoch 70/200\n",
      "4084/4084 - 0s - loss: 140.7648 - mse: 140.7648 - val_loss: 122.3982 - val_mse: 122.3982\n",
      "Epoch 71/200\n",
      "4084/4084 - 0s - loss: 146.9982 - mse: 146.9982 - val_loss: 108.0016 - val_mse: 108.0016\n",
      "Epoch 72/200\n",
      "4084/4084 - 1s - loss: 139.5424 - mse: 139.5424 - val_loss: 113.3829 - val_mse: 113.3829\n",
      "Epoch 73/200\n",
      "4084/4084 - 1s - loss: 144.9749 - mse: 144.9749 - val_loss: 103.7177 - val_mse: 103.7177\n",
      "Epoch 74/200\n",
      "4084/4084 - 1s - loss: 138.8978 - mse: 138.8978 - val_loss: 209.0147 - val_mse: 209.0148\n",
      "Epoch 75/200\n",
      "4084/4084 - 0s - loss: 147.9089 - mse: 147.9090 - val_loss: 134.2750 - val_mse: 134.2750\n",
      "Epoch 76/200\n",
      "4084/4084 - 0s - loss: 139.7748 - mse: 139.7748 - val_loss: 125.7577 - val_mse: 125.7577\n",
      "Epoch 77/200\n",
      "4084/4084 - 1s - loss: 141.7667 - mse: 141.7667 - val_loss: 108.6377 - val_mse: 108.6377\n",
      "Epoch 78/200\n",
      "4084/4084 - 0s - loss: 142.7860 - mse: 142.7861 - val_loss: 120.8128 - val_mse: 120.8129\n",
      "Epoch 79/200\n",
      "4084/4084 - 0s - loss: 145.6912 - mse: 145.6912 - val_loss: 122.1537 - val_mse: 122.1537\n",
      "Epoch 80/200\n",
      "4084/4084 - 0s - loss: 146.4592 - mse: 146.4592 - val_loss: 106.5812 - val_mse: 106.5812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "4084/4084 - 0s - loss: 140.6628 - mse: 140.6628 - val_loss: 113.8739 - val_mse: 113.8739\n",
      "Epoch 82/200\n",
      "4084/4084 - 0s - loss: 138.6875 - mse: 138.6876 - val_loss: 177.3876 - val_mse: 177.3876\n",
      "Epoch 83/200\n",
      "4084/4084 - 1s - loss: 145.3362 - mse: 145.3363 - val_loss: 99.2501 - val_mse: 99.2501\n",
      "Epoch 84/200\n",
      "4084/4084 - 0s - loss: 137.8087 - mse: 137.8087 - val_loss: 105.8226 - val_mse: 105.8226\n",
      "Epoch 85/200\n",
      "4084/4084 - 0s - loss: 139.5275 - mse: 139.5275 - val_loss: 120.6445 - val_mse: 120.6444\n",
      "Epoch 86/200\n",
      "4084/4084 - 0s - loss: 128.1647 - mse: 128.1647 - val_loss: 115.9273 - val_mse: 115.9273\n",
      "Epoch 87/200\n",
      "4084/4084 - 0s - loss: 145.2582 - mse: 145.2582 - val_loss: 116.4363 - val_mse: 116.4363\n",
      "Epoch 88/200\n",
      "4084/4084 - 0s - loss: 136.8126 - mse: 136.8126 - val_loss: 109.6961 - val_mse: 109.6961\n",
      "Epoch 89/200\n",
      "4084/4084 - 0s - loss: 135.4750 - mse: 135.4750 - val_loss: 112.9879 - val_mse: 112.9878\n",
      "Epoch 90/200\n",
      "4084/4084 - 0s - loss: 142.5385 - mse: 142.5385 - val_loss: 189.5248 - val_mse: 189.5248\n",
      "Epoch 91/200\n",
      "4084/4084 - 0s - loss: 137.1603 - mse: 137.1603 - val_loss: 129.2718 - val_mse: 129.2718\n",
      "Epoch 92/200\n",
      "4084/4084 - 0s - loss: 138.6799 - mse: 138.6799 - val_loss: 106.4105 - val_mse: 106.4105\n",
      "Epoch 93/200\n",
      "4084/4084 - 1s - loss: 134.3878 - mse: 134.3878 - val_loss: 96.9184 - val_mse: 96.9184\n",
      "Epoch 94/200\n",
      "4084/4084 - 1s - loss: 136.9783 - mse: 136.9782 - val_loss: 94.2760 - val_mse: 94.2760\n",
      "Epoch 95/200\n",
      "4084/4084 - 0s - loss: 135.5731 - mse: 135.5731 - val_loss: 170.2983 - val_mse: 170.2983\n",
      "Epoch 96/200\n",
      "4084/4084 - 0s - loss: 143.7944 - mse: 143.7944 - val_loss: 123.4217 - val_mse: 123.4217\n",
      "Epoch 97/200\n",
      "4084/4084 - 0s - loss: 138.8460 - mse: 138.8460 - val_loss: 135.8339 - val_mse: 135.8339\n",
      "Epoch 98/200\n",
      "4084/4084 - 0s - loss: 128.5981 - mse: 128.5982 - val_loss: 95.1865 - val_mse: 95.1865\n",
      "Epoch 99/200\n",
      "4084/4084 - 1s - loss: 141.9864 - mse: 141.9863 - val_loss: 121.7370 - val_mse: 121.7370\n",
      "Epoch 100/200\n",
      "4084/4084 - 1s - loss: 134.4776 - mse: 134.4777 - val_loss: 102.4478 - val_mse: 102.4478\n",
      "Epoch 101/200\n",
      "4084/4084 - 0s - loss: 133.8333 - mse: 133.8333 - val_loss: 133.7220 - val_mse: 133.7220\n",
      "Epoch 102/200\n",
      "4084/4084 - 0s - loss: 130.4144 - mse: 130.4144 - val_loss: 99.7128 - val_mse: 99.7127\n",
      "Epoch 103/200\n",
      "4084/4084 - 0s - loss: 140.0464 - mse: 140.0464 - val_loss: 145.8430 - val_mse: 145.8430\n",
      "Epoch 104/200\n",
      "4084/4084 - 1s - loss: 147.1988 - mse: 147.1988 - val_loss: 136.0264 - val_mse: 136.0264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x270a43e7548>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try to fit the model by using callbacks , monitor the validation loss and save only the best model\n",
    "best_ann.fit(scaled_X_train, ny_train,validation_split=0.2,verbose=2,callbacks=[EarlyStopping(patience=10),ModelCheckpoint('best_model.hdf5',save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model from checkpoint\n",
    "best_ann=load_model('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_245 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 343\n",
      "Trainable params: 319\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#the best ANN architechture\n",
    "best_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=best_ann.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolut error 3.981443954264162\n",
      "R2 score 0.7342305400432674\n",
      "root mean squared error 10.733163812094562\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model metrics\n",
    "print('mean absolut error', mean_absolute_error(ny_test,predictions)) \n",
    "print('R2 score',r2_score(ny_test,predictions)) \n",
    "print('root mean squared error',np.sqrt(mean_squared_error(ny_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title('Mean absolute error is {} and R2 score is {}'.format(mean_absolute_error(y_test,predictions),r2_score(y_test,predictions)))\n",
    "plt.plot(np.arange(0,300,1),np.arange(0,300,1),color='green', linestyle='dashed',label='Predicted R.O.P = True R.O.P')\n",
    "sns.scatterplot(x=ny_test,y=predictions[:,0])\n",
    "plt.xlabel('True R.O.P')\n",
    "plt.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.577316 ,  6.4878964,  6.9583817, ..., 38.020683 , 25.305702 ,\n",
       "        4.301601 ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Examining the best model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=Sequential()\n",
    "final_model.add(Dense(6,input_dim=6,activation=\"relu\"))\n",
    "final_model.add(Dense(12,activation=\"relu\"))\n",
    "final_model.add(Dense(12,activation=\"relu\"))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=\"adam\", loss=\"mse\", metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4084 samples, validate on 1021 samples\n",
      "Epoch 1/200\n",
      "4084/4084 - 1s - loss: 451.1289 - mae: 11.4200 - val_loss: 518.6111 - val_mae: 11.5853\n",
      "Epoch 2/200\n",
      "4084/4084 - 0s - loss: 311.3053 - mae: 8.6796 - val_loss: 295.1972 - val_mae: 7.7628\n",
      "Epoch 3/200\n",
      "4084/4084 - 0s - loss: 240.0944 - mae: 6.3322 - val_loss: 192.9825 - val_mae: 4.9039\n",
      "Epoch 4/200\n",
      "4084/4084 - 0s - loss: 211.4279 - mae: 5.3326 - val_loss: 177.7657 - val_mae: 5.1478\n",
      "Epoch 5/200\n",
      "4084/4084 - 0s - loss: 187.5106 - mae: 5.2680 - val_loss: 161.1128 - val_mae: 4.9752\n",
      "Epoch 6/200\n",
      "4084/4084 - 0s - loss: 184.0275 - mae: 5.5751 - val_loss: 166.6875 - val_mae: 5.1826\n",
      "Epoch 7/200\n",
      "4084/4084 - 0s - loss: 181.6865 - mae: 5.4870 - val_loss: 143.1336 - val_mae: 4.6184\n",
      "Epoch 8/200\n",
      "4084/4084 - 0s - loss: 177.6832 - mae: 5.6224 - val_loss: 136.5755 - val_mae: 5.0947\n",
      "Epoch 9/200\n",
      "4084/4084 - 1s - loss: 178.2930 - mae: 5.5083 - val_loss: 128.3143 - val_mae: 4.4879\n",
      "Epoch 10/200\n",
      "4084/4084 - 0s - loss: 173.5647 - mae: 5.7036 - val_loss: 142.7359 - val_mae: 4.6893\n",
      "Epoch 11/200\n",
      "4084/4084 - 0s - loss: 173.0757 - mae: 5.5648 - val_loss: 144.3218 - val_mae: 5.0804\n",
      "Epoch 12/200\n",
      "4084/4084 - 0s - loss: 166.5820 - mae: 5.4498 - val_loss: 141.3035 - val_mae: 4.7841\n",
      "Epoch 13/200\n",
      "4084/4084 - 1s - loss: 166.6576 - mae: 5.5497 - val_loss: 128.9417 - val_mae: 5.3260\n",
      "Epoch 14/200\n",
      "4084/4084 - 0s - loss: 158.5173 - mae: 5.4898 - val_loss: 142.6171 - val_mae: 5.7460\n",
      "Epoch 15/200\n",
      "4084/4084 - 0s - loss: 155.4864 - mae: 5.5760 - val_loss: 123.9403 - val_mae: 5.1690\n",
      "Epoch 16/200\n",
      "4084/4084 - 0s - loss: 152.5569 - mae: 5.5917 - val_loss: 125.1046 - val_mae: 5.7223\n",
      "Epoch 17/200\n",
      "4084/4084 - 0s - loss: 153.7935 - mae: 5.5227 - val_loss: 125.3631 - val_mae: 5.0780\n",
      "Epoch 18/200\n",
      "4084/4084 - 0s - loss: 153.4666 - mae: 5.5017 - val_loss: 118.3468 - val_mae: 4.3743\n",
      "Epoch 19/200\n",
      "4084/4084 - 0s - loss: 150.5828 - mae: 5.3426 - val_loss: 113.5739 - val_mae: 4.4088\n",
      "Epoch 20/200\n",
      "4084/4084 - 0s - loss: 144.8635 - mae: 5.4532 - val_loss: 104.7231 - val_mae: 4.1238\n",
      "Epoch 21/200\n",
      "4084/4084 - 0s - loss: 142.3841 - mae: 5.3388 - val_loss: 113.5991 - val_mae: 4.3327\n",
      "Epoch 22/200\n",
      "4084/4084 - 1s - loss: 146.9730 - mae: 5.4838 - val_loss: 136.6258 - val_mae: 5.0602\n",
      "Epoch 23/200\n",
      "4084/4084 - 0s - loss: 147.6074 - mae: 5.5079 - val_loss: 118.9477 - val_mae: 5.5104\n",
      "Epoch 24/200\n",
      "4084/4084 - 0s - loss: 142.1776 - mae: 5.4605 - val_loss: 115.9393 - val_mae: 4.7382\n",
      "Epoch 25/200\n",
      "4084/4084 - 0s - loss: 137.4015 - mae: 5.4642 - val_loss: 115.4088 - val_mae: 4.2385\n",
      "Epoch 26/200\n",
      "4084/4084 - 0s - loss: 136.2053 - mae: 5.4091 - val_loss: 95.9741 - val_mae: 4.0391\n",
      "Epoch 27/200\n",
      "4084/4084 - 0s - loss: 138.0967 - mae: 5.5327 - val_loss: 97.3144 - val_mae: 4.2229\n",
      "Epoch 28/200\n",
      "4084/4084 - 0s - loss: 142.6520 - mae: 5.5339 - val_loss: 95.6601 - val_mae: 3.9848\n",
      "Epoch 29/200\n",
      "4084/4084 - 0s - loss: 126.6349 - mae: 5.3644 - val_loss: 93.8681 - val_mae: 3.9305\n",
      "Epoch 30/200\n",
      "4084/4084 - 0s - loss: 137.7600 - mae: 5.4977 - val_loss: 115.0095 - val_mae: 4.6164\n",
      "Epoch 31/200\n",
      "4084/4084 - 0s - loss: 132.3616 - mae: 5.3661 - val_loss: 101.2602 - val_mae: 4.0853\n",
      "Epoch 32/200\n",
      "4084/4084 - 0s - loss: 128.2292 - mae: 5.3904 - val_loss: 106.5685 - val_mae: 4.6757\n",
      "Epoch 33/200\n",
      "4084/4084 - 0s - loss: 129.5972 - mae: 5.3741 - val_loss: 104.2261 - val_mae: 4.4855\n",
      "Epoch 34/200\n",
      "4084/4084 - 0s - loss: 129.5068 - mae: 5.4645 - val_loss: 119.6239 - val_mae: 4.5231\n",
      "Epoch 35/200\n",
      "4084/4084 - 0s - loss: 122.1805 - mae: 5.1792 - val_loss: 92.7046 - val_mae: 3.8919\n",
      "Epoch 36/200\n",
      "4084/4084 - 0s - loss: 122.1484 - mae: 5.2280 - val_loss: 106.0406 - val_mae: 4.1139\n",
      "Epoch 37/200\n",
      "4084/4084 - 0s - loss: 128.5174 - mae: 5.3846 - val_loss: 98.4162 - val_mae: 3.9013\n",
      "Epoch 38/200\n",
      "4084/4084 - 0s - loss: 125.9855 - mae: 5.4408 - val_loss: 124.8525 - val_mae: 4.9646\n",
      "Epoch 39/200\n",
      "4084/4084 - 0s - loss: 129.5971 - mae: 5.5742 - val_loss: 100.7843 - val_mae: 3.8929\n",
      "Epoch 40/200\n",
      "4084/4084 - 0s - loss: 121.3397 - mae: 5.2218 - val_loss: 132.4606 - val_mae: 4.6173\n",
      "Epoch 41/200\n",
      "4084/4084 - 0s - loss: 118.7031 - mae: 5.3716 - val_loss: 105.4443 - val_mae: 4.0518\n",
      "Epoch 42/200\n",
      "4084/4084 - 0s - loss: 117.8790 - mae: 5.1912 - val_loss: 98.3415 - val_mae: 4.3972\n",
      "Epoch 43/200\n",
      "4084/4084 - 0s - loss: 120.1564 - mae: 5.3027 - val_loss: 110.6184 - val_mae: 4.0417\n",
      "Epoch 44/200\n",
      "4084/4084 - 0s - loss: 115.8559 - mae: 5.3258 - val_loss: 114.8487 - val_mae: 4.0820\n",
      "Epoch 45/200\n",
      "4084/4084 - 0s - loss: 126.2417 - mae: 5.3187 - val_loss: 107.1769 - val_mae: 3.9610\n",
      "Epoch 46/200\n",
      "4084/4084 - 0s - loss: 115.4094 - mae: 5.4890 - val_loss: 107.2860 - val_mae: 4.0586\n",
      "Epoch 47/200\n",
      "4084/4084 - 0s - loss: 119.4305 - mae: 5.4486 - val_loss: 111.6500 - val_mae: 4.0258\n",
      "Epoch 48/200\n",
      "4084/4084 - 0s - loss: 120.5328 - mae: 5.5811 - val_loss: 120.3816 - val_mae: 4.9763\n",
      "Epoch 49/200\n",
      "4084/4084 - 0s - loss: 117.6030 - mae: 5.3530 - val_loss: 126.2711 - val_mae: 4.0716\n",
      "Epoch 50/200\n",
      "4084/4084 - 0s - loss: 107.5244 - mae: 5.2686 - val_loss: 115.8818 - val_mae: 4.1277\n",
      "Epoch 51/200\n",
      "4084/4084 - 0s - loss: 107.6850 - mae: 5.3175 - val_loss: 139.7100 - val_mae: 4.2647\n",
      "Epoch 52/200\n",
      "4084/4084 - 0s - loss: 116.3154 - mae: 5.3528 - val_loss: 157.0026 - val_mae: 4.6396\n",
      "Epoch 53/200\n",
      "4084/4084 - 0s - loss: 109.7881 - mae: 5.4627 - val_loss: 118.8595 - val_mae: 4.8935\n",
      "Epoch 54/200\n",
      "4084/4084 - 0s - loss: 112.2418 - mae: 5.4581 - val_loss: 144.5135 - val_mae: 4.3081\n",
      "Epoch 55/200\n",
      "4084/4084 - 0s - loss: 106.8179 - mae: 5.2975 - val_loss: 148.3265 - val_mae: 4.7946\n",
      "Epoch 56/200\n",
      "4084/4084 - 0s - loss: 108.2204 - mae: 5.5193 - val_loss: 129.8822 - val_mae: 4.6812\n",
      "Epoch 57/200\n",
      "4084/4084 - 0s - loss: 111.7653 - mae: 5.4765 - val_loss: 128.2800 - val_mae: 4.0559\n",
      "Epoch 58/200\n",
      "4084/4084 - 0s - loss: 109.0607 - mae: 5.3831 - val_loss: 124.6231 - val_mae: 3.9615\n",
      "Epoch 59/200\n",
      "4084/4084 - 0s - loss: 109.5515 - mae: 5.3314 - val_loss: 152.6065 - val_mae: 5.1890\n",
      "Epoch 60/200\n",
      "4084/4084 - 0s - loss: 111.8285 - mae: 5.3398 - val_loss: 108.4790 - val_mae: 4.3398\n",
      "Epoch 61/200\n",
      "4084/4084 - 0s - loss: 110.4795 - mae: 5.4933 - val_loss: 150.2113 - val_mae: 5.2667\n",
      "Epoch 62/200\n",
      "4084/4084 - 0s - loss: 104.5157 - mae: 5.3455 - val_loss: 162.8320 - val_mae: 5.7481\n",
      "Epoch 63/200\n",
      "4084/4084 - 0s - loss: 96.9470 - mae: 5.2820 - val_loss: 144.9198 - val_mae: 4.1458\n",
      "Epoch 64/200\n",
      "4084/4084 - 0s - loss: 98.1377 - mae: 5.3756 - val_loss: 142.9318 - val_mae: 4.1419\n",
      "Epoch 65/200\n",
      "4084/4084 - 0s - loss: 95.5744 - mae: 5.1165 - val_loss: 116.1029 - val_mae: 4.5684\n",
      "Epoch 66/200\n",
      "4084/4084 - 0s - loss: 107.2833 - mae: 5.3305 - val_loss: 134.2946 - val_mae: 4.5832\n",
      "Epoch 67/200\n",
      "4084/4084 - 0s - loss: 104.3814 - mae: 5.3076 - val_loss: 168.9913 - val_mae: 4.3836\n",
      "Epoch 68/200\n",
      "4084/4084 - 0s - loss: 101.2089 - mae: 5.2856 - val_loss: 138.1098 - val_mae: 4.2480\n",
      "Epoch 69/200\n",
      "4084/4084 - 0s - loss: 98.9503 - mae: 5.2731 - val_loss: 171.3975 - val_mae: 5.8568\n",
      "Epoch 70/200\n",
      "4084/4084 - 0s - loss: 106.1627 - mae: 5.3249 - val_loss: 136.4979 - val_mae: 4.4618\n",
      "Epoch 71/200\n",
      "4084/4084 - 0s - loss: 97.5178 - mae: 5.2942 - val_loss: 116.9798 - val_mae: 4.1807\n",
      "Epoch 72/200\n",
      "4084/4084 - 0s - loss: 103.9608 - mae: 5.2227 - val_loss: 132.3425 - val_mae: 3.9684\n",
      "Epoch 73/200\n",
      "4084/4084 - 0s - loss: 100.9024 - mae: 5.3738 - val_loss: 235.7652 - val_mae: 4.9411\n",
      "Epoch 74/200\n",
      "4084/4084 - 0s - loss: 103.8661 - mae: 5.3396 - val_loss: 143.7825 - val_mae: 5.1945\n",
      "Epoch 75/200\n",
      "4084/4084 - 0s - loss: 96.5891 - mae: 5.2235 - val_loss: 371.9630 - val_mae: 5.2544\n",
      "Epoch 76/200\n",
      "4084/4084 - 0s - loss: 100.7698 - mae: 5.3448 - val_loss: 127.9274 - val_mae: 4.2785\n",
      "Epoch 77/200\n",
      "4084/4084 - 0s - loss: 107.6056 - mae: 5.2766 - val_loss: 144.4776 - val_mae: 4.1986\n",
      "Epoch 78/200\n",
      "4084/4084 - 0s - loss: 100.1574 - mae: 5.3147 - val_loss: 179.5218 - val_mae: 4.5646\n",
      "Epoch 79/200\n",
      "4084/4084 - 0s - loss: 95.0960 - mae: 5.1592 - val_loss: 205.3514 - val_mae: 4.7453\n",
      "Epoch 80/200\n",
      "4084/4084 - 0s - loss: 100.7469 - mae: 5.3824 - val_loss: 215.6240 - val_mae: 6.2580\n",
      "Epoch 81/200\n",
      "4084/4084 - 0s - loss: 96.7317 - mae: 5.2629 - val_loss: 110.0394 - val_mae: 4.9251\n",
      "Epoch 82/200\n",
      "4084/4084 - 0s - loss: 112.7645 - mae: 5.3211 - val_loss: 135.2623 - val_mae: 4.2317\n",
      "Epoch 83/200\n",
      "4084/4084 - 0s - loss: 100.1955 - mae: 5.3352 - val_loss: 225.3396 - val_mae: 6.4621\n",
      "Epoch 84/200\n",
      "4084/4084 - 0s - loss: 100.4448 - mae: 5.4644 - val_loss: 292.1478 - val_mae: 4.8740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200\n",
      "4084/4084 - 0s - loss: 97.0814 - mae: 5.2939 - val_loss: 191.9403 - val_mae: 4.3057\n",
      "Epoch 86/200\n",
      "4084/4084 - 0s - loss: 95.7756 - mae: 5.1322 - val_loss: 191.5803 - val_mae: 4.5718\n",
      "Epoch 87/200\n",
      "4084/4084 - 0s - loss: 98.9283 - mae: 5.2903 - val_loss: 274.3554 - val_mae: 7.1163\n",
      "Epoch 88/200\n",
      "4084/4084 - 0s - loss: 92.7696 - mae: 5.0912 - val_loss: 122.4096 - val_mae: 4.3716\n",
      "Epoch 89/200\n",
      "4084/4084 - 0s - loss: 95.2671 - mae: 5.2144 - val_loss: 174.9343 - val_mae: 4.0981\n",
      "Epoch 90/200\n",
      "4084/4084 - 0s - loss: 90.7487 - mae: 5.1764 - val_loss: 152.3530 - val_mae: 4.1572\n",
      "Epoch 91/200\n",
      "4084/4084 - 0s - loss: 100.6752 - mae: 5.2938 - val_loss: 290.6374 - val_mae: 4.8061\n",
      "Epoch 92/200\n",
      "4084/4084 - 0s - loss: 97.9602 - mae: 5.3783 - val_loss: 206.9630 - val_mae: 4.3458\n",
      "Epoch 93/200\n",
      "4084/4084 - 0s - loss: 94.2011 - mae: 5.1327 - val_loss: 173.3738 - val_mae: 4.1838\n",
      "Epoch 94/200\n",
      "4084/4084 - 0s - loss: 96.2597 - mae: 5.2499 - val_loss: 169.0250 - val_mae: 6.2022\n",
      "Epoch 95/200\n",
      "4084/4084 - 0s - loss: 106.4098 - mae: 5.3958 - val_loss: 168.6035 - val_mae: 4.2816\n",
      "Epoch 96/200\n",
      "4084/4084 - 0s - loss: 94.7789 - mae: 5.3201 - val_loss: 119.4991 - val_mae: 5.1406\n",
      "Epoch 97/200\n",
      "4084/4084 - 0s - loss: 87.9608 - mae: 5.0628 - val_loss: 208.5628 - val_mae: 5.5843\n",
      "Epoch 98/200\n",
      "4084/4084 - 0s - loss: 87.4907 - mae: 5.1104 - val_loss: 185.2468 - val_mae: 5.6533\n",
      "Epoch 99/200\n",
      "4084/4084 - 0s - loss: 97.9268 - mae: 5.4450 - val_loss: 123.7288 - val_mae: 4.9886\n",
      "Epoch 100/200\n",
      "4084/4084 - 0s - loss: 109.5170 - mae: 5.2395 - val_loss: 239.2006 - val_mae: 4.6686\n",
      "Epoch 101/200\n",
      "4084/4084 - 0s - loss: 98.0017 - mae: 5.1574 - val_loss: 138.9331 - val_mae: 4.3150\n",
      "Epoch 102/200\n",
      "4084/4084 - 0s - loss: 96.3403 - mae: 5.1782 - val_loss: 199.0585 - val_mae: 6.2830\n",
      "Epoch 103/200\n",
      "4084/4084 - 0s - loss: 95.1185 - mae: 5.2847 - val_loss: 144.8458 - val_mae: 3.8576\n",
      "Epoch 104/200\n",
      "4084/4084 - 0s - loss: 94.0717 - mae: 5.2163 - val_loss: 196.2110 - val_mae: 4.7203\n",
      "Epoch 105/200\n",
      "4084/4084 - 1s - loss: 103.5319 - mae: 5.3207 - val_loss: 332.8243 - val_mae: 5.6243\n",
      "Epoch 106/200\n",
      "4084/4084 - 0s - loss: 92.8851 - mae: 5.0320 - val_loss: 161.0387 - val_mae: 4.0777\n",
      "Epoch 107/200\n",
      "4084/4084 - 0s - loss: 98.5086 - mae: 5.3014 - val_loss: 244.5630 - val_mae: 4.5157\n",
      "Epoch 108/200\n",
      "4084/4084 - 0s - loss: 113.8132 - mae: 5.3850 - val_loss: 176.0442 - val_mae: 4.5536\n",
      "Epoch 109/200\n",
      "4084/4084 - 0s - loss: 102.1999 - mae: 5.5122 - val_loss: 315.1984 - val_mae: 6.1387\n",
      "Epoch 110/200\n",
      "4084/4084 - 0s - loss: 92.7060 - mae: 5.2285 - val_loss: 257.1216 - val_mae: 5.2167\n",
      "Epoch 111/200\n",
      "4084/4084 - 0s - loss: 98.0280 - mae: 5.3724 - val_loss: 206.4626 - val_mae: 4.3261\n",
      "Epoch 112/200\n",
      "4084/4084 - 0s - loss: 92.4337 - mae: 5.2116 - val_loss: 192.6891 - val_mae: 5.5520\n",
      "Epoch 113/200\n",
      "4084/4084 - 0s - loss: 98.5569 - mae: 5.3876 - val_loss: 161.2562 - val_mae: 4.4805\n",
      "Epoch 114/200\n",
      "4084/4084 - 0s - loss: 96.4064 - mae: 5.3352 - val_loss: 262.7388 - val_mae: 5.0372\n",
      "Epoch 115/200\n",
      "4084/4084 - 0s - loss: 96.5554 - mae: 5.1314 - val_loss: 122.3493 - val_mae: 3.8402\n",
      "Epoch 116/200\n",
      "4084/4084 - 0s - loss: 101.1023 - mae: 5.2847 - val_loss: 100.2391 - val_mae: 3.9114\n",
      "Epoch 117/200\n",
      "4084/4084 - 0s - loss: 92.9014 - mae: 5.1040 - val_loss: 175.2413 - val_mae: 4.0929\n",
      "Epoch 118/200\n",
      "4084/4084 - 0s - loss: 94.3553 - mae: 5.2203 - val_loss: 127.0943 - val_mae: 4.2739\n",
      "Epoch 119/200\n",
      "4084/4084 - 0s - loss: 97.9018 - mae: 5.2442 - val_loss: 226.4416 - val_mae: 4.9558\n",
      "Epoch 120/200\n",
      "4084/4084 - 0s - loss: 92.5352 - mae: 5.2990 - val_loss: 167.3396 - val_mae: 4.5460\n",
      "Epoch 121/200\n",
      "4084/4084 - 0s - loss: 91.1251 - mae: 5.0896 - val_loss: 208.6072 - val_mae: 4.5064\n",
      "Epoch 122/200\n",
      "4084/4084 - 0s - loss: 97.8554 - mae: 5.2977 - val_loss: 253.5308 - val_mae: 8.3712\n",
      "Epoch 123/200\n",
      "4084/4084 - 0s - loss: 98.8930 - mae: 5.4129 - val_loss: 190.1895 - val_mae: 5.0892\n",
      "Epoch 124/200\n",
      "4084/4084 - 0s - loss: 99.6113 - mae: 5.4665 - val_loss: 164.0306 - val_mae: 5.3796\n",
      "Epoch 125/200\n",
      "4084/4084 - 0s - loss: 96.4457 - mae: 5.3609 - val_loss: 205.1058 - val_mae: 7.2902\n",
      "Epoch 126/200\n",
      "4084/4084 - 0s - loss: 88.7426 - mae: 5.0700 - val_loss: 197.2773 - val_mae: 4.6933\n",
      "Epoch 127/200\n",
      "4084/4084 - 0s - loss: 93.4736 - mae: 5.4536 - val_loss: 208.3508 - val_mae: 5.6972\n",
      "Epoch 128/200\n",
      "4084/4084 - 0s - loss: 93.1452 - mae: 5.1912 - val_loss: 204.2464 - val_mae: 4.2365\n",
      "Epoch 129/200\n",
      "4084/4084 - 0s - loss: 95.3849 - mae: 5.1797 - val_loss: 217.6570 - val_mae: 4.9773\n",
      "Epoch 130/200\n",
      "4084/4084 - 0s - loss: 99.0862 - mae: 5.4429 - val_loss: 259.0259 - val_mae: 5.5921\n",
      "Epoch 131/200\n",
      "4084/4084 - 0s - loss: 123.1144 - mae: 5.3294 - val_loss: 266.9413 - val_mae: 5.3536\n",
      "Epoch 132/200\n",
      "4084/4084 - 0s - loss: 99.8492 - mae: 5.3495 - val_loss: 131.5247 - val_mae: 4.3876\n",
      "Epoch 133/200\n",
      "4084/4084 - 0s - loss: 102.1144 - mae: 5.1988 - val_loss: 143.3584 - val_mae: 5.0066\n",
      "Epoch 134/200\n",
      "4084/4084 - 0s - loss: 91.7586 - mae: 5.2408 - val_loss: 186.2812 - val_mae: 4.2189\n",
      "Epoch 135/200\n",
      "4084/4084 - 0s - loss: 90.9267 - mae: 5.1296 - val_loss: 258.2563 - val_mae: 4.6873\n",
      "Epoch 136/200\n",
      "4084/4084 - 0s - loss: 107.8109 - mae: 5.2858 - val_loss: 381.0236 - val_mae: 6.5466\n",
      "Epoch 137/200\n",
      "4084/4084 - 0s - loss: 96.8688 - mae: 5.3027 - val_loss: 142.0188 - val_mae: 3.9011\n",
      "Epoch 138/200\n",
      "4084/4084 - 0s - loss: 100.0184 - mae: 5.3329 - val_loss: 171.4085 - val_mae: 5.0548\n",
      "Epoch 139/200\n",
      "4084/4084 - 0s - loss: 100.4466 - mae: 5.2120 - val_loss: 172.6361 - val_mae: 6.6820\n",
      "Epoch 140/200\n",
      "4084/4084 - 0s - loss: 96.5019 - mae: 5.3220 - val_loss: 133.5576 - val_mae: 5.0930\n",
      "Epoch 141/200\n",
      "4084/4084 - 0s - loss: 88.1690 - mae: 5.2107 - val_loss: 155.4793 - val_mae: 4.1931\n",
      "Epoch 142/200\n",
      "4084/4084 - 0s - loss: 99.4481 - mae: 5.2188 - val_loss: 200.6309 - val_mae: 4.2085\n",
      "Epoch 143/200\n",
      "4084/4084 - 0s - loss: 101.9710 - mae: 5.3228 - val_loss: 292.2400 - val_mae: 4.7373\n",
      "Epoch 144/200\n",
      "4084/4084 - 0s - loss: 95.3924 - mae: 5.2740 - val_loss: 186.0520 - val_mae: 4.1062\n",
      "Epoch 145/200\n",
      "4084/4084 - 0s - loss: 105.1358 - mae: 5.2410 - val_loss: 293.2866 - val_mae: 5.0868\n",
      "Epoch 146/200\n",
      "4084/4084 - 0s - loss: 91.7385 - mae: 5.2407 - val_loss: 350.4748 - val_mae: 5.0594\n",
      "Epoch 147/200\n",
      "4084/4084 - 0s - loss: 94.9329 - mae: 5.2221 - val_loss: 227.3281 - val_mae: 4.1699\n",
      "Epoch 148/200\n",
      "4084/4084 - 0s - loss: 95.8848 - mae: 5.3348 - val_loss: 190.7067 - val_mae: 4.3012\n",
      "Epoch 149/200\n",
      "4084/4084 - 0s - loss: 93.9698 - mae: 5.1829 - val_loss: 172.1877 - val_mae: 4.3694\n",
      "Epoch 150/200\n",
      "4084/4084 - 0s - loss: 92.4660 - mae: 5.1318 - val_loss: 216.6559 - val_mae: 5.2997\n",
      "Epoch 151/200\n",
      "4084/4084 - 0s - loss: 92.9175 - mae: 5.2217 - val_loss: 436.6682 - val_mae: 5.3017\n",
      "Epoch 152/200\n",
      "4084/4084 - 0s - loss: 102.1782 - mae: 5.3778 - val_loss: 158.3082 - val_mae: 4.2889\n",
      "Epoch 153/200\n",
      "4084/4084 - 0s - loss: 99.6472 - mae: 5.3024 - val_loss: 122.9937 - val_mae: 4.2750\n",
      "Epoch 154/200\n",
      "4084/4084 - 0s - loss: 98.5543 - mae: 5.2735 - val_loss: 230.7003 - val_mae: 5.3674\n",
      "Epoch 155/200\n",
      "4084/4084 - 0s - loss: 89.6324 - mae: 5.2232 - val_loss: 157.0869 - val_mae: 4.8601\n",
      "Epoch 156/200\n",
      "4084/4084 - 0s - loss: 92.8448 - mae: 5.2841 - val_loss: 382.3698 - val_mae: 4.8259\n",
      "Epoch 157/200\n",
      "4084/4084 - 0s - loss: 93.5100 - mae: 5.3391 - val_loss: 233.7569 - val_mae: 4.3383\n",
      "Epoch 158/200\n",
      "4084/4084 - 0s - loss: 121.0064 - mae: 5.3757 - val_loss: 149.8741 - val_mae: 4.2080\n",
      "Epoch 159/200\n",
      "4084/4084 - 0s - loss: 103.7102 - mae: 5.3683 - val_loss: 150.6745 - val_mae: 4.4352\n",
      "Epoch 160/200\n",
      "4084/4084 - 0s - loss: 93.2575 - mae: 5.0811 - val_loss: 192.3517 - val_mae: 6.6792\n",
      "Epoch 161/200\n",
      "4084/4084 - 0s - loss: 90.3085 - mae: 5.2269 - val_loss: 224.0837 - val_mae: 4.8248\n",
      "Epoch 162/200\n",
      "4084/4084 - 0s - loss: 93.7238 - mae: 5.3119 - val_loss: 163.2724 - val_mae: 5.8167\n",
      "Epoch 163/200\n",
      "4084/4084 - 0s - loss: 91.1526 - mae: 5.2388 - val_loss: 156.5139 - val_mae: 4.5727\n",
      "Epoch 164/200\n",
      "4084/4084 - 0s - loss: 95.4750 - mae: 5.3201 - val_loss: 178.0914 - val_mae: 4.2323\n",
      "Epoch 165/200\n",
      "4084/4084 - 0s - loss: 94.9354 - mae: 5.1598 - val_loss: 202.7579 - val_mae: 4.2434\n",
      "Epoch 166/200\n",
      "4084/4084 - 0s - loss: 97.8677 - mae: 5.3715 - val_loss: 151.7172 - val_mae: 4.1580\n",
      "Epoch 167/200\n",
      "4084/4084 - 0s - loss: 101.3275 - mae: 5.3508 - val_loss: 238.5818 - val_mae: 5.4161\n",
      "Epoch 168/200\n",
      "4084/4084 - 1s - loss: 95.6408 - mae: 5.2455 - val_loss: 317.7991 - val_mae: 5.2249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "4084/4084 - 0s - loss: 96.1811 - mae: 5.2165 - val_loss: 573.2956 - val_mae: 5.6144\n",
      "Epoch 170/200\n",
      "4084/4084 - 0s - loss: 109.4075 - mae: 5.6501 - val_loss: 255.6899 - val_mae: 4.4373\n",
      "Epoch 171/200\n",
      "4084/4084 - 0s - loss: 91.1439 - mae: 5.0456 - val_loss: 171.3664 - val_mae: 4.3683\n",
      "Epoch 172/200\n",
      "4084/4084 - 0s - loss: 94.3773 - mae: 5.1699 - val_loss: 265.9099 - val_mae: 5.4114\n",
      "Epoch 173/200\n",
      "4084/4084 - 0s - loss: 90.4140 - mae: 5.1771 - val_loss: 289.8115 - val_mae: 4.6547\n",
      "Epoch 174/200\n",
      "4084/4084 - 0s - loss: 90.7473 - mae: 5.1196 - val_loss: 119.4365 - val_mae: 5.0861\n",
      "Epoch 175/200\n",
      "4084/4084 - 0s - loss: 88.5441 - mae: 5.1709 - val_loss: 252.3508 - val_mae: 4.9459\n",
      "Epoch 176/200\n",
      "4084/4084 - 0s - loss: 88.6098 - mae: 5.0394 - val_loss: 154.2845 - val_mae: 4.1055\n",
      "Epoch 177/200\n",
      "4084/4084 - 0s - loss: 86.7151 - mae: 5.1540 - val_loss: 109.3642 - val_mae: 5.0202\n",
      "Epoch 178/200\n",
      "4084/4084 - 0s - loss: 95.5331 - mae: 5.1929 - val_loss: 192.7692 - val_mae: 4.8853\n",
      "Epoch 179/200\n",
      "4084/4084 - 0s - loss: 95.1235 - mae: 5.3054 - val_loss: 189.9415 - val_mae: 4.2237\n",
      "Epoch 180/200\n",
      "4084/4084 - 0s - loss: 95.6996 - mae: 5.1750 - val_loss: 316.0642 - val_mae: 4.6068\n",
      "Epoch 181/200\n",
      "4084/4084 - 0s - loss: 93.8241 - mae: 5.2378 - val_loss: 204.2506 - val_mae: 4.9735\n",
      "Epoch 182/200\n",
      "4084/4084 - 0s - loss: 97.5411 - mae: 5.3660 - val_loss: 328.7719 - val_mae: 5.6351\n",
      "Epoch 183/200\n",
      "4084/4084 - 0s - loss: 97.1093 - mae: 5.2422 - val_loss: 934.0255 - val_mae: 6.6209\n",
      "Epoch 184/200\n",
      "4084/4084 - 0s - loss: 92.6342 - mae: 5.1835 - val_loss: 326.4358 - val_mae: 6.5523\n",
      "Epoch 185/200\n",
      "4084/4084 - 0s - loss: 89.6222 - mae: 4.9720 - val_loss: 290.7341 - val_mae: 4.5207\n",
      "Epoch 186/200\n",
      "4084/4084 - 0s - loss: 92.1503 - mae: 5.2077 - val_loss: 135.8008 - val_mae: 4.3068\n",
      "Epoch 187/200\n",
      "4084/4084 - 0s - loss: 91.5326 - mae: 5.1154 - val_loss: 370.7323 - val_mae: 4.9076\n",
      "Epoch 188/200\n",
      "4084/4084 - 0s - loss: 96.4520 - mae: 5.2013 - val_loss: 419.2847 - val_mae: 5.5216\n",
      "Epoch 189/200\n",
      "4084/4084 - 0s - loss: 89.0846 - mae: 5.1259 - val_loss: 207.6781 - val_mae: 4.3226\n",
      "Epoch 190/200\n",
      "4084/4084 - 0s - loss: 89.2411 - mae: 5.2804 - val_loss: 109.6647 - val_mae: 4.2614\n",
      "Epoch 191/200\n",
      "4084/4084 - 0s - loss: 93.0953 - mae: 5.2439 - val_loss: 297.1532 - val_mae: 4.4325\n",
      "Epoch 192/200\n",
      "4084/4084 - 0s - loss: 88.0578 - mae: 5.1779 - val_loss: 305.0488 - val_mae: 5.7085\n",
      "Epoch 193/200\n",
      "4084/4084 - 0s - loss: 88.7628 - mae: 4.9890 - val_loss: 393.0920 - val_mae: 5.6442\n",
      "Epoch 194/200\n",
      "4084/4084 - 0s - loss: 101.6044 - mae: 5.2935 - val_loss: 171.9786 - val_mae: 4.1978\n",
      "Epoch 195/200\n",
      "4084/4084 - 0s - loss: 94.6687 - mae: 5.1867 - val_loss: 143.7686 - val_mae: 3.8095\n",
      "Epoch 196/200\n",
      "4084/4084 - 0s - loss: 91.8912 - mae: 5.1749 - val_loss: 410.5881 - val_mae: 8.4200\n",
      "Epoch 197/200\n",
      "4084/4084 - 0s - loss: 93.8276 - mae: 5.2358 - val_loss: 268.9834 - val_mae: 4.6491\n",
      "Epoch 198/200\n",
      "4084/4084 - 0s - loss: 93.0285 - mae: 5.2902 - val_loss: 131.4143 - val_mae: 4.1720\n",
      "Epoch 199/200\n",
      "4084/4084 - 0s - loss: 91.0224 - mae: 5.1995 - val_loss: 164.7125 - val_mae: 4.1633\n",
      "Epoch 200/200\n",
      "4084/4084 - 0s - loss: 95.4847 - mae: 5.1982 - val_loss: 273.5608 - val_mae: 4.4462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x270b51a2288>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(scaled_X_train, ny_train, batch_size=16 ,epochs=200,validation_split=0.2,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#by ploting the learning curves of the neural netwrok we can see that 100 epochs are enough train it \n",
    "plt.figure(figsize=(15,8))\n",
    "# Use the history metrics\n",
    "plt.plot(final_model.history.history['loss'])\n",
    "plt.plot(final_model.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.legend(['Train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots(figsize=(15,30))\n",
    "ax.plot(y,X['Depth(m)'].values.reshape(-1,1),'r',label='ROP')\n",
    "ax.scatter(predictions,X_test['Depth(m)'].values.reshape(-1,1),label='predicted ROP')\n",
    "ay=plt.gca()\n",
    "ay.set_ylim(ay.get_ylim()[::-1])\n",
    "plt.ylabel('Depth (ft)')\n",
    "plt.xlabel('Rate of penetration (m/hr)')\n",
    "plt.title('Rate of penetration prediction');\n",
    "plt.legend(loc=\"best\")\n",
    "depth=[depth for depth in range(200,2400,200) ]\n",
    "for i in range(len(depth)):\n",
    "    plt.axhline(depth[i],color='black' )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
